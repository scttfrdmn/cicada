{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Cicada: Small Lab Data Commons Platform","text":"<p>Dormant data commons for academic research - Lightweight, cost-effective platform providing federated storage, automated metadata extraction, and comprehensive data management. Like a cicada, it lies dormant (consuming minimal resources) until needed, then emerges powerfully for data-intensive work.</p>"},{"location":"#overview","title":"Overview","text":"<p>Cicada transforms your lab into a comprehensive data commons: store, sync, organize, and manage all your research data from a single platform.</p> <ul> <li> <p> Storage &amp; Sync</p> <p>Multi-backend storage (local, S3) with efficient bi-directional sync. Only transfer what's changed. Watch mode for automatic syncing.</p> <p> Learn more</p> </li> <li> <p> Metadata Management</p> <p>Extract metadata from 14 file formats automatically. Microscopy, sequencing, mass spec, and more. S3 object tagging included.</p> <p> Explore formats</p> </li> <li> <p> Data Quality</p> <p>Validate data with 8 instrument presets. Track quality scores (0-100). Ensure data meets standards before analysis or sharing.</p> <p> See presets</p> </li> <li> <p> Production Ready</p> <p>100+ tests, comprehensive error handling, cross-platform support. Built for small labs (2-10 people).</p> <p> Get started</p> </li> </ul>"},{"location":"#why-cicada","title":"Why Cicada?","text":""},{"location":"#built-for-small-labs","title":"Built for Small Labs","text":"<p>Most research data management solutions are designed for large institutions with dedicated IT staff and budgets. Cicada is purpose-built for small academic labs:</p> <ul> <li>Low cost: Use affordable S3 storage (~$23/TB/month)</li> <li>Simple setup: Running in &lt; 15 minutes</li> <li>Minimal maintenance: Set and forget with watch mode</li> <li>No servers: Runs on your existing infrastructure</li> </ul>"},{"location":"#comprehensive-data-commons","title":"Comprehensive Data Commons","text":"<p>Stop cobbling together multiple tools. Cicada provides everything you need:</p> StorageMetadataQualityOptional <ul> <li>Local filesystem + cloud (S3)</li> <li>Multi-backend sync</li> <li>Incremental transfers</li> <li>Cost: ~$23/TB/month</li> </ul> <ul> <li>14 file format extractors</li> <li>6 instrument-specific types</li> <li>Automatic S3 tagging</li> <li>Searchable and organized</li> </ul> <ul> <li>8 instrument presets</li> <li>Quality scoring (0-100)</li> <li>Validation feedback</li> <li>Lab standardization</li> </ul> <ul> <li>DOI preparation</li> <li>Provider integration</li> <li>Publication workflows</li> <li>When you need it</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Sync data to S3\ncicada sync /data/microscope s3://lab-data/microscopy\n\n# Extract and validate metadata\ncicada metadata extract image.czi --preset zeiss-lsm-880\n\n# Auto-sync with watch mode\ncicada watch add /data/sequencer s3://lab-data/sequencing\n\n# Check data quality\ncicada metadata validate sample.fastq --preset illumina-novaseq\n</code></pre>"},{"location":"#features","title":"Features","text":""},{"location":"#core-platform","title":"Core Platform","text":"<p>Storage &amp; Sync</p> <ul> <li>Multi-backend storage (local, S3, future: Azure, GCS)</li> <li>Bi-directional sync with MD5/ETag comparison</li> <li>Concurrent transfers (4-8x speedup)</li> <li>Watch mode for automatic syncing</li> <li>Dry-run mode for safety</li> </ul> <p>Metadata &amp; Quality</p> <ul> <li>14 file format extractors across multiple domains</li> <li>6 instrument-specific metadata types</li> <li>8 built-in instrument presets</li> <li>Quality scoring (0-100 scale)</li> <li>S3 metadata tagging</li> <li>Extensible architecture</li> </ul> <p>Advanced Features (Optional)</p> <ul> <li>DOI preparation (DataCite Schema v4.5)</li> <li>Provider integration framework</li> <li>Publication workflows</li> </ul>"},{"location":"#platform-characteristics","title":"Platform Characteristics","text":"<p>\u2705 Cross-platform: Linux, macOS, Windows \u2705 Fast: Sub-millisecond metadata extraction \u2705 Reliable: 100+ tests, comprehensive error handling \u2705 Configurable: YAML config, extensive customization \u2705 Production-ready: Used in active research labs</p>"},{"location":"#supported-file-formats","title":"Supported File Formats","text":"Domain Formats Microscopy TIFF, OME-TIFF, Zeiss CZI, Nikon ND2, Leica LIF Sequencing FASTQ, BAM Mass Spec mzML, MGF Data Arrays HDF5, Zarr Medical/Flow DICOM, FCS Fallback Generic extractor for any file <p>See complete format reference \u2192</p>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to transform your lab's data management?</p> <ul> <li> <p> Install Cicada</p> <p>Download pre-built binaries or build from source.</p> <p> Installation guide</p> </li> <li> <p> Quick Start</p> <p>Get running in 15 minutes with our quick start guide.</p> <p> Quick start</p> </li> <li> <p> User Guide</p> <p>Learn about all features with detailed examples.</p> <p> User guide</p> </li> <li> <p> CLI Reference</p> <p>Complete command-line reference.</p> <p> CLI docs</p> </li> </ul>"},{"location":"#current-version","title":"Current Version","text":"<p>v0.2.0 - Released January 23, 2025</p> <ul> <li>14 file format extractors</li> <li>6 instrument-specific metadata types</li> <li>8 instrument presets</li> <li>S3 metadata tagging</li> <li>Optional DOI preparation</li> </ul> <p>View changelog \u2192 | Release notes \u2192</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub: scttfrdmn/cicada</li> <li>Issues: Report bugs or request features</li> <li>License: Apache 2.0</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use Cicada in your research, please cite:</p> <pre><code>@software{cicada2025,\n  title = {Cicada: Dormant Data Commons for Academic Research},\n  author = {Scott Friedman},\n  year = {2025},\n  url = {https://github.com/scttfrdmn/cicada},\n  version = {0.2.0}\n}\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/","title":"Cicada Credential Management Specification","text":"<p>Version: 0.3.0 Status: Design Specification Related Issue: #26 (Provider Configuration System)</p>"},{"location":"CREDENTIAL_MANAGEMENT/#overview","title":"Overview","text":"<p>Cicada must handle sensitive credentials (API tokens, passwords) securely while providing a flexible, user-friendly configuration experience. This document specifies the credential management system for v0.3.0.</p>"},{"location":"CREDENTIAL_MANAGEMENT/#design-principles","title":"Design Principles","text":"<ol> <li>Security First - Never log, expose, or leak credentials</li> <li>Defense in Depth - Multiple layers of protection</li> <li>Fail Secure - Reject insecure configurations with clear errors</li> <li>User Choice - Support multiple configuration methods</li> <li>Zero Surprises - Clear precedence rules and validation</li> </ol>"},{"location":"CREDENTIAL_MANAGEMENT/#configuration-methods","title":"Configuration Methods","text":"<p>Cicada supports four configuration methods with clear precedence:</p>"},{"location":"CREDENTIAL_MANAGEMENT/#precedence-order-highest-to-lowest","title":"Precedence Order (Highest to Lowest)","text":"<pre><code>1. Command-line flags         (--zenodo-token, --datacite-password)\n2. Environment variables       (CICADA_ZENODO_TOKEN, CICADA_DATACITE_PASSWORD)\n3. Cicada config file         (~/.config/cicada/config.yaml)\n4. Project .env file          (./.env in current directory)\n</code></pre> <p>Rule: Higher precedence always overrides lower precedence. No merging.</p> <p>Example: <pre><code># .env has ZENODO_TOKEN=old_token\n# Command specifies --zenodo-token=new_token\n# Result: Uses new_token (command-line wins)\n</code></pre></p>"},{"location":"CREDENTIAL_MANAGEMENT/#method-1-command-line-flags","title":"Method 1: Command-Line Flags","text":""},{"location":"CREDENTIAL_MANAGEMENT/#usage","title":"Usage","text":"<pre><code>cicada doi publish sample.fastq \\\n  --provider zenodo \\\n  --zenodo-token \"xyzabc123...\"\n</code></pre> <pre><code>cicada doi publish sample.fastq \\\n  --provider datacite \\\n  --datacite-repository-id \"10.5072/FK2\" \\\n  --datacite-password \"secret123\"\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#flag-names","title":"Flag Names","text":"Provider Flags Zenodo <code>--zenodo-token</code> Zenodo Sandbox <code>--zenodo-token</code> + <code>--zenodo-sandbox</code> DataCite <code>--datacite-repository-id</code>, <code>--datacite-password</code> DataCite Sandbox Same + <code>--datacite-sandbox</code>"},{"location":"CREDENTIAL_MANAGEMENT/#security-considerations","title":"Security Considerations","text":"<p>Pros: - \u2705 Explicit and clear - \u2705 Works for one-off commands - \u2705 No persistent storage</p> <p>Cons: - \u26a0\ufe0f Visible in shell history (use <code>history -d</code> or <code>HISTCONTROL</code>) - \u26a0\ufe0f Visible in process list (<code>ps aux</code> shows args) - \u26a0\ufe0f Visible in CI/CD logs if not masked</p> <p>Best Practice: <pre><code># Read from stdin to avoid history\nread -s ZENODO_TOKEN\nexport ZENODO_TOKEN\ncicada doi publish sample.fastq --provider zenodo\n\n# Or use environment variables (Method 2)\n</code></pre></p> <p>Recommendation: Use command-line flags only for: - One-off commands - When other methods aren't available - Testing/debugging</p>"},{"location":"CREDENTIAL_MANAGEMENT/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>Flags are parsed but never logged</li> <li>Help text shows flag names but not values</li> <li>Error messages show <code>--zenodo-token=***</code> not actual value</li> </ul>"},{"location":"CREDENTIAL_MANAGEMENT/#method-2-environment-variables","title":"Method 2: Environment Variables","text":""},{"location":"CREDENTIAL_MANAGEMENT/#usage_1","title":"Usage","text":"<pre><code># Zenodo\nexport CICADA_ZENODO_TOKEN=\"xyzabc123...\"\ncicada doi publish sample.fastq --provider zenodo\n\n# DataCite\nexport CICADA_DATACITE_REPOSITORY_ID=\"10.5072/FK2\"\nexport CICADA_DATACITE_PASSWORD=\"secret123\"\ncicada doi publish sample.fastq --provider datacite\n\n# Sandbox environments\nexport CICADA_ZENODO_SANDBOX=true\nexport CICADA_DATACITE_SANDBOX=true\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#variable-names","title":"Variable Names","text":"Provider Variables Zenodo Production <code>CICADA_ZENODO_TOKEN</code> Zenodo Sandbox <code>CICADA_ZENODO_TOKEN</code> + <code>CICADA_ZENODO_SANDBOX=true</code> DataCite Production <code>CICADA_DATACITE_REPOSITORY_ID</code>, <code>CICADA_DATACITE_PASSWORD</code> DataCite Sandbox Same + <code>CICADA_DATACITE_SANDBOX=true</code>"},{"location":"CREDENTIAL_MANAGEMENT/#alternative-provider-agnostic-names","title":"Alternative: Provider-Agnostic Names","text":"<p>For compatibility with existing tools:</p> <pre><code># Also supported (without CICADA_ prefix)\nexport ZENODO_TOKEN=\"...\"\nexport DATACITE_REPOSITORY_ID=\"...\"\nexport DATACITE_PASSWORD=\"...\"\n</code></pre> <p>Precedence: <code>CICADA_*</code> variables override non-prefixed variables.</p> <pre><code>export ZENODO_TOKEN=\"old_token\"\nexport CICADA_ZENODO_TOKEN=\"new_token\"\n# Uses: new_token\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#security-considerations_1","title":"Security Considerations","text":"<p>Pros: - \u2705 Standard Unix pattern - \u2705 Works with CI/CD secrets - \u2705 Session-scoped (not persistent) - \u2705 Doesn't appear in process args</p> <p>Cons: - \u26a0\ufe0f Can leak in error messages if not careful - \u26a0\ufe0f Visible to child processes - \u26a0\ufe0f Can leak in system logs (<code>/proc</code> on Linux)</p> <p>Best Practice: <pre><code># In ~/.bashrc or ~/.zshrc (for persistent use)\nexport CICADA_ZENODO_TOKEN=\"xyzabc123...\"\n\n# Or in a separate file (source when needed)\n# ~/.cicada/env.sh\nexport CICADA_ZENODO_TOKEN=\"xyzabc123...\"\nexport CICADA_DATACITE_REPOSITORY_ID=\"10.5072/FK2\"\nexport CICADA_DATACITE_PASSWORD=\"secret123\"\n\n# Usage:\nsource ~/.cicada/env.sh\ncicada doi publish sample.fastq --provider zenodo\n</code></pre></p> <p>Recommendation: Use environment variables for: - CI/CD pipelines (GitHub Actions, GitLab CI) - Docker containers - Temporary credentials - Development environments</p>"},{"location":"CREDENTIAL_MANAGEMENT/#implementation-notes_1","title":"Implementation Notes","text":"<ul> <li>Check both <code>CICADA_*</code> and unprefixed variants</li> <li>Never log environment variable values</li> <li>Warn if credentials found in both prefixed and unprefixed</li> </ul>"},{"location":"CREDENTIAL_MANAGEMENT/#method-3-cicada-config-file","title":"Method 3: Cicada Config File","text":""},{"location":"CREDENTIAL_MANAGEMENT/#location","title":"Location","text":"<p>Primary: <pre><code>~/.config/cicada/config.yaml\n</code></pre></p> <p>Fallback (if XDG_CONFIG_HOME not set or Windows): <pre><code>~/.cicada/config.yaml\n</code></pre></p> <p>Discovery order: 1. <code>$XDG_CONFIG_HOME/cicada/config.yaml</code> (if <code>XDG_CONFIG_HOME</code> set) 2. <code>~/.config/cicada/config.yaml</code> (Linux/macOS default) 3. <code>~/.cicada/config.yaml</code> (fallback) 4. <code>%APPDATA%\\cicada\\config.yaml</code> (Windows)</p>"},{"location":"CREDENTIAL_MANAGEMENT/#format","title":"Format","text":"<pre><code># ~/.config/cicada/config.yaml\n\n# Provider credentials\nproviders:\n  zenodo:\n    token: \"xyzabc123...\"\n    environment: production  # or \"sandbox\"\n\n  datacite:\n    repository_id: \"10.5072/FK2\"\n    password: \"secret123\"\n    environment: production  # or \"sandbox\"\n\n# Optional: Default provider\ndefault_provider: zenodo\n\n# Optional: Security settings\nsecurity:\n  check_permissions: true  # Verify file permissions\n  warn_insecure: true      # Warn about insecure configurations\n\n# Optional: Other settings (future use)\nmetadata:\n  default_publisher: \"My Lab\"\n  default_rights: \"CC-BY-4.0\"\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#minimal-example","title":"Minimal Example","text":"<pre><code>providers:\n  zenodo:\n    token: \"xyzabc123...\"\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#security-considerations_2","title":"Security Considerations","text":"<p>Pros: - \u2705 Persistent configuration - \u2705 Structured format (YAML) - \u2705 Can include multiple providers - \u2705 Can be encrypted (future enhancement) - \u2705 Not visible in process list or history</p> <p>Cons: - \u26a0\ufe0f Persistent file - must be secured - \u26a0\ufe0f Can be accidentally committed to git - \u26a0\ufe0f Can be copied/backed up insecurely</p> <p>Best Practice: <pre><code># Create config directory\nmkdir -p ~/.config/cicada\n\n# Create config file\ncat &gt; ~/.config/cicada/config.yaml &lt;&lt;'EOF'\nproviders:\n  zenodo:\n    token: \"your-token-here\"\nEOF\n\n# Secure permissions (REQUIRED)\nchmod 600 ~/.config/cicada/config.yaml\n\n# Verify permissions\nls -la ~/.config/cicada/config.yaml\n# Should show: -rw------- (600)\n</code></pre></p> <p>Recommendation: Use config file for: - Persistent credentials (daily use) - Multiple provider configurations - Shared settings across commands - Production use</p>"},{"location":"CREDENTIAL_MANAGEMENT/#security-requirements","title":"Security Requirements","text":""},{"location":"CREDENTIAL_MANAGEMENT/#1-file-permissions","title":"1. File Permissions","text":"<p>On Unix-like systems (Linux, macOS):</p> <p>Cicada MUST enforce <code>600</code> permissions (owner read/write only):</p> <pre><code># Required permissions\n-rw-------  1 user  group  123 Jan 24 10:00 config.yaml\n</code></pre> <p>Behavior: <pre><code>// Check permissions\nfileInfo, _ := os.Stat(configPath)\nmode := fileInfo.Mode()\n\nif mode.Perm() &amp; 0077 != 0 {  // Check group/other bits\n    return fmt.Errorf(\"insecure permissions on %s: %o (must be 600)\",\n                      configPath, mode.Perm())\n}\n</code></pre></p> <p>Error message: <pre><code>Error: Insecure permissions on /home/user/.config/cicada/config.yaml: 0644\n\n  Your config file contains sensitive credentials but has insecure permissions.\n  Other users on this system can read your credentials.\n\n  Fix with: chmod 600 /home/user/.config/cicada/config.yaml\n\n  Current: -rw-r--r-- (0644)\n  Required: -rw------- (0600)\n</code></pre></p> <p>On Windows:</p> <p>Check ACLs to ensure only owner has access: <pre><code>// Use Windows ACL APIs to verify only owner can read\n// Warn if BUILTIN\\Users or Everyone has access\n</code></pre></p>"},{"location":"CREDENTIAL_MANAGEMENT/#2-gitignore-protection","title":"2. .gitignore Protection","text":"<p>Cicada should check parent directories for <code>.git</code> and warn:</p> <pre><code>Warning: Config file is in a git repository\n\n  File: /home/user/project/.cicada/config.yaml\n  Repo: /home/user/project\n\n  Your config file contains credentials but is in a git repository.\n  Ensure it's in .gitignore to prevent accidental commit.\n\n  Add to .gitignore:\n    .cicada/config.yaml\n\n  Or move config to: ~/.config/cicada/config.yaml\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#3-world-readable-check","title":"3. World-Readable Check","text":"<p>Warn if config is in shared/world-readable locations: <pre><code>Warning: Config file in potentially shared location\n\n  File: /tmp/cicada/config.yaml\n\n  This location may be accessible to other users.\n  Consider moving to: ~/.config/cicada/config.yaml\n</code></pre></p>"},{"location":"CREDENTIAL_MANAGEMENT/#implementation-notes_2","title":"Implementation Notes","text":""},{"location":"CREDENTIAL_MANAGEMENT/#config-loading","title":"Config Loading","text":"<pre><code>func LoadConfig() (*Config, error) {\n    // 1. Find config file\n    configPath := findConfigFile()\n\n    // 2. Check security\n    if err := checkConfigSecurity(configPath); err != nil {\n        return nil, err\n    }\n\n    // 3. Parse YAML\n    data, err := os.ReadFile(configPath)\n    if err != nil {\n        return nil, err\n    }\n\n    // 4. Unmarshal (but never log contents)\n    var config Config\n    if err := yaml.Unmarshal(data, &amp;config); err != nil {\n        return nil, fmt.Errorf(\"invalid config format: %w\", err)\n    }\n\n    // 5. Validate (but don't log credential values)\n    if err := config.Validate(); err != nil {\n        return nil, err\n    }\n\n    return &amp;config, nil\n}\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#security-checks","title":"Security Checks","text":"<pre><code>func checkConfigSecurity(path string) error {\n    // Check 1: File permissions\n    if err := checkPermissions(path); err != nil {\n        return err\n    }\n\n    // Check 2: Inside git repo?\n    if inGitRepo(path) {\n        // Warning, not error (user might have .gitignore)\n        warnGitRepo(path)\n    }\n\n    // Check 3: Shared location?\n    if isSharedLocation(path) {\n        warnSharedLocation(path)\n    }\n\n    return nil\n}\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#method-4-project-env-file","title":"Method 4: Project .env File","text":""},{"location":"CREDENTIAL_MANAGEMENT/#location_1","title":"Location","text":"<pre><code>./.env\n</code></pre> <p>Discovery: Current working directory only (not parent directories).</p>"},{"location":"CREDENTIAL_MANAGEMENT/#format_1","title":"Format","text":"<pre><code># .env (dotenv format)\n\n# Zenodo\nCICADA_ZENODO_TOKEN=xyzabc123...\n\n# DataCite\nCICADA_DATACITE_REPOSITORY_ID=10.5072/FK2\nCICADA_DATACITE_PASSWORD=secret123\n\n# Environment\nCICADA_ZENODO_SANDBOX=true\nCICADA_DATACITE_SANDBOX=false\n\n# Optional: Default provider\nCICADA_DEFAULT_PROVIDER=zenodo\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#usage_2","title":"Usage","text":"<pre><code># Cicada automatically loads .env if present\ncd /path/to/project\ncicada doi publish sample.fastq --provider zenodo\n# Uses credentials from ./.env\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#security-considerations_3","title":"Security Considerations","text":"<p>Pros: - \u2705 Project-specific credentials - \u2705 Works with docker-compose patterns - \u2705 Familiar to developers</p> <p>Cons: - \u26a0\ufe0f EASY TO ACCIDENTALLY COMMIT TO GIT - \u26a0\ufe0f Must be in <code>.gitignore</code> - \u26a0\ufe0f Persistent file - must be secured</p> <p>Best Practice: <pre><code># Create .env\ncat &gt; .env &lt;&lt;'EOF'\nCICADA_ZENODO_TOKEN=your-token-here\nEOF\n\n# Secure permissions\nchmod 600 .env\n\n# Add to .gitignore (CRITICAL)\necho \".env\" &gt;&gt; .gitignore\n\n# Verify not tracked\ngit status .env\n# Should show: \"use git add\" (not tracked)\n</code></pre></p> <p>Recommendation: Use .env for: - Project-specific credentials (sandbox tokens) - Development environments - When using docker-compose - Team projects (with .env.example template)</p>"},{"location":"CREDENTIAL_MANAGEMENT/#security-requirements_1","title":"Security Requirements","text":"<p>Same as config file:</p> <ol> <li>File permissions must be 600</li> <li>Warn if in git repository without .gitignore</li> <li>Check for common mistakes:</li> </ol> <pre><code>Error: .env file has insecure permissions: 0644\n\n  Fix with: chmod 600 .env\n\nError: .env file is tracked by git\n\n  Your .env file is tracked by git and will be committed.\n  This will expose your credentials.\n\n  Fix:\n    1. Add to .gitignore:  echo \".env\" &gt;&gt; .gitignore\n    2. Remove from git:    git rm --cached .env\n    3. Commit changes:     git commit -m \"Remove .env from tracking\"\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#envexample-template","title":".env.example Template","text":"<p>Cicada should support creating a template:</p> <pre><code># Generate template (no credentials)\ncicada config init --env-example\n\n# Creates .env.example:\nCICADA_ZENODO_TOKEN=your-zenodo-token-here\nCICADA_DATACITE_REPOSITORY_ID=your-datacite-repo-id\nCICADA_DATACITE_PASSWORD=your-datacite-password\n\n# Usage: cp .env.example .env, then edit\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#credential-resolution-algorithm","title":"Credential Resolution Algorithm","text":""},{"location":"CREDENTIAL_MANAGEMENT/#pseudocode","title":"Pseudocode","text":"<pre><code>def get_credential(provider: str, key: str) -&gt; str:\n    \"\"\"\n    Get credential with proper precedence.\n\n    Precedence (highest to lowest):\n    1. Command-line flags\n    2. Environment variables\n    3. Config file\n    4. .env file\n    \"\"\"\n\n    # 1. Check command-line flags\n    flag_value = get_flag_value(provider, key)\n    if flag_value is not None:\n        log_source(\"command-line flag\", provider, key)\n        return flag_value\n\n    # 2. Check environment variables\n    env_value = get_env_value(provider, key)\n    if env_value is not None:\n        log_source(\"environment variable\", provider, key)\n        return env_value\n\n    # 3. Check config file\n    config_value = get_config_value(provider, key)\n    if config_value is not None:\n        log_source(\"config file\", provider, key)\n        return config_value\n\n    # 4. Check .env file\n    dotenv_value = get_dotenv_value(provider, key)\n    if dotenv_value is not None:\n        log_source(\".env file\", provider, key)\n        return dotenv_value\n\n    # 5. Not found\n    return None\n\ndef log_source(source: str, provider: str, key: str):\n    \"\"\"Log where credential came from (but NOT the value).\"\"\"\n    log.debug(f\"Using {provider} {key} from {source}\")\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#go-implementation","title":"Go Implementation","text":"<pre><code>type CredentialSource string\n\nconst (\n    SourceCommandLine CredentialSource = \"command-line flag\"\n    SourceEnvVar      CredentialSource = \"environment variable\"\n    SourceConfigFile  CredentialSource = \"config file\"\n    SourceDotEnv      CredentialSource = \".env file\"\n)\n\ntype Credential struct {\n    Value  string\n    Source CredentialSource\n}\n\nfunc GetCredential(provider, key string) (*Credential, error) {\n    // 1. Command-line flags\n    if val := getFromFlags(provider, key); val != \"\" {\n        return &amp;Credential{Value: val, Source: SourceCommandLine}, nil\n    }\n\n    // 2. Environment variables\n    if val := getFromEnv(provider, key); val != \"\" {\n        return &amp;Credential{Value: val, Source: SourceEnvVar}, nil\n    }\n\n    // 3. Config file\n    if val := getFromConfig(provider, key); val != \"\" {\n        return &amp;Credential{Value: val, Source: SourceConfigFile}, nil\n    }\n\n    // 4. .env file\n    if val := getFromDotEnv(provider, key); val != \"\" {\n        return &amp;Credential{Value: val, Source: SourceDotEnv}, nil\n    }\n\n    // 5. Not found\n    return nil, fmt.Errorf(\"%s %s not configured\", provider, key)\n}\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#security-best-practices-implementation","title":"Security Best Practices (Implementation)","text":""},{"location":"CREDENTIAL_MANAGEMENT/#1-never-log-credentials","title":"1. Never Log Credentials","text":"<pre><code>// \u274c BAD\nlog.Debug(\"Using token: %s\", token)\nlog.Debug(\"Config: %+v\", config)  // config contains credentials\n\n// \u2705 GOOD\nlog.Debug(\"Using token from %s\", source)\nlog.Debug(\"Config loaded from %s\", configPath)\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#2-redact-in-error-messages","title":"2. Redact in Error Messages","text":"<pre><code>// \u274c BAD\nreturn fmt.Errorf(\"authentication failed with token %s\", token)\n\n// \u2705 GOOD\nreturn fmt.Errorf(\"authentication failed (check token)\")\n\n// \u2705 GOOD (show source but not value)\nreturn fmt.Errorf(\"authentication failed (token from %s)\", cred.Source)\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#3-redact-in-helpdebug-output","title":"3. Redact in Help/Debug Output","text":"<pre><code>// When printing config for debugging:\nfunc (c *Config) String() string {\n    return fmt.Sprintf(\"Config{provider=%s, token=*****, env=%s}\",\n                       c.Provider, c.Environment)\n}\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#4-zero-memory-after-use","title":"4. Zero Memory After Use","text":"<pre><code>// For sensitive strings\nfunc clearString(s *string) {\n    if s != nil &amp;&amp; *s != \"\" {\n        for i := range *s {\n            (*s)[i] = 0\n        }\n    }\n}\n\n// Usage:\ndefer clearString(&amp;token)\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#5-validate-before-use","title":"5. Validate Before Use","text":"<pre><code>func ValidateZenodoToken(token string) error {\n    if token == \"\" {\n        return errors.New(\"token is empty\")\n    }\n    if len(token) &lt; 20 {\n        return errors.New(\"token too short (expected 40+ chars)\")\n    }\n    if strings.Contains(token, \" \") {\n        return errors.New(\"token contains whitespace\")\n    }\n    return nil\n}\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#6-prevent-credential-leaks-in-version-control","title":"6. Prevent Credential Leaks in Version Control","text":"<pre><code># Cicada should create .gitignore entries\ncicada config init\n\n# Creates/updates .gitignore:\n.env\n.cicada/config.yaml\n*.credentials\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#user-facing-documentation","title":"User-Facing Documentation","text":""},{"location":"CREDENTIAL_MANAGEMENT/#quick-start","title":"Quick Start","text":"<pre><code># Credential Configuration\n\nCicada needs credentials to publish DOIs. Choose the method that works best for you:\n\n## Option 1: Config File (Recommended)\n\nCreate `~/.config/cicada/config.yaml`:\n\n```yaml\nproviders:\n  zenodo:\n    token: \"your-zenodo-token\"\n</code></pre> <p>Secure the file: <pre><code>chmod 600 ~/.config/cicada/config.yaml\n</code></pre></p> <p>Use it: <pre><code>cicada doi publish sample.fastq --provider zenodo\n</code></pre></p>"},{"location":"CREDENTIAL_MANAGEMENT/#option-2-environment-variables","title":"Option 2: Environment Variables","text":"<pre><code>export CICADA_ZENODO_TOKEN=\"your-zenodo-token\"\ncicada doi publish sample.fastq --provider zenodo\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#option-3-project-env-file","title":"Option 3: Project .env File","text":"<p>Create <code>.env</code> in your project: <pre><code>CICADA_ZENODO_TOKEN=your-zenodo-token\n</code></pre></p> <p>Important: Add to <code>.gitignore</code>: <pre><code>echo \".env\" &gt;&gt; .gitignore\n</code></pre></p>"},{"location":"CREDENTIAL_MANAGEMENT/#security-checklist","title":"Security Checklist","text":"<ul> <li> Config file has 600 permissions (<code>chmod 600</code>)</li> <li> .env file in .gitignore</li> <li> Never commit credentials to git</li> <li> Use sandbox tokens for testing</li> <li> Rotate tokens regularly <pre><code>### Troubleshooting\n\n```markdown\n# Common Errors\n\n## \"Insecure permissions on config.yaml\"\n\n**Cause:** Config file readable by others\n\n**Fix:**\n```bash\nchmod 600 ~/.config/cicada/config.yaml\n</code></pre></li> </ul>"},{"location":"CREDENTIAL_MANAGEMENT/#env-file-is-tracked-by-git","title":"\".env file is tracked by git\"","text":"<p>Cause: .env not in .gitignore</p> <p>Fix: <pre><code>echo \".env\" &gt;&gt; .gitignore\ngit rm --cached .env\ngit commit -m \"Remove .env from tracking\"\n</code></pre></p>"},{"location":"CREDENTIAL_MANAGEMENT/#authentication-failed","title":"\"Authentication failed\"","text":"<p>Cause: Invalid or expired credentials</p> <p>Check: 1. Verify token is correct (copy/paste carefully) 2. Check token hasn't expired 3. Verify using correct environment (sandbox vs production) 4. Test token directly with provider API</p> <p>Debug: <pre><code>cicada doi publish --debug sample.fastq --provider zenodo\n# Shows: \"Using token from config file\"\n</code></pre> <pre><code>## CLI Commands for Credential Management\n\n### Initialize Configuration\n\n```bash\n# Create config directory and file\ncicada config init\n\n# Creates:\n# - ~/.config/cicada/config.yaml (empty template)\n# - .gitignore entries (if in git repo)\n</code></pre></p>"},{"location":"CREDENTIAL_MANAGEMENT/#set-credentials","title":"Set Credentials","text":"<pre><code># Interactive prompt (secure input)\ncicada config set zenodo-token\n# Prompts: Enter Zenodo token: [hidden input]\n\ncicada config set datacite-repository-id\ncicada config set datacite-password\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#view-configuration-redacted","title":"View Configuration (Redacted)","text":"<pre><code>cicada config show\n\n# Output:\nProviders:\n  zenodo:\n    token: ****ab123 (from config file)\n    environment: production\n  datacite:\n    repository_id: 10.5072/FK2 (from environment variable)\n    password: ***** (from config file)\n    environment: sandbox\n\nConfig file: /home/user/.config/cicada/config.yaml\nPermissions: 600 \u2713\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#validate-configuration","title":"Validate Configuration","text":"<pre><code># Check security\ncicada config validate\n\n# Output:\n\u2713 Config file permissions: 600\n\u2713 Not in git repository\n\u2713 Zenodo token format valid\n\u2713 DataCite repository ID format valid\n\u26a0 Warning: Using sandbox environment\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#test-credentials","title":"Test Credentials","text":"<pre><code># Test authentication without publishing\ncicada config test zenodo\n# Output: \u2713 Zenodo authentication successful\n\ncicada config test datacite\n# Output: \u2713 DataCite authentication successful\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#implementation-checklist-issue-26","title":"Implementation Checklist (Issue #26)","text":""},{"location":"CREDENTIAL_MANAGEMENT/#core-implementation","title":"Core Implementation","text":"<ul> <li> Config file loading (<code>~/.config/cicada/config.yaml</code>)</li> <li> Environment variable support (<code>CICADA_*</code>)</li> <li> .env file loading (current directory)</li> <li> Command-line flag parsing</li> <li> Credential precedence resolution</li> </ul>"},{"location":"CREDENTIAL_MANAGEMENT/#security-implementation","title":"Security Implementation","text":"<ul> <li> File permission checking (600 required)</li> <li> Git repository detection and warning</li> <li> Credential redaction in logs</li> <li> Credential redaction in error messages</li> <li> Memory zeroing after use</li> <li> Input validation (token format, etc.)</li> </ul>"},{"location":"CREDENTIAL_MANAGEMENT/#cli-commands","title":"CLI Commands","text":"<ul> <li> <code>cicada config init</code> - Create config template</li> <li> <code>cicada config set &lt;key&gt;</code> - Set credential (secure prompt)</li> <li> <code>cicada config show</code> - View config (redacted)</li> <li> <code>cicada config validate</code> - Check security</li> <li> <code>cicada config test &lt;provider&gt;</code> - Test authentication</li> </ul>"},{"location":"CREDENTIAL_MANAGEMENT/#testing","title":"Testing","text":"<ul> <li> Unit tests for precedence resolution</li> <li> Unit tests for security checks</li> <li> Integration tests with all config methods</li> <li> Test insecure permissions (should fail)</li> <li> Test git repo detection</li> <li> Test credential redaction</li> </ul>"},{"location":"CREDENTIAL_MANAGEMENT/#documentation","title":"Documentation","text":"<ul> <li> User guide for configuration</li> <li> Security best practices</li> <li> Troubleshooting guide</li> <li> Examples for each method</li> </ul>"},{"location":"CREDENTIAL_MANAGEMENT/#example-complete-workflow","title":"Example: Complete Workflow","text":""},{"location":"CREDENTIAL_MANAGEMENT/#developer-setup","title":"Developer Setup","text":"<pre><code># 1. Initialize Cicada config\ncicada config init\n\n# 2. Get Zenodo token\n# - Go to https://zenodo.org/account/settings/applications/tokens/new/\n# - Create token with deposit:write scope\n# - Copy token\n\n# 3. Configure Cicada (secure prompt)\ncicada config set zenodo-token\n# Enter Zenodo token: [paste token, hidden]\n# \u2713 Token saved to ~/.config/cicada/config.yaml\n\n# 4. Verify setup\ncicada config validate\n# \u2713 Config file permissions: 600\n# \u2713 Zenodo token format valid\n\n# 5. Test authentication\ncicada config test zenodo\n# \u2713 Zenodo authentication successful\n\n# 6. Publish DOI\ncicada doi publish sample.fastq --provider zenodo\n# \u2713 DOI: 10.5281/zenodo.123456\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#cicd-setup-github-actions","title":"CI/CD Setup (GitHub Actions)","text":"<pre><code># .github/workflows/publish-doi.yml\nname: Publish DOI\n\non:\n  release:\n    types: [published]\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Install Cicada\n        run: |\n          wget https://github.com/scttfrdmn/cicada/releases/download/v0.3.0/cicada_linux_amd64\n          chmod +x cicada_linux_amd64\n          sudo mv cicada_linux_amd64 /usr/local/bin/cicada\n\n      - name: Publish DOI\n        env:\n          CICADA_ZENODO_TOKEN: ${{ secrets.ZENODO_TOKEN }}\n        run: |\n          cicada doi publish data/sample.fastq \\\n            --provider zenodo \\\n            --enrich metadata.yaml\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#project-specific-setup","title":"Project-Specific Setup","text":"<pre><code># 1. Create .env for project (sandbox)\ncat &gt; .env &lt;&lt;'EOF'\nCICADA_ZENODO_TOKEN=sandbox-token-here\nCICADA_ZENODO_SANDBOX=true\nEOF\n\n# 2. Secure .env\nchmod 600 .env\n\n# 3. Add to .gitignore\necho \".env\" &gt;&gt; .gitignore\n\n# 4. Create template for team\ncat &gt; .env.example &lt;&lt;'EOF'\n# Copy to .env and add your credentials\nCICADA_ZENODO_TOKEN=your-token-here\nCICADA_ZENODO_SANDBOX=true\nEOF\n\n# 5. Commit template (not .env!)\ngit add .env.example\ngit commit -m \"Add Cicada config template\"\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#future-enhancements-post-v030","title":"Future Enhancements (Post v0.3.0)","text":""},{"location":"CREDENTIAL_MANAGEMENT/#encrypted-config-file","title":"Encrypted Config File","text":"<pre><code># ~/.config/cicada/config.yaml (encrypted)\nproviders:\n  zenodo:\n    token: !encrypted |\n      AES256:base64-encrypted-data-here\n</code></pre> <p>Unlock with passphrase or system keychain.</p>"},{"location":"CREDENTIAL_MANAGEMENT/#keychain-integration","title":"Keychain Integration","text":"<pre><code># Store in system keychain (macOS, GNOME, etc.)\ncicada config set zenodo-token --keychain\n# Stored in: macOS Keychain / GNOME Keyring / Windows Credential Manager\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#credential-helper","title":"Credential Helper","text":"<pre><code># ~/.config/cicada/config.yaml\nproviders:\n  zenodo:\n    token: !helper \"get-zenodo-token.sh\"\n</code></pre> <pre><code># get-zenodo-token.sh\n#!/bin/bash\n# Fetch token from secret manager\naws secretsmanager get-secret-value --secret-id cicada/zenodo-token | jq -r .SecretString\n</code></pre>"},{"location":"CREDENTIAL_MANAGEMENT/#multi-profile-support","title":"Multi-Profile Support","text":"<pre><code># Switch between profiles\ncicada config use-profile personal\ncicada config use-profile work\ncicada config use-profile sandbox\n</code></pre> <p>Next Steps: 1. Review this specification 2. Implement in Issue #26 (Provider Configuration System) 3. Write comprehensive tests 4. Document for users</p>"},{"location":"DATACITE_SERVICE_PROVIDER/","title":"DataCite Integration Strategy for Cicada","text":"<p>Date: January 24, 2025 Status: Research Complete - Awaiting Decision Related Issues: #27 (DataCite API Client), #36 (Integration Tests), #37 (Provider Documentation)</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#executive-summary","title":"Executive Summary","text":"<p>After researching DataCite integration options, becoming a DataCite Registered Service Provider is the recommended path for Cicada. This approach:</p> <ul> <li>\u2705 Zero cost to Cicada as a project</li> <li>\u2705 Scales to unlimited users (each brings their own credentials)</li> <li>\u2705 Official certification from DataCite</li> <li>\u2705 Best user experience for institutional users</li> </ul> <p>Alternative path for testing: Use Lyrasis consortium membership ($1,625/year) for sandbox/early development, then transition to Registered Service Provider for production.</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#understanding-datacite-membership-models","title":"Understanding DataCite Membership Models","text":""},{"location":"DATACITE_SERVICE_PROVIDER/#model-1-direct-institutional-membership","title":"Model 1: Direct Institutional Membership","text":"<p>Who it's for: Individual institutions (universities, research centers)</p> <p>Cost: - \u20ac2,000/year base fee - Plus per-DOI fees (volume-based pricing) - Total: \u20ac2,500 - \u20ac5,000+/year depending on usage</p> <p>For Cicada: \u274c Not Recommended - Expensive for a single open-source project - Doesn't scale to multiple users - Requires Cicada to manage all DOIs</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#model-2-consortium-membership","title":"Model 2: Consortium Membership","text":"<p>Who it's for: Regional organizations (e.g., US institutions via Lyrasis)</p> <p>Cost (Lyrasis example): - \\(1,625/year for 1-1,999 DOIs (\\)1/DOI after) - $3,600/year flat rate for 2,000-10,000 DOIs - Other consortia: British Library (UK), EUDAT (EU)</p> <p>For Cicada: \u26a0\ufe0f Potential for Early Development - Lower cost than direct membership - Could be used for sandbox testing - Still requires annual fees - Temporary solution, not long-term strategy</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#model-3-registered-service-provider-recommended","title":"Model 3: Registered Service Provider (RECOMMENDED)","text":"<p>Who it's for: Software applications that integrate DataCite API</p> <p>Cost: - FREE for the software provider (Cicada) - Users bring their own DataCite credentials - No DOI limits, no per-user fees</p> <p>For Cicada: \u2705 RECOMMENDED - No cost to Cicada project - Users use their institution's existing DataCite membership - Official DataCite certification and listing - Scales to unlimited users - Best fit for open-source CLI tool</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#what-is-a-datacite-registered-service-provider","title":"What is a DataCite Registered Service Provider?","text":""},{"location":"DATACITE_SERVICE_PROVIDER/#definition","title":"Definition","text":"<p>A Registered Service Provider is software that: 1. Integrates the DataCite REST API 2. Allows DataCite members to register DOIs using their own credentials 3. Meets DataCite's technical and security requirements 4. Is officially certified and listed by DataCite</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#how-it-works","title":"How It Works","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Cicada User                        \u2502\n\u2502                  (Research Institution)                 \u2502\n\u2502                                                          \u2502\n\u2502  Has DataCite membership through:                       \u2502\n\u2502  - Direct membership (\u20ac2,000/year)                      \u2502\n\u2502  - Consortium (e.g., Lyrasis $1,625/year)               \u2502\n\u2502  - Institutional affiliation                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u2502 Provides credentials:\n                           \u2502 - Repository ID (prefix)\n                           \u2502 - Password\n                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Cicada (v0.3.0+)                     \u2502\n\u2502              Registered Service Provider                \u2502\n\u2502                                                          \u2502\n\u2502  $ cicada doi publish sample.fastq \\                    \u2502\n\u2502      --provider datacite \\                              \u2502\n\u2502      --datacite-repository-id &lt;user's ID&gt; \\             \u2502\n\u2502      --datacite-password &lt;user's password&gt;              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u2502 API Request with\n                           \u2502 user's credentials\n                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              DataCite REST API                          \u2502\n\u2502          https://api.datacite.org                       \u2502\n\u2502                                                          \u2502\n\u2502  Validates credentials, mints DOI using                 \u2502\n\u2502  user's allocation                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u2502 DOI: 10.5072/xxxxx\n                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  User's DOI Record                      \u2502\n\u2502              Registered under their prefix              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"DATACITE_SERVICE_PROVIDER/#key-benefits-for-cicada","title":"Key Benefits for Cicada","text":"<ol> <li>Zero Cost: Cicada doesn't need a DataCite membership</li> <li>No User Limits: Support unlimited institutions</li> <li>Credential Security: Users keep their own credentials</li> <li>Official Status: DataCite certification badge</li> <li>Discoverability: Listed on DataCite's website</li> <li>User Trust: Official certification provides credibility</li> </ol>"},{"location":"DATACITE_SERVICE_PROVIDER/#key-benefits-for-users","title":"Key Benefits for Users","text":"<ol> <li>Use Existing Membership: No additional DataCite costs</li> <li>Institutional Control: DOIs registered under their prefix</li> <li>Quota Management: Uses their existing allocation</li> <li>Compliance: Meets institutional requirements</li> <li>Certified Software: Validated by DataCite</li> </ol>"},{"location":"DATACITE_SERVICE_PROVIDER/#requirements-for-becoming-a-registered-service-provider","title":"Requirements for Becoming a Registered Service Provider","text":""},{"location":"DATACITE_SERVICE_PROVIDER/#1-technical-requirements","title":"1. Technical Requirements","text":""},{"location":"DATACITE_SERVICE_PROVIDER/#api-integration","title":"API Integration","text":"<ul> <li>\u2705 Implement DataCite REST API v2</li> <li>\u2705 Support all CRUD operations (Create, Read, Update, Delete)</li> <li>\u2705 Handle authentication (HTTP Basic Auth)</li> <li>\u2705 Support both sandbox and production environments</li> <li>\u2705 Implement proper error handling and retry logic</li> </ul> <p>Status for Cicada: Planned in Issue #27 (DataCite API Client)</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#metadata-schema-support","title":"Metadata Schema Support","text":"<ul> <li>\u2705 Support DataCite Metadata Schema v4.5 (current)</li> <li>\u2705 Commit to updating when schema changes</li> <li>\u2705 Validate metadata before submission</li> </ul> <p>Status for Cicada: \u2705 Already implemented in v0.2.0</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#demonstration-requirement","title":"Demonstration Requirement","text":"<ul> <li>\u2705 Have registered DOIs in production (findable, not just draft)</li> <li>\u2705 Demonstrate working integration on verification call</li> </ul> <p>Status for Cicada: \u23f3 Will be ready after Milestone 1 (Issue #36)</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#2-security-requirements","title":"2. Security Requirements","text":"<ul> <li>\u2705 Secure credential handling:</li> <li>Never log credentials</li> <li>Support environment variables</li> <li>Support config files with proper permissions</li> <li> <p>Clear documentation on credential security</p> </li> <li> <p>\u2705 HTTPS only: All API calls over TLS</p> </li> <li>\u2705 Error handling: Don't expose credentials in error messages</li> <li>\u2705 Documentation: Security best practices for users</li> </ul> <p>Status for Cicada: Planned in Issue #26 (Provider Configuration)</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#3-documentation-requirements","title":"3. Documentation Requirements","text":"<ul> <li>\u2705 User guide for DataCite integration</li> <li>\u2705 API usage documentation</li> <li>\u2705 Security best practices</li> <li>\u2705 Troubleshooting guide</li> </ul> <p>Status for Cicada: Planned in Issue #37 (Provider Documentation)</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#4-best-practices","title":"4. Best Practices","text":"<ul> <li>\u2705 Follow RESTful principles</li> <li>\u2705 Implement rate limiting</li> <li>\u2705 Use appropriate HTTP methods and status codes</li> <li>\u2705 Provide clear error messages to users</li> <li>\u2705 Monitor API deprecation notices</li> </ul> <p>Status for Cicada: Planned in Issue #35 (Error Handling &amp; Retry Logic)</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#the-registration-process","title":"The Registration Process","text":""},{"location":"DATACITE_SERVICE_PROVIDER/#step-1-prepare-integration-weeks-1-4","title":"Step 1: Prepare Integration (Weeks 1-4)","text":"<p>Goal: Implement DataCite API client meeting all requirements</p> <p>Tasks: - \u2705 Issue #26: Provider Configuration System - \u2705 Issue #27: DataCite API Client - \u2705 Issue #30: DataCite Metadata Mapping - \u2705 Issue #32: CLI <code>doi publish</code> Command - \u2705 Issue #35: Error Handling &amp; Retry Logic - \u2705 Issue #36: Integration Tests (with sandbox)</p> <p>Deliverable: Working integration with DataCite sandbox</p> <p>Timeline: Milestone 1 (4 weeks)</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#step-2-contact-datacite-week-4-5","title":"Step 2: Contact DataCite (Week 4-5)","text":"<p>Action: Email support@datacite.org</p> <p>Email Template: <pre><code>Subject: Application to Become DataCite Registered Service Provider - Cicada\n\nDear DataCite Team,\n\nI am writing to apply for Cicada to become a DataCite Registered Service Provider.\n\nAbout Cicada:\n- Open-source CLI tool for scientific data management\n- Written in Go, available on GitHub: https://github.com/scttfrdmn/cicada\n- Target users: Research labs, bioinformatics facilities, microscopy cores\n- Version 0.3.0 will include full DataCite REST API v2 integration\n\nCurrent Implementation Status:\n\u2705 DataCite Metadata Schema v4.5 support\n\u2705 REST API v2 client implementation\n\u2705 Sandbox environment testing\n\u2705 Security best practices (credential handling, HTTPS)\n\u2705 Comprehensive documentation\n\nWe have successfully:\n- Registered test DOIs in the sandbox environment\n- Implemented all CRUD operations\n- Validated metadata compliance\n- Created user documentation for DataCite integration\n\nWe would like to schedule a verification call to demonstrate our integration\nand complete the registration process.\n\nBest regards,\n[Your Name]\nCicada Project Lead\n</code></pre></p> <p>Expected Response: DataCite will review and schedule verification call</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#step-3-verification-call-week-5-6","title":"Step 3: Verification Call (Week 5-6)","text":"<p>What to Demonstrate: 1. Live demo of Cicada registering DOI via DataCite sandbox 2. Show metadata validation 3. Show error handling 4. Show credential security 5. Walk through documentation</p> <p>Preparation: - Have sandbox credentials ready - Prepare test data file - Have documentation open - Rehearse demo flow</p> <p>Demo Script: <pre><code># 1. Show version and provider support\ncicada version\ncicada doi provider list\n\n# 2. Show metadata extraction\ncicada metadata extract sample.fastq --preset illumina-novaseq\n\n# 3. Show DOI preparation (without publishing)\ncicada doi prepare sample.fastq \\\n  --enrich metadata.yaml \\\n  --publisher \"Test Lab\" \\\n  --output doi-metadata.json\n\n# 4. Show validation\ncat doi-metadata.json | jq '.'\n\n# 5. Publish to sandbox\ncicada doi publish sample.fastq \\\n  --enrich metadata.yaml \\\n  --provider datacite \\\n  --publisher \"Test Lab\" \\\n  --datacite-repository-id &lt;SANDBOX_ID&gt; \\\n  --datacite-password &lt;SANDBOX_PASSWORD&gt; \\\n  --datacite-sandbox\n\n# 6. Show registered DOI\ncicada doi status &lt;DOI&gt;\n</code></pre></p>"},{"location":"DATACITE_SERVICE_PROVIDER/#step-4-complete-registration-form-week-6","title":"Step 4: Complete Registration Form (Week 6)","text":"<p>Information Needed: - Application name: Cicada - Version: 0.3.0 - Website: https://github.com/scttfrdmn/cicada - Documentation URL: https://github.com/scttfrdmn/cicada/blob/main/docs/DOI_PUBLISHING.md - Contact email: [Your email] - API version: REST API v2 - Supported schema: DataCite Metadata Schema v4.5 - License: MIT (or whatever Cicada uses) - Description: \"Open-source CLI tool for scientific data management and DOI registration. Supports automated metadata extraction from scientific file formats and DOI minting through multiple providers including DataCite and Zenodo.\"</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#step-5-receive-certification-week-7","title":"Step 5: Receive Certification (Week 7)","text":"<p>What You Get: - \u2705 Official listing on DataCite website - \u2705 \"DataCite Registered Service Provider\" badge - \u2705 Logo usage permission - \u2705 Direct support contact at DataCite - \u2705 Early notice of API changes</p> <p>What to Do: - Add badge to README.md - Update documentation with official status - Announce in release notes - Add to project website</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#cost-comparison-three-scenarios","title":"Cost Comparison: Three Scenarios","text":""},{"location":"DATACITE_SERVICE_PROVIDER/#scenario-a-registered-service-provider-recommended","title":"Scenario A: Registered Service Provider (RECOMMENDED)","text":"<p>Cost to Cicada: $0</p> <p>Cost to Users: Their existing DataCite membership - Direct: \u20ac2,000/year + per-DOI fees - Consortium (Lyrasis): \\(1,625-\\)3,600/year - Already have it: $0 additional</p> <p>Pros: - \u2705 No cost to Cicada - \u2705 Scales to unlimited users - \u2705 Users control their own DOIs - \u2705 Official DataCite certification - \u2705 Best for open-source model</p> <p>Cons: - \u26a0\ufe0f Users must have DataCite membership - \u26a0\ufe0f More complex credential management - \u26a0\ufe0f Can't provide \"free\" DataCite DOIs to users</p> <p>Best for: - Open-source projects - Tools for institutional users - Multi-tenant scenarios</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#scenario-b-direct-institutional-membership","title":"Scenario B: Direct Institutional Membership","text":"<p>Cost to Cicada: \u20ac2,000-\u20ac5,000/year</p> <p>Cost to Users: $0 (Cicada provides DOIs)</p> <p>Pros: - \u2705 Can provide \"free\" DOIs to users - \u2705 Simple credential management - \u2705 Full control</p> <p>Cons: - \u274c Expensive for open-source project - \u274c Doesn't scale (volume pricing) - \u274c Ongoing maintenance cost - \u274c Single point of failure - \u274c All DOIs under Cicada's prefix (user ownership unclear)</p> <p>Best for: - Commercial services - Institutional tools with dedicated budget - Low-volume use cases</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#scenario-c-consortium-membership-temporary","title":"Scenario C: Consortium Membership (TEMPORARY)","text":"<p>Cost to Cicada: $1,625/year (Lyrasis, up to 2,000 DOIs)</p> <p>Cost to Users: $0 (Cicada provides DOIs)</p> <p>Pros: - \u2705 Lower cost than direct membership - \u2705 Good for early testing/development - \u2705 Can provide DOIs to beta testers</p> <p>Cons: - \u26a0\ufe0f Still has annual cost - \u26a0\ufe0f Volume limits - \u26a0\ufe0f Not sustainable long-term - \u26a0\ufe0f US-specific (Lyrasis)</p> <p>Best for: - Early development phase - Beta testing with real DOIs - Temporary solution before becoming Service Provider</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#recommended-strategy-for-cicada","title":"Recommended Strategy for Cicada","text":""},{"location":"DATACITE_SERVICE_PROVIDER/#phase-1-sandbox-development-weeks-1-4-free","title":"Phase 1: Sandbox Development (Weeks 1-4) - FREE","text":"<p>Goal: Build and test DataCite integration</p> <p>Approach: 1. Use DataCite sandbox (no membership required) 2. Implement all API features (Issues #26, #27, #30) 3. Write integration tests (Issue #36) 4. Document provider setup (Issue #37)</p> <p>Cost: $0</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#phase-2-verification-weeks-5-7-free","title":"Phase 2: Verification (Weeks 5-7) - FREE","text":"<p>Goal: Become Registered Service Provider</p> <p>Approach: 1. Contact DataCite (support@datacite.org) 2. Schedule verification call 3. Demonstrate integration 4. Complete registration form 5. Receive certification</p> <p>Cost: $0</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#phase-3-production-use-v030-release-free","title":"Phase 3: Production Use (v0.3.0 Release) - FREE","text":"<p>Goal: Launch with official DataCite support</p> <p>Approach: 1. Document credential requirements for users 2. Provide setup guides for different institution types 3. Include Service Provider badge in documentation 4. Support users with their own credentials</p> <p>Cost: $0 (users bring their own DataCite membership)</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#optional-early-testing-with-real-dois-week-4-6","title":"Optional: Early Testing with Real DOIs (Week 4-6)","text":"<p>If you need production DOIs before becoming Service Provider:</p> <p>Option A: Personal/Institutional Account - If you have access to a DataCite account (university, etc.) - Use for early testing only - Cost: $0 (already have it)</p> <p>Option B: Lyrasis Consortium Membership - Sign up at https://www.lyrasis.org/programs/Pages/DataCite-US-Community-Membership.aspx - $1,625/year (1-1,999 DOIs) - Use for beta testing, then transition to Service Provider - Cost: $1,625 (one-time or temporary)</p> <p>Recommendation: Not necessary - sandbox is sufficient for development and certification</p>"},{"location":"DATACITE_SERVICE_PROVIDER/#user-documentation-strategy","title":"User Documentation Strategy","text":""},{"location":"DATACITE_SERVICE_PROVIDER/#for-institutional-users-most-common","title":"For Institutional Users (Most Common)","text":"<p>Title: Using Cicada with Your Institution's DataCite Membership</p> <p>Content: <pre><code># Using Cicada with DataCite\n\nCicada is a DataCite Registered Service Provider. If your institution has a\nDataCite membership, you can use Cicada to mint DOIs using your credentials.\n\n## Prerequisites\n\n- DataCite repository ID (prefix)\n- DataCite password\n- Your institution's DataCite membership (direct or consortium)\n\n## Getting Your Credentials\n\nContact your institution's library, IT department, or research data office.\nThey can provide:\n- Your repository ID (format: XXXX.YYYY)\n- Your password\n- Information about your DOI allocation\n\n## Configuration\n\n### Option 1: Environment Variables (Recommended)\n\nexport DATACITE_REPOSITORY_ID=\"your-repo-id\"\nexport DATACITE_PASSWORD=\"your-password\"\n\ncicada doi publish sample.fastq --provider datacite\n\n### Option 2: Command Line Flags\n\ncicada doi publish sample.fastq \\\n  --provider datacite \\\n  --datacite-repository-id your-repo-id \\\n  --datacite-password your-password\n\n### Option 3: Config File\n\nCreate ~/.cicada/config.yaml:\n\nproviders:\n  datacite:\n    repository_id: your-repo-id\n    password: your-password\n    environment: production\n\ncicada doi publish sample.fastq --provider datacite\n\n## Security Best Practices\n\n- Never commit credentials to version control\n- Use environment variables or config files with proper permissions\n- Rotate passwords regularly\n- Use separate credentials for production and testing\n</code></pre></p>"},{"location":"DATACITE_SERVICE_PROVIDER/#for-users-without-datacite","title":"For Users Without DataCite","text":"<p>Title: Getting Started with DOI Registration</p> <p>Content: <pre><code># DOI Registration Options\n\n## Option 1: Use Zenodo (FREE, Recommended for Individuals)\n\nZenodo provides free DOI registration for all users:\n\ncicada doi publish sample.fastq --provider zenodo\n\nSee docs/ZENODO_SETUP.md for details.\n\n## Option 2: DataCite via Your Institution\n\nIf you're affiliated with a university or research institution:\n\n1. Check if your institution has a DataCite membership\n2. Contact your library or IT department\n3. Request DataCite credentials\n4. Use Cicada with your institutional credentials\n\nSee docs/DATACITE_SETUP.md for details.\n\n## Option 3: DataCite Consortium Membership\n\nFor independent researchers or small labs:\n\n- US: Lyrasis ($1,625/year for up to 2,000 DOIs)\n  https://www.lyrasis.org/programs/Pages/DataCite-US-Community-Membership.aspx\n\n- UK: British Library (~\u00a31,100/year)\n  https://www.bl.uk/britishlibrary/~/media/bl/global/services/collection%20metadata/pdfs/datacite-membership-application-form.pdf\n\n- Europe: EUDAT\n  https://www.eudat.eu/catalogue/B2SHARE\n\n- Australia: ANDS/ARDC\n  https://ardc.edu.au/services/identifier/\n</code></pre></p>"},{"location":"DATACITE_SERVICE_PROVIDER/#next-steps","title":"Next Steps","text":""},{"location":"DATACITE_SERVICE_PROVIDER/#immediate-actions-this-week","title":"Immediate Actions (This Week)","text":"<ul> <li> \u2705 Research DataCite options (COMPLETE)</li> <li> \u2705 Create this decision document (COMPLETE)</li> <li> \ud83d\udc64 User Decision: Confirm Registered Service Provider approach</li> <li> \ud83d\udc64 User Action: Review Zenodo account setup (already created)</li> </ul>"},{"location":"DATACITE_SERVICE_PROVIDER/#before-starting-development-week-1","title":"Before Starting Development (Week 1)","text":"<ul> <li> Create DataCite sandbox account (free)</li> <li>URL: https://support.datacite.org/docs/testing-guide</li> <li>Sign up at: https://doi.test.datacite.org/</li> <li> Store sandbox credentials securely</li> <li> Review DataCite REST API documentation</li> <li>API Docs: https://support.datacite.org/docs/api</li> <li>Schema: https://schema.datacite.org/</li> </ul>"},{"location":"DATACITE_SERVICE_PROVIDER/#during-development-weeks-1-4","title":"During Development (Weeks 1-4)","text":"<ul> <li> Implement Issue #26: Provider Configuration System</li> <li> Implement Issue #27: DataCite API Client</li> <li> Implement Issue #30: DataCite Metadata Mapping</li> <li> Implement Issue #32: <code>doi publish</code> Command</li> <li> Implement Issue #35: Error Handling &amp; Retry Logic</li> <li> Implement Issue #36: Integration Tests (sandbox)</li> <li> Write Issue #37: Provider Documentation</li> </ul>"},{"location":"DATACITE_SERVICE_PROVIDER/#after-development-weeks-5-7","title":"After Development (Weeks 5-7)","text":"<ul> <li> Email DataCite: support@datacite.org</li> <li> Prepare verification demo</li> <li> Schedule verification call</li> <li> Complete registration form</li> <li> Receive certification</li> <li> Update documentation with badge</li> <li> Announce in v0.3.0 release notes</li> </ul>"},{"location":"DATACITE_SERVICE_PROVIDER/#optional-early-production-testing","title":"Optional: Early Production Testing","text":"<ul> <li> Decide if needed (probably not necessary)</li> <li> If yes, evaluate Lyrasis vs institutional account</li> <li> Register test account</li> <li> Use for beta testing only</li> <li> Transition to Service Provider model for release</li> </ul>"},{"location":"DATACITE_SERVICE_PROVIDER/#decision-required","title":"Decision Required","text":"<p>Question: Confirm the Registered Service Provider approach for Cicada v0.3.0?</p> <p>Recommendation: \u2705 YES - Proceed with Registered Service Provider path</p> <p>Rationale: 1. Zero cost to Cicada project 2. Scales to unlimited institutional users 3. Official DataCite certification 4. Best fit for open-source model 5. Sandbox sufficient for development</p> <p>Alternative considered: Temporary Lyrasis membership for early testing - Verdict: Not necessary - sandbox is adequate</p> <p>Next Step: Confirm decision, then begin Milestone 1 (Issue #26: Provider Configuration System)</p>"},{"location":"DOI_WORKFLOW/","title":"DOI Workflow Guide","text":"<p>Note: DOI preparation is an optional advanced feature of Cicada's data commons platform. Most labs use Cicada for daily data management, storage, and metadata extraction. DOI support is provided for labs that need to publish datasets to repositories like Zenodo or institutional data repositories.</p> <p>For core data management features, see the User Guide (coming in v0.3.0) and Metadata Extraction Guide.</p> <p>This guide covers preparing datasets for DOI (Digital Object Identifier) registration using Cicada's automated metadata mapping and validation tools. Learn how to assess DOI readiness, enrich metadata, and prepare DataCite-compliant metadata for repository submission.</p>"},{"location":"DOI_WORKFLOW/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Quick Start</li> <li>Understanding DOI Requirements</li> <li>Command Reference</li> <li>Metadata Enrichment</li> <li>Quality Scoring</li> <li>Validation Modes</li> <li>Repository Submission</li> <li>Complete Workflow Examples</li> <li>Troubleshooting</li> </ul>"},{"location":"DOI_WORKFLOW/#overview","title":"Overview","text":"<p>DOIs (Digital Object Identifiers) provide permanent, citable identifiers for research datasets that you wish to publish. This is typically the final step in your data lifecycle:</p> <pre><code>1. Generate data \u2192 2. Store &amp; sync \u2192 3. Extract metadata \u2192 4. Organize &amp; manage \u2192 5. Publish (DOI)\n   (Instrument)      (Cicada core)     (Cicada core)      (Cicada core)           (This guide)\n</code></pre> <p>Most research data stays within your lab's data commons for analysis and collaboration. You only need DOI registration when: - Publishing datasets alongside papers - Making data publicly available - Meeting funder requirements for data sharing - Archiving data in institutional repositories</p>"},{"location":"DOI_WORKFLOW/#doi-preparation-workflow","title":"DOI Preparation Workflow","text":"<p>Cicada helps prepare datasets for DOI registration by:</p> <ul> <li>Extracting base metadata from instrument files (core platform feature)</li> <li>Mapping metadata to DataCite Schema v4.5</li> <li>Validating metadata completeness and quality</li> <li>Enriching metadata with author information and descriptions</li> <li>Scoring metadata quality (0-100 scale)</li> <li>Exporting DataCite-compliant metadata for repository submission</li> </ul>"},{"location":"DOI_WORKFLOW/#benefits","title":"Benefits","text":"<p>\u2705 Leverages existing metadata: Builds on metadata already extracted for data management \u2705 Guided workflow: Know exactly what metadata is required for publication \u2705 Quality scoring: Track metadata completeness (0-100) \u2705 Standards compliance: DataCite Schema v4.5 compatible \u2705 Repository ready: Export metadata for Zenodo, Dryad, institutional repositories</p>"},{"location":"DOI_WORKFLOW/#quick-start","title":"Quick Start","text":""},{"location":"DOI_WORKFLOW/#check-doi-readiness","title":"Check DOI Readiness","text":"<pre><code># Assess current metadata quality\ncicada doi validate sample.fastq.gz\n</code></pre> <p>Output: <pre><code>DOI Validation Results\n======================\n\nFile: sample.fastq.gz\n\n\u2717 NOT READY for DOI minting\n\nQuality Score: 47.0/100 (Moderate)\n\nPresent Fields (8):\n  \u2713 identifier\n  \u2713 title\n  \u2713 publisher\n  \u2713 publication_year\n  \u2713 resource_type\n  \u2713 description\n  \u2713 license\n  \u2713 keywords\n\nMissing Fields (9):\n  \u2717 real creator names\n  \u2717 url\n  \u2717 author ORCIDs\n  \u2717 author affiliations\n  ...\n\nRecommendations:\n  Fix errors before minting DOI:\n    - Add real author names with ORCIDs\n    - Enhance description with methodology\n</code></pre></p>"},{"location":"DOI_WORKFLOW/#prepare-doi-with-enrichment","title":"Prepare DOI with Enrichment","text":"<pre><code># Prepare with enriched metadata\ncicada doi prepare sample.fastq.gz \\\n  --enrich enrichment.yaml \\\n  --publisher \"University Lab\"\n</code></pre> <p>Output: <pre><code>DOI Preparation Results\n=======================\n\nFile: sample.fastq.gz\n\nDataset Information:\n  Title: My Research Dataset\n  Authors: 2 (both with ORCIDs)\n  Quality Score: 91.0/100 (Excellent)\n\n\u2713 Ready for DOI minting\n</code></pre></p>"},{"location":"DOI_WORKFLOW/#understanding-doi-requirements","title":"Understanding DOI Requirements","text":""},{"location":"DOI_WORKFLOW/#datacite-required-fields","title":"DataCite Required Fields","text":"<p>These fields are mandatory for DOI registration:</p> Field Description How Cicada Provides Identifier Unique identifier Auto-generated or from filename Creators Authors/creators From enrichment file Titles Dataset title From enrichment file or filename Publisher Publishing entity From <code>--publisher</code> flag Publication Year Year of publication Current year or from enrichment Resource Type Type of resource Auto-detected (\"Dataset\")"},{"location":"DOI_WORKFLOW/#datacite-recommended-fields","title":"DataCite Recommended Fields","text":"<p>These fields are highly recommended for discovery and attribution:</p> Field Description Impact on Quality Score Subjects Keywords/topics +5 points Contributors Other contributors +3 points Dates Relevant dates +3 points Related Identifiers Related DOIs/URLs +5 points Descriptions Detailed descriptions +10 points Geo Locations Geographic coverage +3 points Language Primary language +2 points Sizes Data sizes Auto-extracted Formats File formats Auto-extracted Version Dataset version +2 points Rights License information +5 points Funding References Grant information +5 points"},{"location":"DOI_WORKFLOW/#quality-score-calculation","title":"Quality Score Calculation","text":"<ul> <li>0-59: Not ready for DOI (fix errors first)</li> <li>60-79: Acceptable (minimum for DOI registration)</li> <li>80-89: Good (recommended for publication)</li> <li>90-100: Excellent (comprehensive metadata)</li> </ul>"},{"location":"DOI_WORKFLOW/#command-reference","title":"Command Reference","text":""},{"location":"DOI_WORKFLOW/#cicada-doi-validate","title":"<code>cicada doi validate</code>","text":"<p>Validate metadata for DOI readiness without preparing.</p>"},{"location":"DOI_WORKFLOW/#syntax","title":"Syntax","text":"<pre><code>cicada doi validate &lt;file&gt; [flags]\n</code></pre>"},{"location":"DOI_WORKFLOW/#flags","title":"Flags","text":"Flag Type Default Description <code>--format</code> string <code>table</code> Output format: <code>table</code>, <code>json</code>, <code>yaml</code> <code>--min-score</code> float <code>60.0</code> Minimum quality score threshold"},{"location":"DOI_WORKFLOW/#examples","title":"Examples","text":"<pre><code># Basic validation\ncicada doi validate data.fastq.gz\n\n# JSON output for parsing\ncicada doi validate data.fastq.gz --format json\n\n# Require higher quality score\ncicada doi validate data.fastq.gz --min-score 80\n</code></pre>"},{"location":"DOI_WORKFLOW/#cicada-doi-prepare","title":"<code>cicada doi prepare</code>","text":"<p>Prepare metadata for DOI registration with enrichment.</p>"},{"location":"DOI_WORKFLOW/#syntax_1","title":"Syntax","text":"<pre><code>cicada doi prepare &lt;file&gt; [flags]\n</code></pre>"},{"location":"DOI_WORKFLOW/#flags_1","title":"Flags","text":"Flag Type Default Description <code>--enrich</code> string - Enrichment metadata file (YAML or JSON) <code>--publisher</code> string - Publisher name (required) <code>--license</code> string <code>CC-BY-4.0</code> License identifier <code>--preset</code> string - Instrument preset for validation <code>--format</code> string <code>table</code> Output format: <code>table</code>, <code>json</code>, <code>yaml</code> <code>--output</code> string stdout Output file path"},{"location":"DOI_WORKFLOW/#examples_1","title":"Examples","text":"<pre><code># Prepare with enrichment\ncicada doi prepare data.fastq.gz \\\n  --enrich metadata.yaml \\\n  --publisher \"State University Lab\"\n\n# Save DataCite JSON\ncicada doi prepare data.fastq.gz \\\n  --enrich metadata.yaml \\\n  --publisher \"Lab Name\" \\\n  --format json \\\n  --output datacite.json\n\n# With preset validation\ncicada doi prepare data.fastq.gz \\\n  --enrich metadata.yaml \\\n  --publisher \"Lab\" \\\n  --preset illumina-novaseq\n\n# Different license\ncicada doi prepare data.fastq.gz \\\n  --enrich metadata.yaml \\\n  --publisher \"Lab\" \\\n  --license CC0-1.0\n</code></pre>"},{"location":"DOI_WORKFLOW/#metadata-enrichment","title":"Metadata Enrichment","text":""},{"location":"DOI_WORKFLOW/#creating-an-enrichment-file","title":"Creating an Enrichment File","text":"<p>Enrichment files provide additional metadata not extractable from files.</p>"},{"location":"DOI_WORKFLOW/#yaml-format-recommended","title":"YAML Format (Recommended)","text":"<pre><code># enrichment.yaml\n\ntitle: \"Whole genome sequencing of antibiotic-resistant bacteria\"\n\nauthors:\n  - name: Dr. Jane Smith\n    orcid: 0000-0002-1234-5678\n    affiliation: Department of Microbiology, State University\n  - name: Dr. John Doe\n    orcid: 0000-0003-9876-5432\n    affiliation: Department of Bioinformatics, State University\n    role: supervisor\n\ndescription: |\n  This dataset contains whole genome sequencing data from 50 clinical\n  isolates of antibiotic-resistant bacteria collected from hospital\n  patients between 2023-2024.\n\n  Methodology:\n  - DNA extraction: Qiagen DNeasy Kit\n  - Sequencing: Illumina NovaSeq 6000, 2x150bp paired-end\n  - Coverage: Average 100x per isolate\n  - Quality filtering: fastp v0.23.0 (Q30)\n\n  Data includes raw FASTQ files and quality control reports.\n\nkeywords:\n  - whole genome sequencing\n  - antibiotic resistance\n  - bacterial genomics\n  - clinical isolates\n  - antimicrobial resistance genes\n\npublisher: State University Genomics Core\n\nlicense: CC-BY-4.0\n\nfunding_references:\n  - funder_name: National Institutes of Health\n    award_number: R01AI123456\n  - funder_name: University Research Foundation\n    award_number: URF-2024-789\n\nrelated_identifiers:\n  - identifier: \"10.1234/journal.2025.456\"\n    relation: IsSupplementTo\n    type: DOI\n    description: \"Associated publication\"\n\ntemporal_coverage:\n  start: \"2023-01-01\"\n  end: \"2024-12-31\"\n\nversion: \"1.0\"\n\nlanguage: en\n</code></pre>"},{"location":"DOI_WORKFLOW/#json-format","title":"JSON Format","text":"<pre><code>{\n  \"title\": \"My Research Dataset\",\n  \"authors\": [\n    {\n      \"name\": \"Dr. Jane Smith\",\n      \"orcid\": \"0000-0002-1234-5678\",\n      \"affiliation\": \"State University\"\n    }\n  ],\n  \"description\": \"Dataset description with methodology...\",\n  \"keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"],\n  \"funding_references\": [\n    {\n      \"funder_name\": \"NSF\",\n      \"award_number\": \"NSF-123456\"\n    }\n  ]\n}\n</code></pre>"},{"location":"DOI_WORKFLOW/#enrichment-file-fields","title":"Enrichment File Fields","text":""},{"location":"DOI_WORKFLOW/#required-in-enrichment","title":"Required in Enrichment","text":"Field Type Description Example <code>title</code> string Dataset title <code>\"RNA-seq analysis of...\"</code> <code>authors</code> array List of authors See below <code>description</code> string Detailed description Multi-line text"},{"location":"DOI_WORKFLOW/#author-object","title":"Author Object","text":"<pre><code>authors:\n  - name: Dr. Jane Smith         # Required\n    orcid: 0000-0002-1234-5678  # Highly recommended\n    affiliation: State University  # Recommended\n    role: principal investigator   # Optional\n</code></pre>"},{"location":"DOI_WORKFLOW/#recommended-in-enrichment","title":"Recommended in Enrichment","text":"Field Type Description <code>keywords</code> array 5-10 keywords <code>funding_references</code> array Funding sources <code>related_identifiers</code> array Related DOIs/URLs <code>temporal_coverage</code> object Time period covered <code>version</code> string Dataset version <code>language</code> string Primary language (ISO 639-1)"},{"location":"DOI_WORKFLOW/#quality-scoring","title":"Quality Scoring","text":""},{"location":"DOI_WORKFLOW/#score-breakdown","title":"Score Breakdown","text":"<p>Quality scores are calculated based on metadata completeness:</p> <pre><code>Total Score = Base Score + Optional Score\n\nBase Score (60 points):\n  - Required fields present: 60 points\n  - Any required field missing: 0 points\n\nOptional Score (40 points):\n  - Each optional field present: varies by importance\n  - Author ORCIDs: +5 points\n  - Detailed description: +10 points\n  - Funding information: +5 points\n  - Related publications: +5 points\n  - Keywords (5+): +5 points\n  - ... (other optional fields)\n</code></pre>"},{"location":"DOI_WORKFLOW/#improving-your-score","title":"Improving Your Score","text":"Current Score Actions to Improve 0-40 Add required fields: title, authors, description 40-60 Complete all required fields 60-70 Add author ORCIDs and affiliations 70-80 Add funding information and keywords 80-90 Add related publications and detailed methodology 90-100 Add temporal/spatial coverage, version info"},{"location":"DOI_WORKFLOW/#example-score-progression","title":"Example Score Progression","text":"<pre><code># Initial validation (no enrichment)\ncicada doi validate data.fastq.gz\n# Quality Score: 47.0/100 (Moderate)\n\n# Add basic enrichment (title, authors, description)\ncicada doi prepare data.fastq.gz --enrich basic.yaml\n# Quality Score: 68.0/100 (Acceptable)\n\n# Add ORCIDs and funding\ncicada doi prepare data.fastq.gz --enrich enhanced.yaml\n# Quality Score: 85.0/100 (Good)\n\n# Add related publications and full methodology\ncicada doi prepare data.fastq.gz --enrich complete.yaml\n# Quality Score: 94.0/100 (Excellent)\n</code></pre>"},{"location":"DOI_WORKFLOW/#validation-modes","title":"Validation Modes","text":""},{"location":"DOI_WORKFLOW/#lenient-mode-default","title":"Lenient Mode (Default)","text":"<p>Allows DOI preparation with warnings: - Score must be \u2265 60 - Required fields must be present - Warnings don't block preparation</p> <p>Use when: Preparing for internal/initial DOI registration</p>"},{"location":"DOI_WORKFLOW/#strict-mode","title":"Strict Mode","text":"<p>Requires high-quality metadata: - Score must be \u2265 80 (configurable) - All warnings should be addressed - Best practices enforced</p> <p>Use when: Preparing for publication or public repositories</p> <pre><code># Strict validation\ncicada doi prepare data.fastq.gz \\\n  --enrich metadata.yaml \\\n  --publisher \"Lab\" \\\n  --min-score 80\n</code></pre>"},{"location":"DOI_WORKFLOW/#repository-submission","title":"Repository Submission","text":""},{"location":"DOI_WORKFLOW/#supported-repositories","title":"Supported Repositories","text":"<p>Cicada prepares metadata for any DataCite-compatible repository:</p> Repository DOI Provider Test Environment Cost Zenodo DataCite \u2705 sandbox.zenodo.org Free Dryad DataCite \u2705 Test instance $120/dataset* Figshare DataCite \u2705 Test instance Free (&lt; 20GB) Mendeley Data DataCite Limited Free Institutional Repositories Varies Varies Varies <p>*Some institutions have subscriptions</p>"},{"location":"DOI_WORKFLOW/#zenodo-submission-workflow","title":"Zenodo Submission Workflow","text":"<ol> <li> <p>Prepare metadata:    <pre><code>cicada doi prepare data.fastq.gz \\\n  --enrich enrichment.yaml \\\n  --publisher \"Your Institution\" \\\n  --format json \\\n  --output datacite.json\n</code></pre></p> </li> <li> <p>Upload to Zenodo:</p> </li> <li>Go to https://zenodo.org (or https://sandbox.zenodo.org for testing)</li> <li>Click \"New upload\"</li> <li> <p>Upload your data files</p> </li> <li> <p>Import metadata:</p> </li> <li>Click \"Import\" \u2192 \"DataCite JSON\"</li> <li>Upload <code>datacite.json</code></li> <li> <p>Review and edit if needed</p> </li> <li> <p>Add landing page URL:</p> </li> <li> <p>Will be provided by Zenodo after publication</p> </li> <li> <p>Publish:</p> </li> <li>Review all fields</li> <li>Click \"Publish\"</li> <li>Get your DOI!</li> </ol>"},{"location":"DOI_WORKFLOW/#dryad-submission-workflow","title":"Dryad Submission Workflow","text":"<ol> <li> <p>Prepare metadata:    <pre><code>cicada doi prepare data.fastq.gz \\\n  --enrich enrichment.yaml \\\n  --publisher \"Your Institution\" \\\n  --format json \\\n  --output datacite.json\n</code></pre></p> </li> <li> <p>Create Dryad submission:</p> </li> <li>Go to https://datadryad.org</li> <li>Click \"Submit Data\"</li> <li> <p>Upload files</p> </li> <li> <p>Fill metadata form:</p> </li> <li>Copy information from <code>datacite.json</code></li> <li> <p>Dryad web form is user-friendly</p> </li> <li> <p>Submit for curation:</p> </li> <li>Dryad curators review submission</li> <li> <p>May request improvements</p> </li> <li> <p>Publication:</p> </li> <li>After approval, DOI is minted</li> <li>Dataset becomes publicly available</li> </ol>"},{"location":"DOI_WORKFLOW/#complete-workflow-examples","title":"Complete Workflow Examples","text":""},{"location":"DOI_WORKFLOW/#example-1-sequencing-data-for-publication","title":"Example 1: Sequencing Data for Publication","text":"<pre><code># Step 1: Extract base metadata\ncicada metadata extract sample_R1.fastq.gz --output base_metadata.json\n\n# Step 2: Check initial quality\ncicada doi validate sample_R1.fastq.gz\n\n# Output: Quality Score: 47.0/100 - need enrichment\n\n# Step 3: Create enrichment file\ncat &gt; enrichment.yaml &lt;&lt;'EOF'\ntitle: \"RNA-seq analysis of drug resistance in cancer cells\"\nauthors:\n  - name: Dr. Sarah Chen\n    orcid: 0000-0002-1234-5678\n    affiliation: Cancer Biology Department, State University\ndescription: |\n  RNA-seq data from drug-resistant cancer cell lines...\n  [detailed methodology]\nkeywords: [RNA-seq, cancer, drug resistance, cell lines]\nfunding_references:\n  - funder_name: National Cancer Institute\n    award_number: CA123456\nrelated_identifiers:\n  - identifier: \"10.1101/2025.123456\"\n    relation: IsSupplementTo\n    type: DOI\nEOF\n\n# Step 4: Prepare DOI\ncicada doi prepare sample_R1.fastq.gz \\\n  --enrich enrichment.yaml \\\n  --publisher \"State University Genomics Core\" \\\n  --format json \\\n  --output datacite.json\n\n# Output: Quality Score: 91.0/100 - Ready!\n\n# Step 5: Submit to Zenodo\n# Upload files and datacite.json via web interface\n\n# Step 6: Get DOI and cite in paper\n# DOI: 10.5281/zenodo.123456\n</code></pre>"},{"location":"DOI_WORKFLOW/#example-2-batch-doi-preparation","title":"Example 2: Batch DOI Preparation","text":"<pre><code>#!/bin/bash\n# prepare_dois.sh - Prepare DOI metadata for multiple files\n\nENRICHMENT=\"project_metadata.yaml\"\nPUBLISHER=\"University Research Lab\"\n\nfor file in data/*.fastq.gz; do\n  basename=$(basename \"$file\" .fastq.gz)\n\n  echo \"Processing: $basename\"\n\n  # Prepare DOI\n  cicada doi prepare \"$file\" \\\n    --enrich \"$ENRICHMENT\" \\\n    --publisher \"$PUBLISHER\" \\\n    --format json \\\n    --output \"doi_metadata/${basename}.datacite.json\"\n\n  if [ $? -eq 0 ]; then\n    echo \"  \u2713 Ready for DOI\"\n  else\n    echo \"  \u2717 Failed - check metadata\"\n  fi\ndone\n\necho \"DOI preparation complete\"\necho \"Upload datacite.json files to repository\"\n</code></pre>"},{"location":"DOI_WORKFLOW/#example-3-incremental-improvement","title":"Example 3: Incremental Improvement","text":"<pre><code># Start with minimal metadata\ncat &gt; basic.yaml &lt;&lt;EOF\ntitle: \"My Dataset\"\nauthors:\n  - name: John Doe\ndescription: \"Research data\"\nEOF\n\ncicada doi prepare data.fastq.gz --enrich basic.yaml --publisher \"Lab\"\n# Score: 68/100 - needs improvement\n\n# Add ORCIDs and better description\ncat &gt; improved.yaml &lt;&lt;EOF\ntitle: \"Whole genome sequencing of E. coli strains\"\nauthors:\n  - name: Dr. John Doe\n    orcid: 0000-0002-1234-5678\n    affiliation: Microbiology Department\ndescription: |\n  Complete methodology:\n  - Sample collection and preparation\n  - Sequencing platform and parameters\n  - Data processing pipeline\nkeywords: [genomics, bacteria, sequencing]\nEOF\n\ncicada doi prepare data.fastq.gz --enrich improved.yaml --publisher \"Lab\"\n# Score: 85/100 - much better!\n\n# Add funding and related work\ncat &gt; complete.yaml &lt;&lt;EOF\ntitle: \"Whole genome sequencing of E. coli strains\"\nauthors:\n  - name: Dr. John Doe\n    orcid: 0000-0002-1234-5678\n    affiliation: Microbiology Department\ndescription: |\n  [Complete methodology as above]\nkeywords: [genomics, bacteria, sequencing, comparative genomics]\nfunding_references:\n  - funder_name: National Institutes of Health\n    award_number: R01GM123456\nrelated_identifiers:\n  - identifier: \"10.1234/journal.2025.789\"\n    relation: IsSupplementTo\n    type: DOI\nversion: \"1.0\"\nEOF\n\ncicada doi prepare data.fastq.gz --enrich complete.yaml --publisher \"Lab\"\n# Score: 94/100 - excellent!\n</code></pre>"},{"location":"DOI_WORKFLOW/#troubleshooting","title":"Troubleshooting","text":""},{"location":"DOI_WORKFLOW/#low-quality-score","title":"Low Quality Score","text":"<p>Problem: Score is below 60</p> <p>Solutions: 1. Check for required fields:    <pre><code>cicada doi validate file.fastq.gz --format json | jq '.errors'\n</code></pre></p> <ol> <li> <p>Add missing required fields to enrichment file</p> </li> <li> <p>Validate again to see improvement</p> </li> </ol>"},{"location":"DOI_WORKFLOW/#author-orcid-issues","title":"Author ORCID Issues","text":"<p>Problem: ORCIDs not recognized or invalid</p> <p>Solutions: 1. Verify ORCID format: <code>0000-0002-1234-5678</code> (16 digits with dashes)</p> <ol> <li> <p>Check ORCID is real: https://orcid.org/0000-0002-1234-5678</p> </li> <li> <p>Use correct field in enrichment:    <pre><code>authors:\n  - name: Dr. Jane Smith\n    orcid: 0000-0002-1234-5678  # No \"https://\" prefix\n</code></pre></p> </li> </ol>"},{"location":"DOI_WORKFLOW/#validation-fails","title":"Validation Fails","text":"<p>Problem: <code>Error: validation failed: 1 errors</code></p> <p>Cause: Required field is missing or invalid</p> <p>Solution: 1. Read error message carefully 2. Check enrichment file has the field 3. Verify field format is correct 4. Re-run preparation</p>"},{"location":"DOI_WORKFLOW/#publisher-not-set","title":"Publisher Not Set","text":"<p>Problem: <code>Error: publisher is required</code></p> <p>Solution: Always provide <code>--publisher</code> flag: <pre><code>cicada doi prepare file.fastq.gz \\\n  --enrich metadata.yaml \\\n  --publisher \"Your Institution Name\"\n</code></pre></p>"},{"location":"DOI_WORKFLOW/#related-identifier-format","title":"Related Identifier Format","text":"<p>Problem: Related identifier rejected</p> <p>Solution: Use correct format: <pre><code>related_identifiers:\n  - identifier: \"10.1234/journal.2025.123\"  # DOI without https://\n    relation: IsSupplementTo  # Use DataCite relation types\n    type: DOI  # Use DataCite identifier types\n</code></pre></p> <p>Valid relation types: - <code>IsSupplementTo</code> - This dataset supplements a publication - <code>IsPartOf</code> - This dataset is part of a larger collection - <code>IsCitedBy</code> - This dataset is cited by a publication - <code>Cites</code> - This dataset cites another work - See DataCite documentation for full list</p>"},{"location":"DOI_WORKFLOW/#best-practices","title":"Best Practices","text":""},{"location":"DOI_WORKFLOW/#1-start-early","title":"1. Start Early","text":"<p>Begin DOI preparation during data collection: - Draft enrichment file as you work - Easier to document while fresh in mind - Can identify missing information early</p>"},{"location":"DOI_WORKFLOW/#2-use-version-control","title":"2. Use Version Control","text":"<p>Track enrichment files in git: <pre><code>git add enrichment.yaml\ngit commit -m \"Add DOI metadata for dataset\"\n</code></pre></p>"},{"location":"DOI_WORKFLOW/#3-reuse-enrichment-templates","title":"3. Reuse Enrichment Templates","text":"<p>Create templates for your lab: <pre><code># lab_template.yaml\npublisher: State University Genomics Core\nlicense: CC-BY-4.0\nfunding_references:\n  - funder_name: National Science Foundation\n    award_number: NSF-XXXXX  # Update per project\n</code></pre></p>"},{"location":"DOI_WORKFLOW/#4-validate-before-submission","title":"4. Validate Before Submission","text":"<p>Always validate before repository submission: <pre><code>cicada doi prepare data.fastq.gz \\\n  --enrich metadata.yaml \\\n  --publisher \"Lab\" \\\n  --min-score 80  # Require high quality\n</code></pre></p>"},{"location":"DOI_WORKFLOW/#5-document-everything","title":"5. Document Everything","text":"<p>Include in your enrichment: - Detailed methodology - Data processing steps - Quality control procedures - Known limitations - Related publications</p>"},{"location":"DOI_WORKFLOW/#next-steps","title":"Next Steps","text":"<ul> <li>Extract metadata: See METADATA_EXTRACTION.md</li> <li>Use presets: See PRESETS.md</li> <li>Provider setup: See PROVIDERS.md</li> <li>User scenarios: See USER_SCENARIOS_v0.2.0.md</li> </ul>"},{"location":"DOI_WORKFLOW/#support","title":"Support","text":"<p>Questions or Issues? - \ud83d\udcd6 Full documentation: README.md - \ud83d\udc1b Report bugs: GitHub Issues - \ud83d\udcac Discussions: GitHub Discussions</p>"},{"location":"GETTING_STARTED_CREDENTIALS/","title":"Getting Started with DOI Publishing","text":"<p>Setting up Cicada to publish DOIs for your research data</p> <p>For researchers and lab managers with minimal technical experience</p> <p>This guide will help you set up Cicada to publish DOIs (Digital Object Identifiers) for your research data. No technical background required - just follow the steps!</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#what-should-get-a-doi","title":"What Should Get a DOI?","text":"<p>Before setting up credentials, it's helpful to understand what research outputs typically get DOIs and what your field expects.</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#what-is-a-doi","title":"What is a DOI?","text":"<p>A DOI (Digital Object Identifier) is a permanent identifier for your research data - like a permanent web address that never breaks. Even if you move your data to a different server or repository, the DOI always points to the correct location.</p> <p>Example DOI: <code>10.5281/zenodo.123456</code> - Anyone can click this and find your data - It works forever, even if URLs change - It's citable in publications</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#what-typically-gets-a-doi","title":"What Typically Gets a DOI","text":"<p>\u2705 Should have a DOI:</p> <p>Research Data: - Raw data that supports a publication - Datasets that others might reuse - Data required by journal or funder policies - Data that you want to cite in papers</p> <p>Examples: - Sequencing data (FASTQ, BAM files) - Microscopy images (CZI, OME-TIFF files) - Tabular data (CSV, Excel files) - Simulation outputs - Survey data</p> <p>Software &amp; Code: - Research software releases - Analysis scripts and pipelines - Computational models</p> <p>Other Research Outputs: - Protocols - Supplementary materials - Technical reports - Preprints (though use preprint servers like bioRxiv)</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#what-typically-doesnt-need-a-doi","title":"What Typically Doesn't Need a DOI","text":"<p>\u274c Usually doesn't need a DOI:</p> <ul> <li>Intermediate/temporary files - Processing outputs that aren't final</li> <li>Duplicate data - Data already published elsewhere with a DOI</li> <li>Preliminary data - Still collecting or analyzing</li> <li>Personal notes - Lab notebooks, internal documentation</li> <li>Already published data - Available in established databases (NCBI, ENA, etc.)</li> <li>Tiny test files - Sample data for tutorials</li> </ul>"},{"location":"GETTING_STARTED_CREDENTIALS/#domain-specific-practices","title":"Domain-Specific Practices","text":"<p>Different research fields have different expectations and norms:</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#genomics-bioinformatics","title":"Genomics &amp; Bioinformatics","text":"<p>Common practice: - Raw sequencing data (FASTQ) \u2192 Submit to NCBI SRA/ENA (they provide accessions) - Assembled genomes \u2192 Submit to NCBI GenBank - Supporting data (metadata, analysis results) \u2192 DOI via Zenodo/DataCite - Software/pipelines \u2192 DOI via Zenodo + GitHub release</p> <p>When to use Cicada: - Small-scale sequencing projects not in public databases - Supplementary data for publications - Custom reference datasets - Analysis pipelines and scripts</p> <p>Example: <pre><code># Raw reads submitted to SRA (gets accession: SRR123456)\n# Supplementary metadata gets DOI via Cicada:\ncicada doi publish sample_metadata.csv quality_metrics.csv \\\n  --provider zenodo \\\n  --enrich metadata.yaml\n</code></pre></p>"},{"location":"GETTING_STARTED_CREDENTIALS/#microscopy-imaging","title":"Microscopy &amp; Imaging","text":"<p>Common practice: - Publication-quality images \u2192 DOI via institutional repository or Zenodo - Large imaging datasets \u2192 Specialized repositories (IDR, BioImage Archive) - Instrument calibration data \u2192 DOI for reproducibility - Image analysis workflows \u2192 DOI for transparency</p> <p>When to use Cicada: - Original microscopy files (CZI, OME-TIFF) with metadata - Supporting images for papers - Method validation datasets - Training datasets for analysis</p> <p>Example: <pre><code># Publish microscopy dataset with DOI\ncicada doi publish experiment_*.czi \\\n  --preset zeiss-lsm-880 \\\n  --provider zenodo \\\n  --enrich metadata.yaml\n</code></pre></p>"},{"location":"GETTING_STARTED_CREDENTIALS/#structural-biology-chemistry","title":"Structural Biology &amp; Chemistry","text":"<p>Common practice: - Crystal structures \u2192 PDB (Protein Data Bank) - NMR data \u2192 BMRB - Supporting data \u2192 DOI via Zenodo/DataCite - Protocols \u2192 DOI via protocols.io or Zenodo</p> <p>When to use Cicada: - Supplementary crystallography data - Raw diffraction images - Validation datasets - Custom structure libraries</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#ecology-environmental-science","title":"Ecology &amp; Environmental Science","text":"<p>Common practice: - Long-term datasets \u2192 Domain repositories (LTER, DataONE) - Species occurrence data \u2192 GBIF - Climate/sensor data \u2192 Specialized repositories - Small studies \u2192 DOI via Zenodo/DataCite</p> <p>When to use Cicada: - Field study data - Sensor/instrument data - Observation records - Derived datasets</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#social-sciences-humanities","title":"Social Sciences &amp; Humanities","text":"<p>Common practice: - Survey data \u2192 Domain repositories (ICPSR, UK Data Service) - Qualitative data \u2192 Institutional repositories + DOI - Interview transcripts \u2192 Repository + DOI - Datasets for replication \u2192 DOI required</p> <p>When to use Cicada: - Anonymized survey data - Supplementary data for publications - Replication packages - Coded datasets</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#publisher-funder-requirements","title":"Publisher &amp; Funder Requirements","text":"<p>Many journals and funding agencies now require data DOIs:</p> <p>Publishers with data policies: - Nature journals - \"Data availability statement\" with DOIs - PLOS - Data deposition required - eLife - Data must be accessible with identifiers - Cell Press - Data availability required</p> <p>Funders requiring data sharing: - NIH - Data Management and Sharing Policy - NSF - Data sharing plans required - Wellcome Trust - Data must be accessible - European Commission (Horizon) - Open data by default</p> <p>Check your requirements: 1. Journal submission guidelines (\"Data Availability\") 2. Funder data management plan 3. Institutional policies</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#best-practices","title":"Best Practices","text":""},{"location":"GETTING_STARTED_CREDENTIALS/#when-to-create-a-doi","title":"When to Create a DOI","text":"<p>\u2705 Create a DOI when: - Data is finalized and won't change significantly - You're ready to share publicly (or with embargo) - Data supports a publication - Required by journal/funder - You want others to cite your data</p> <p>\u23f8\ufe0f Wait to create a DOI if: - Still collecting/analyzing data - Data quality is uncertain - Need institutional approval first - Preparing for submission to domain repository</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#versioning","title":"Versioning","text":"<p>When to create a new version (same DOI): - Minor corrections to metadata - Adding additional files - Fixing errors in existing data - Updating documentation</p> <p>When to create a new DOI: - Major changes to dataset - Different analysis/processing - Conceptually different dataset - Different paper/project</p> <p>Example: <pre><code># Original dataset (DOI: 10.5281/zenodo.123456)\ncicada doi publish data_v1.fastq --provider zenodo\n\n# Minor update (new version, same DOI concept)\ncicada doi update 10.5281/zenodo.123456 \\\n  --add-file corrected_metadata.csv\n\n# Major reanalysis (new DOI)\ncicada doi publish reanalyzed_data_v2.fastq --provider zenodo\n# Gets new DOI: 10.5281/zenodo.789012\n</code></pre></p>"},{"location":"GETTING_STARTED_CREDENTIALS/#what-metadata-to-include","title":"What Metadata to Include","text":"<p>Minimum (required): - Title (descriptive, not just filename) - Authors/Creators (with affiliations) - Description (what is the data, how was it collected) - Keywords (for discoverability) - License (how can others use it?)</p> <p>Recommended: - Related publications (DOIs) - Funding information - Methods/protocols - File formats and software requirements - Related identifiers (ORCID, grant numbers)</p> <p>Example metadata file: <pre><code>title: \"RNA-seq data from drought stress experiment in Arabidopsis\"\ndescription: |\n  Paired-end Illumina RNA sequencing (150bp) from Arabidopsis thaliana\n  leaves under drought stress and control conditions. Three biological\n  replicates per condition. Sequenced on NovaSeq 6000.\ncreators:\n  - name: \"Jane Smith\"\n    affiliation: \"Plant Biology Lab, State University\"\n    orcid: \"0000-0002-1234-5678\"\n  - name: \"John Doe\"\n    affiliation: \"Plant Biology Lab, State University\"\nkeywords:\n  - RNA-seq\n  - Arabidopsis thaliana\n  - drought stress\n  - transcriptomics\nlicense: \"CC-BY-4.0\"\nrelated_publications:\n  - doi: \"10.1234/journal.2024.123\"\n    relation: \"IsSupplementTo\"\nfunding:\n  - funder: \"National Science Foundation\"\n    award: \"NSF-1234567\"\n</code></pre></p>"},{"location":"GETTING_STARTED_CREDENTIALS/#quick-decision-guide","title":"Quick Decision Guide","text":"<p>Ask yourself:</p> <ol> <li>Is this data final?</li> <li>Yes \u2192 Create DOI \u2705</li> <li> <p>No \u2192 Wait \u23f8\ufe0f</p> </li> <li> <p>Will others need to cite or access it?</p> </li> <li>Yes \u2192 Create DOI \u2705</li> <li> <p>No \u2192 Maybe not needed \u274c</p> </li> <li> <p>Is there a domain-specific repository?</p> </li> <li>Yes \u2192 Check if they provide DOIs (use that first)</li> <li> <p>No \u2192 Use Zenodo/DataCite via Cicada \u2705</p> </li> <li> <p>Does my journal/funder require it?</p> </li> <li>Yes \u2192 Create DOI \u2705</li> <li> <p>No \u2192 Optional but recommended</p> </li> <li> <p>Is the data already public with a stable ID?</p> </li> <li>Yes \u2192 Use existing identifier \u274c</li> <li>No \u2192 Create DOI \u2705</li> </ol> <p>Still not sure? Ask your: - Institutional library - Research data office - Department administrator - Journal editor - Colleagues in your field</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#what-you-need","title":"What You Need","text":"<p>To publish DOIs with Cicada, you need credentials (like a password) from a DOI registration service.</p> <p>Two main options: - Zenodo (free for everyone) - DataCite (requires institutional membership)</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#understanding-zenodo-vs-datacite","title":"Understanding Zenodo vs DataCite","text":"<p>What are these services?</p> <p>Both Zenodo and DataCite are services that create permanent DOIs (Digital Object Identifiers) for your research data. A DOI is like a permanent web address that always points to your data, even if it moves to a different server.</p> <p>Zenodo: - A free repository and DOI service run by CERN (the European physics lab) - Anyone can create an account and start publishing immediately - Includes free file storage (up to 50 GB per dataset) - DOIs look like: <code>10.5281/zenodo.123456</code> - Perfect for: Individual researchers, small labs, open science projects - Website: https://zenodo.org</p> <p>DataCite: - A membership-based DOI service for institutions - Your university or research institution needs to be a member - You use your institution's credentials through Cicada - DOIs include your institution's prefix: <code>10.12345/yourdata</code> - Perfect for: Large institutions, institutional repositories, compliance requirements - Website: https://datacite.org</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#which-one-should-you-use","title":"Which One Should You Use?","text":"<p>Check with your institution's library FIRST!</p> <p>Before setting anything up, contact your institutional library or research data office and ask:</p> <p>\"Does our institution have a DataCite membership or preferred DOI service for research data?\"</p> <p>If they say YES to DataCite: - \u2705 Use DataCite (see Part 2B: DataCite Setup) - Your DOIs will be under your institution's prefix - May be required for institutional compliance - Ask them for your credentials (repository ID and password)</p> <p>If they say NO or don't have DOI services: - \u2705 Use Zenodo (see Part 1: Zenodo Setup) - Free and easy to set up yourself - No institutional approval needed - Works great for most research data</p> <p>If you're not sure or don't have institutional support: - \u2705 Start with Zenodo - it's free and takes 5 minutes - You can always add DataCite later if needed</p> <p>Can I use both?</p> <p>Yes! Cicada supports both, and you can switch between them with a simple flag: <pre><code>cicada doi publish data.fastq --provider zenodo\ncicada doi publish data.fastq --provider datacite\n</code></pre></p>"},{"location":"GETTING_STARTED_CREDENTIALS/#part-1-getting-your-zenodo-credentials-5-minutes","title":"Part 1: Getting Your Zenodo Credentials (5 minutes)","text":""},{"location":"GETTING_STARTED_CREDENTIALS/#step-1-create-a-zenodo-account","title":"Step 1: Create a Zenodo Account","text":"<ol> <li>Go to https://zenodo.org</li> <li>Click \"Sign Up\" in the top right corner</li> <li>You can sign up with:</li> <li>Your email address</li> <li>Your GitHub account (if you have one)</li> <li>Your ORCID (if you have one)</li> <li>Follow the instructions to complete sign-up</li> </ol> <p>That's it! Your Zenodo account is ready.</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#step-2-create-an-access-token","title":"Step 2: Create an Access Token","text":"<p>An access token is like a password that Cicada uses to publish DOIs on your behalf.</p> <ol> <li>Log in to Zenodo: https://zenodo.org</li> <li>Click your name/profile icon in the top right</li> <li>Click \"Applications\" in the dropdown menu</li> <li>Click the \"New Token\" button</li> <li>Give your token a name (e.g., \"Cicada DOI Publishing\")</li> <li>Under \"Scopes\", check the box for \"deposit:write\"</li> <li>This lets Cicada create DOIs for you</li> <li>Don't worry - it can only create DOIs, not delete anything</li> <li>Click \"Create\"</li> <li>Important: Copy the token that appears - you'll need it in the next step</li> <li>It looks like: <code>AbCdEf123456...</code> (40+ characters)</li> <li>You can only see it once, so copy it now!</li> </ol> <p>\u2713 You now have your Zenodo access token!</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#part-2-giving-cicada-your-credentials","title":"Part 2: Giving Cicada Your Credentials","text":"<p>You have three options for giving Cicada your token. Choose the one that sounds easiest to you.</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#option-a-simple-setup-recommended-for-most-users","title":"Option A: Simple Setup (Recommended for Most Users)","text":"<p>This is the easiest method. Cicada will guide you through setup.</p> <p>On macOS or Linux:</p> <ol> <li>Open Terminal (don't worry, we'll guide you!)</li> <li>macOS: Press <code>Cmd + Space</code>, type \"Terminal\", press Enter</li> <li> <p>Linux: Press <code>Ctrl + Alt + T</code></p> </li> <li> <p>Type this command and press Enter:    <pre><code>cicada config init\n</code></pre></p> </li> <li> <p>Cicada will ask: \"Enter your Zenodo token:\"</p> </li> <li>Paste the token you copied earlier (right-click \u2192 Paste)</li> <li>Don't worry if you don't see the token when you paste - that's for security!</li> <li> <p>Press Enter</p> </li> <li> <p>Cicada will save your token securely</p> </li> </ol> <p>That's it! Your credentials are set up.</p> <p>To test that it worked: <pre><code>cicada config test zenodo\n</code></pre></p> <p>You should see: <code>\u2713 Zenodo authentication successful</code></p>"},{"location":"GETTING_STARTED_CREDENTIALS/#option-b-manual-setup-config-file","title":"Option B: Manual Setup (Config File)","text":"<p>If you prefer to create the file yourself:</p> <p>On macOS or Linux:</p> <ol> <li> <p>Create a folder for Cicada settings:    <pre><code>mkdir -p ~/.config/cicada\n</code></pre></p> </li> <li> <p>Create a settings file:    <pre><code>nano ~/.config/cicada/config.yaml\n</code></pre>    (You can also use your favorite text editor instead of <code>nano</code>)</p> </li> <li> <p>Copy and paste this into the file:    <pre><code>providers:\n  zenodo:\n    token: \"PASTE_YOUR_TOKEN_HERE\"\n</code></pre></p> </li> <li> <p>Replace <code>PASTE_YOUR_TOKEN_HERE</code> with the token you copied from Zenodo</p> </li> <li>Keep the quotes around it!</li> <li> <p>Example: <code>token: \"AbCdEf123456...\"</code></p> </li> <li> <p>Save and close the file:</p> </li> <li> <p>In nano: Press <code>Ctrl + X</code>, then <code>Y</code>, then Enter</p> </li> <li> <p>Secure your file (important!):    <pre><code>chmod 600 ~/.config/cicada/config.yaml\n</code></pre>    This makes sure only you can read your credentials.</p> </li> </ol> <p>On Windows:</p> <ol> <li> <p>Open Notepad</p> </li> <li> <p>Copy and paste this:    <pre><code>providers:\n  zenodo:\n    token: \"PASTE_YOUR_TOKEN_HERE\"\n</code></pre></p> </li> <li> <p>Replace <code>PASTE_YOUR_TOKEN_HERE</code> with your Zenodo token</p> </li> <li> <p>Save the file as:    <pre><code>C:\\Users\\YourUsername\\AppData\\Roaming\\cicada\\config.yaml\n</code></pre>    Replace <code>YourUsername</code> with your Windows username.</p> </li> </ol>"},{"location":"GETTING_STARTED_CREDENTIALS/#option-c-project-specific-setup-for-advanced-users","title":"Option C: Project-Specific Setup (For Advanced Users)","text":"<p>If you're working on a specific project and want credentials just for that project:</p> <ol> <li> <p>In your project folder, create a file named <code>.env</code></p> </li> <li> <p>Add this line:    <pre><code>CICADA_ZENODO_TOKEN=your-token-here\n</code></pre></p> </li> <li> <p>Important: Tell Git to ignore this file (so you don't accidentally share your token):    <pre><code>echo \".env\" &gt;&gt; .gitignore\n</code></pre></p> </li> </ol>"},{"location":"GETTING_STARTED_CREDENTIALS/#part-3-testing-your-setup","title":"Part 3: Testing Your Setup","text":"<p>Let's make sure everything is working:</p> <pre><code>cicada config test zenodo\n</code></pre> <p>If you see: <code>\u2713 Zenodo authentication successful</code> - Success! You're all set up and ready to publish DOIs.</p> <p>If you see an error, see the Troubleshooting section below.</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#part-4-publishing-your-first-doi","title":"Part 4: Publishing Your First DOI","text":"<p>Now that you're set up, here's how to publish a DOI:</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#simple-example","title":"Simple Example","text":"<pre><code>cicada doi publish my-data-file.fastq --provider zenodo\n</code></pre> <p>This will: 1. Extract metadata from your file 2. Upload it to Zenodo 3. Create a DOI for it 4. Print the DOI (something like <code>10.5281/zenodo.123456</code>)</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#with-additional-information","title":"With Additional Information","text":"<p>You probably want to add more details about your data:</p> <ol> <li> <p>Create a file called <code>metadata.yaml</code> with your information:    <pre><code>title: \"My Research Data\"\ndescription: \"RNA sequencing data from...\"\ncreators:\n  - name: \"Jane Smith\"\n    affiliation: \"University Lab\"\nkeywords:\n  - RNA-seq\n  - genomics\n</code></pre></p> </li> <li> <p>Publish with the extra information:    <pre><code>cicada doi publish my-data-file.fastq \\\n  --enrich metadata.yaml \\\n  --provider zenodo\n</code></pre></p> </li> </ol> <p>That's it! You've published your first DOI.</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#part-2b-datacite-setup-for-institutional-users","title":"Part 2B: DataCite Setup (For Institutional Users)","text":"<p>If your institution has a DataCite membership (recommended to check with your library first):</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#step-1-get-your-credentials-from-your-institution","title":"Step 1: Get Your Credentials from Your Institution","text":"<p>Contact your institution's library, IT department, or research data office and ask:</p> <p>\"I need DataCite credentials to publish DOIs. Can you provide me with: - A DataCite repository ID - A DataCite password - Information about our DOI allocation\"</p> <p>They should give you: - Repository ID: Looks like <code>10.12345/INST</code> or <code>CLIENT.MEMBER</code> - Password: A password for that repository</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#step-2-configure-cicada","title":"Step 2: Configure Cicada","text":"<p>Option 1: Simple setup (recommended): <pre><code>cicada config init\n</code></pre> When prompted, enter your DataCite repository ID and password.</p> <p>Option 2: Manual config file:</p> <p>Create <code>~/.config/cicada/config.yaml</code>: <pre><code>providers:\n  datacite:\n    repository_id: \"your-repo-id-here\"\n    password: \"your-password-here\"\n</code></pre></p> <p>Then secure it: <pre><code>chmod 600 ~/.config/cicada/config.yaml\n</code></pre></p>"},{"location":"GETTING_STARTED_CREDENTIALS/#step-3-test-it","title":"Step 3: Test It","text":"<pre><code>cicada config test datacite\n</code></pre> <p>Should show: <code>\u2713 DataCite authentication successful</code></p>"},{"location":"GETTING_STARTED_CREDENTIALS/#step-4-publish-dois","title":"Step 4: Publish DOIs","text":"<pre><code>cicada doi publish my-data-file.fastq --provider datacite\n</code></pre>"},{"location":"GETTING_STARTED_CREDENTIALS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"GETTING_STARTED_CREDENTIALS/#command-not-found-cicada","title":"\"Command not found: cicada\"","text":"<p>Problem: Your computer doesn't know where Cicada is installed.</p> <p>Solution: 1. Make sure you installed Cicada (see main README) 2. Try the full path: <code>/usr/local/bin/cicada</code> instead of just <code>cicada</code></p>"},{"location":"GETTING_STARTED_CREDENTIALS/#authentication-failed","title":"\"Authentication failed\"","text":"<p>Problem: Your credentials aren't working.</p> <p>Check these:</p> <ol> <li>Did you copy the token correctly?</li> <li>Go back to Zenodo and create a new token</li> <li>Make sure you copy the entire token (no spaces before/after)</li> <li> <p>Try pasting it again</p> </li> <li> <p>Did you create the token with the right permissions?</p> </li> <li>The token needs \"deposit:write\" scope</li> <li> <p>Create a new token if you're not sure</p> </li> <li> <p>Are you using sandbox vs production?</p> </li> <li>If you're testing, make sure you use a sandbox token</li> <li>See \"Testing with Sandbox\" below</li> </ol>"},{"location":"GETTING_STARTED_CREDENTIALS/#insecure-permissions-on-configyaml","title":"\"Insecure permissions on config.yaml\"","text":"<p>Problem: Your credentials file can be read by other users on your computer.</p> <p>Solution: <pre><code>chmod 600 ~/.config/cicada/config.yaml\n</code></pre></p> <p>This command makes the file readable only by you.</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#file-not-found-configcicadaconfigyaml","title":"\"File not found: ~/.config/cicada/config.yaml\"","text":"<p>Problem: The config file doesn't exist yet.</p> <p>Solution: Run the setup command: <pre><code>cicada config init\n</code></pre></p> <p>Or create the file manually (see Option B above).</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#env-file-is-tracked-by-git","title":"\".env file is tracked by git\"","text":"<p>Problem: Your credentials are about to be shared on GitHub!</p> <p>Solution: <pre><code>echo \".env\" &gt;&gt; .gitignore\ngit rm --cached .env\ngit commit -m \"Remove credentials from git\"\n</code></pre></p> <p>This removes the credentials from git and prevents it from happening again.</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#cant-connect-to-zenododatacite","title":"\"Can't connect to Zenodo/DataCite\"","text":"<p>Problem: Network or service issue.</p> <p>Check: 1. Do you have internet connection? 2. Can you access https://zenodo.org in your browser? 3. Is your institution's firewall blocking API access?</p> <p>Try: - Wait a few minutes and try again - Contact your IT department if the problem persists</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#testing-with-sandbox-optional","title":"Testing with Sandbox (Optional)","text":"<p>Before publishing real DOIs, you might want to test with a sandbox (a test environment).</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#zenodo-sandbox","title":"Zenodo Sandbox","text":"<ol> <li>Create a sandbox account: https://sandbox.zenodo.org</li> <li>This is separate from your main Zenodo account</li> <li>Create a token the same way as before</li> <li>Tell Cicada to use sandbox:    <pre><code>cicada doi publish test-file.fastq \\\n  --provider zenodo \\\n  --zenodo-sandbox\n</code></pre></li> </ol> <p>Or in your config file: <pre><code>providers:\n  zenodo:\n    token: \"your-sandbox-token\"\n    environment: sandbox\n</code></pre></p>"},{"location":"GETTING_STARTED_CREDENTIALS/#datacite-sandbox","title":"DataCite Sandbox","text":"<ol> <li>Get sandbox credentials from your institution (or create test account)</li> <li>Use the <code>--datacite-sandbox</code> flag:    <pre><code>cicada doi publish test-file.fastq \\\n  --provider datacite \\\n  --datacite-sandbox\n</code></pre></li> </ol> <p>Sandbox DOIs are NOT REAL - they're for testing only and will be deleted eventually.</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#security-tips-please-read","title":"Security Tips (Please Read!)","text":"<p>Your credentials are like passwords - keep them safe:</p>"},{"location":"GETTING_STARTED_CREDENTIALS/#do","title":"\u2705 DO:","text":"<ul> <li>Keep credentials in the config file (<code>~/.config/cicada/config.yaml</code>)</li> <li>Make sure the config file has secure permissions (<code>chmod 600</code>)</li> <li>Create different tokens for different purposes (sandbox vs production)</li> <li>Delete old tokens when you're not using them anymore</li> </ul>"},{"location":"GETTING_STARTED_CREDENTIALS/#dont","title":"\u274c DON'T:","text":"<ul> <li>Share your tokens with anyone</li> <li>Commit tokens to git/GitHub</li> <li>Email tokens</li> <li>Put tokens in filenames or comments</li> <li>Use production tokens for testing</li> </ul>"},{"location":"GETTING_STARTED_CREDENTIALS/#if-your-token-is-exposed","title":"If Your Token is Exposed","text":"<p>If you accidentally shared your token:</p> <ol> <li>Go to Zenodo \u2192 Applications</li> <li>Find your token</li> <li>Click \"Revoke\" to delete it</li> <li>Create a new token</li> <li>Update your Cicada config with the new token</li> </ol>"},{"location":"GETTING_STARTED_CREDENTIALS/#quick-reference","title":"Quick Reference","text":""},{"location":"GETTING_STARTED_CREDENTIALS/#setup-commands","title":"Setup Commands","text":"<pre><code># Initialize config (guided setup)\ncicada config init\n\n# Test credentials\ncicada config test zenodo\ncicada config test datacite\n\n# View current config (tokens hidden)\ncicada config show\n\n# Validate security\ncicada config validate\n</code></pre>"},{"location":"GETTING_STARTED_CREDENTIALS/#publishing-commands","title":"Publishing Commands","text":"<pre><code># Basic publish\ncicada doi publish file.fastq --provider zenodo\n\n# With metadata\ncicada doi publish file.fastq \\\n  --enrich metadata.yaml \\\n  --provider zenodo\n\n# Using DataCite instead\ncicada doi publish file.fastq --provider datacite\n\n# Test with sandbox\ncicada doi publish file.fastq \\\n  --provider zenodo \\\n  --zenodo-sandbox\n</code></pre>"},{"location":"GETTING_STARTED_CREDENTIALS/#getting-help","title":"Getting Help","text":"<pre><code># General help\ncicada --help\n\n# Help for specific command\ncicada doi publish --help\ncicada config --help\n</code></pre>"},{"location":"GETTING_STARTED_CREDENTIALS/#example-complete-workflow","title":"Example: Complete Workflow","text":"<p>Here's a complete example from start to finish:</p> <pre><code># 1. Set up credentials (one time)\ncicada config init\n# Enter your Zenodo token when prompted\n\n# 2. Test that it works\ncicada config test zenodo\n# \u2713 Zenodo authentication successful\n\n# 3. Create metadata file\ncat &gt; metadata.yaml &lt;&lt;EOF\ntitle: \"My RNA-seq Data\"\ndescription: \"Sequencing data from experiment 123\"\ncreators:\n  - name: \"Jane Smith\"\n    affiliation: \"University Lab\"\n    orcid: \"0000-0002-1234-5678\"\nkeywords:\n  - RNA-seq\n  - gene expression\nrights: \"Creative Commons Attribution 4.0 (CC-BY-4.0)\"\nEOF\n\n# 4. Publish your data\ncicada doi publish my-data.fastq \\\n  --enrich metadata.yaml \\\n  --provider zenodo\n\n# Output:\n# \u2713 Metadata extracted from my-data.fastq\n# \u2713 Uploading to Zenodo...\n# \u2713 DOI created: 10.5281/zenodo.123456\n# \u2713 URL: https://zenodo.org/record/123456\n\n# 5. Your data is now published with a DOI!\n</code></pre>"},{"location":"GETTING_STARTED_CREDENTIALS/#getting-more-help","title":"Getting More Help","text":""},{"location":"GETTING_STARTED_CREDENTIALS/#documentation","title":"Documentation","text":"<ul> <li>Main README: https://github.com/scttfrdmn/cicada</li> <li>Detailed guides: https://github.com/scttfrdmn/cicada/docs</li> </ul>"},{"location":"GETTING_STARTED_CREDENTIALS/#support","title":"Support","text":"<ul> <li>GitHub Issues: https://github.com/scttfrdmn/cicada/issues</li> <li>Ask a question or report a problem</li> </ul>"},{"location":"GETTING_STARTED_CREDENTIALS/#zenodo-help","title":"Zenodo Help","text":"<ul> <li>Zenodo FAQ: https://help.zenodo.org</li> <li>Zenodo Support: info@zenodo.org</li> </ul>"},{"location":"GETTING_STARTED_CREDENTIALS/#datacite-help","title":"DataCite Help","text":"<ul> <li>DataCite Support Guide: https://support.datacite.org</li> <li>Contact: support@datacite.org</li> </ul>"},{"location":"GETTING_STARTED_CREDENTIALS/#glossary","title":"Glossary","text":"<p>DOI (Digital Object Identifier): A permanent identifier for your data (like <code>10.5281/zenodo.123456</code>)</p> <p>Token: A secret code that lets Cicada access your Zenodo account</p> <p>Credentials: Your login information (tokens, passwords, etc.)</p> <p>Sandbox: A test environment where you can practice without creating real DOIs</p> <p>Config file: A file where Cicada stores your settings and credentials</p> <p>Repository ID: Your institution's DataCite identifier</p> <p>Metadata: Information about your data (title, authors, description, etc.)</p> <p>Provider: The service that creates your DOI (Zenodo or DataCite)</p> <p>Questions? Create an issue on GitHub: https://github.com/scttfrdmn/cicada/issues</p> <p>Ready to start? Go back to Part 1 and follow the steps!</p>"},{"location":"METADATA_EXTRACTION/","title":"Metadata Extraction Guide","text":"<p>This guide covers Cicada's automated metadata extraction capabilities for scientific instrument files. Learn how to extract comprehensive metadata, validate against instrument specifications, and integrate metadata extraction into your workflows.</p>"},{"location":"METADATA_EXTRACTION/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Quick Start</li> <li>Supported Formats</li> <li>Command Reference</li> <li>Output Formats</li> <li>Batch Processing</li> <li>Validation with Presets</li> <li>Integration Examples</li> <li>Metadata Schema</li> <li>Troubleshooting</li> </ul>"},{"location":"METADATA_EXTRACTION/#overview","title":"Overview","text":"<p>Cicada automatically extracts rich metadata from scientific instrument files, including:</p> <ul> <li>File information: Format, size, compression</li> <li>Content statistics: Read counts, base counts, sequence lengths</li> <li>Quality metrics: Quality scores, GC content</li> <li>Instrument details: Type, pairing information</li> <li>Format-specific fields: Depends on file type</li> </ul>"},{"location":"METADATA_EXTRACTION/#why-extract-metadata","title":"Why Extract Metadata?","text":"<p>Metadata extraction is essential for effective data commons management:</p> <ul> <li>Data Discovery: Find files by instrument, date, sample, or other criteria</li> <li>Data Quality: Validate data meets quality standards</li> <li>Data Integrity: Verify files haven't been corrupted</li> <li>Data Organization: Automatically catalog and classify datasets</li> <li>Compliance: Track required information for regulatory compliance</li> <li>Publication: Have metadata ready when publishing datasets (optional)</li> </ul>"},{"location":"METADATA_EXTRACTION/#benefits","title":"Benefits","text":"<p>\u2705 Automated: No manual metadata tracking \u2705 Multi-Format: 14 file format extractors (microscopy, sequencing, mass spec, and more) \u2705 Fast: Processes large files efficiently with sampling \u2705 Accurate: Validates metadata during extraction \u2705 Flexible: Multiple output formats (JSON, YAML, table)</p>"},{"location":"METADATA_EXTRACTION/#quick-start","title":"Quick Start","text":""},{"location":"METADATA_EXTRACTION/#extract-metadata-from-a-single-file","title":"Extract Metadata from a Single File","text":"<pre><code># Basic extraction (JSON to stdout)\ncicada metadata extract sample.fastq.gz\n\n# Human-readable table format\ncicada metadata extract sample.fastq.gz --format table\n\n# Save to file\ncicada metadata extract sample.fastq.gz --output metadata.json\n</code></pre>"},{"location":"METADATA_EXTRACTION/#example-output","title":"Example Output","text":"<pre><code>{\n  \"format\": \"FASTQ\",\n  \"compression\": \"gzip\",\n  \"file_name\": \"/data/sample.fastq.gz\",\n  \"file_size\": 2147483648,\n  \"total_reads\": 45623891,\n  \"total_bases\": 6843583650,\n  \"mean_read_length\": 150,\n  \"gc_content_percent\": 42.3,\n  \"mean_quality_score\": 36.8,\n  \"is_paired_end\": true,\n  \"read_pair\": \"R1\",\n  \"instrument_type\": \"sequencing\",\n  \"data_type\": \"nucleotide_sequence\"\n}\n</code></pre>"},{"location":"METADATA_EXTRACTION/#supported-formats","title":"Supported Formats","text":"<p>Cicada v0.2.0 includes 14 file format extractors across multiple scientific domains:</p>"},{"location":"METADATA_EXTRACTION/#microscopy","title":"Microscopy","text":"Format Extensions Description TIFF <code>.tif</code>, <code>.tiff</code> Generic microscopy images OME-TIFF <code>.ome.tif</code>, <code>.ome.tiff</code> Open Microscopy Environment TIFF Zeiss CZI <code>.czi</code> Zeiss microscopy format (20+ fields) Nikon ND2 <code>.nd2</code> Nikon microscopy format Leica LIF <code>.lif</code> Leica microscopy format"},{"location":"METADATA_EXTRACTION/#sequencing-genomics","title":"Sequencing &amp; Genomics","text":"Format Extensions Description FASTQ <code>.fastq</code>, <code>.fq</code>, <code>.fastq.gz</code> Nucleotide sequencing data BAM <code>.bam</code> Binary alignment/map format"},{"location":"METADATA_EXTRACTION/#mass-spectrometry","title":"Mass Spectrometry","text":"Format Extensions Description mzML <code>.mzml</code> Mass spectrometry XML format MGF <code>.mgf</code> Mascot generic format"},{"location":"METADATA_EXTRACTION/#data-arrays","title":"Data Arrays","text":"Format Extensions Description HDF5 <code>.h5</code>, <code>.hdf5</code> Hierarchical Data Format 5 Zarr <code>.zarr</code> Chunked, compressed array storage"},{"location":"METADATA_EXTRACTION/#medical-imaging-flow-cytometry","title":"Medical Imaging &amp; Flow Cytometry","text":"Format Extensions Description DICOM <code>.dcm</code>, <code>.dicom</code> Medical imaging standard FCS <code>.fcs</code> Flow Cytometry Standard"},{"location":"METADATA_EXTRACTION/#fallback","title":"Fallback","text":"Format Description Generic Basic file metadata for unsupported formats <p>Each extractor automatically detects applicable files and extracts format-specific metadata.</p>"},{"location":"METADATA_EXTRACTION/#command-reference","title":"Command Reference","text":""},{"location":"METADATA_EXTRACTION/#cicada-metadata-extract","title":"<code>cicada metadata extract</code>","text":"<p>Extract metadata from a scientific instrument file.</p>"},{"location":"METADATA_EXTRACTION/#syntax","title":"Syntax","text":"<pre><code>cicada metadata extract &lt;path&gt; [flags]\n</code></pre>"},{"location":"METADATA_EXTRACTION/#arguments","title":"Arguments","text":"<ul> <li><code>&lt;path&gt;</code> - Path to the file to extract metadata from (required)</li> </ul>"},{"location":"METADATA_EXTRACTION/#flags","title":"Flags","text":"Flag Short Type Default Description <code>--format</code> <code>-f</code> string <code>json</code> Output format: <code>json</code>, <code>yaml</code>, or <code>table</code> <code>--output</code> <code>-o</code> string stdout Output file path (omit for stdout) <code>--extractor</code> string auto Force specific extractor (e.g., <code>fastq</code>)"},{"location":"METADATA_EXTRACTION/#examples","title":"Examples","text":"<pre><code># Basic usage\ncicada metadata extract data.fastq.gz\n\n# Save to file\ncicada metadata extract data.fastq.gz -o metadata.json\n\n# YAML format\ncicada metadata extract data.fastq.gz --format yaml\n\n# Table format for human reading\ncicada metadata extract data.fastq.gz --format table\n\n# Force specific extractor\ncicada metadata extract ambiguous.txt --extractor fastq\n</code></pre>"},{"location":"METADATA_EXTRACTION/#output-formats","title":"Output Formats","text":""},{"location":"METADATA_EXTRACTION/#json-format","title":"JSON Format","text":"<p>Structured data ideal for programmatic processing:</p> <pre><code>{\n  \"format\": \"FASTQ\",\n  \"file_size\": 2147483648,\n  \"total_reads\": 45623891,\n  \"mean_quality_score\": 36.8\n}\n</code></pre> <p>Use cases: - Parsing with <code>jq</code> or scripts - Database import - API integration - Automated workflows</p>"},{"location":"METADATA_EXTRACTION/#yaml-format","title":"YAML Format","text":"<p>Human-readable structured format:</p> <pre><code>format: FASTQ\nfile_size: 2147483648\ntotal_reads: 45623891\nmean_quality_score: 36.8\n</code></pre> <p>Use cases: - Configuration files - Documentation - Manual review - Git-trackable metadata</p>"},{"location":"METADATA_EXTRACTION/#table-format","title":"Table Format","text":"<p>Readable format for terminal display:</p> <pre><code>Metadata for: sample.fastq.gz\n================================\n\nFile Information:\n  format                : FASTQ\n  compression           : gzip\n  file_size             : 2.0 GB\n\nSequence Statistics:\n  total_reads           : 45,623,891\n  total_bases           : 6,843,583,650\n  mean_read_length      : 150\n\nQuality Metrics:\n  mean_quality_score    : 36.8\n  min_quality_score     : 12\n  max_quality_score     : 41\n</code></pre> <p>Use cases: - Quick inspection - Lab notebook entries - Reports - Presentations</p>"},{"location":"METADATA_EXTRACTION/#batch-processing","title":"Batch Processing","text":""},{"location":"METADATA_EXTRACTION/#extract-metadata-from-multiple-files","title":"Extract Metadata from Multiple Files","text":""},{"location":"METADATA_EXTRACTION/#shell-loop","title":"Shell Loop","text":"<pre><code># Extract metadata for all FASTQ files\nfor file in *.fastq.gz; do\n  basename=$(basename \"$file\" .fastq.gz)\n  cicada metadata extract \"$file\" \\\n    --format json \\\n    --output \"metadata/${basename}.json\"\ndone\n</code></pre>"},{"location":"METADATA_EXTRACTION/#parallel-processing","title":"Parallel Processing","text":"<pre><code># Use GNU parallel for speed\nfind . -name \"*.fastq.gz\" | parallel \\\n  'cicada metadata extract {} --output {}.metadata.json'\n</code></pre>"},{"location":"METADATA_EXTRACTION/#create-summary-report","title":"Create Summary Report","text":"<pre><code>#!/bin/bash\n# summarize_metadata.sh - Create summary from metadata files\n\necho \"Dataset Summary Report\"\necho \"======================\"\necho \"\"\n\ntotal_reads=0\ntotal_bases=0\nfile_count=0\n\nfor json in metadata/*.json; do\n  reads=$(jq '.total_reads' \"$json\")\n  bases=$(jq '.total_bases' \"$json\")\n\n  total_reads=$((total_reads + reads))\n  total_bases=$((total_bases + bases))\n  file_count=$((file_count + 1))\ndone\n\necho \"Total files: $file_count\"\necho \"Total reads: $(numfmt --grouping $total_reads)\"\necho \"Total bases: $(numfmt --grouping $total_bases)\"\necho \"Average reads per file: $(numfmt --grouping $((total_reads / file_count)))\"\necho \"Average quality: $(jq -s 'map(.mean_quality_score) | add / length' metadata/*.json)\"\n</code></pre> <p>Output: <pre><code>Dataset Summary Report\n======================\n\nTotal files: 48\nTotal reads: 2,189,946,768\nTotal bases: 328,492,015,200\nAverage reads per file: 45,623,891\nAverage quality: 37.2\n</code></pre></p>"},{"location":"METADATA_EXTRACTION/#integration-with-cicada-sync","title":"Integration with Cicada Sync","text":"<p>Automatically extract metadata when uploading to S3:</p> <pre><code>#!/bin/bash\n# sync_with_metadata.sh - Upload files and metadata together\n\nFILE=\"$1\"\n\necho \"Processing: $(basename \"$FILE\")\"\n\n# Extract metadata\nMETADATA_FILE=\"${FILE}.metadata.json\"\ncicada metadata extract \"$FILE\" \\\n  --format json \\\n  --output \"$METADATA_FILE\"\n\n# Upload both file and metadata to S3\ncicada sync \"$FILE\" s3://my-bucket/data/\ncicada sync \"$METADATA_FILE\" s3://my-bucket/metadata/\n\necho \"\u2713 File and metadata uploaded\"\n</code></pre> <p>Usage: <pre><code>./sync_with_metadata.sh sample_R1.fastq.gz\n</code></pre></p>"},{"location":"METADATA_EXTRACTION/#validation-with-presets","title":"Validation with Presets","text":"<p>Combine extraction with preset validation to ensure metadata completeness.</p>"},{"location":"METADATA_EXTRACTION/#validate-against-instrument-preset","title":"Validate Against Instrument Preset","text":"<pre><code># Extract and validate in one command\ncicada metadata validate sample.fastq.gz --preset illumina-novaseq\n</code></pre> <p>Output: <pre><code>\u2713 sample.fastq.gz: valid (FASTQ)\n     Quality Score: 100.0/100\n\nValidation Results:\n  Present Fields (8):\n    \u2713 format\n    \u2713 instrument_type\n    \u2713 total_reads\n    \u2713 total_bases\n    \u2713 mean_quality_score\n    \u2713 gc_content_percent\n    \u2713 is_paired_end\n    \u2713 read_pair\n\n  Missing Optional Fields (0):\n    (All optional fields present)\n\n  Errors (0):\n    No errors\n</code></pre></p>"},{"location":"METADATA_EXTRACTION/#available-presets","title":"Available Presets","text":"<pre><code># List all available presets\ncicada metadata preset list\n</code></pre> <p>See PRESETS.md for detailed preset documentation.</p>"},{"location":"METADATA_EXTRACTION/#integration-examples","title":"Integration Examples","text":""},{"location":"METADATA_EXTRACTION/#nextflow-pipeline-integration","title":"Nextflow Pipeline Integration","text":"<pre><code>// Extract metadata as part of pipeline\nprocess extract_metadata {\n  publishDir \"${params.outdir}/metadata\", mode: 'copy'\n\n  input:\n  path fastq_file\n\n  output:\n  path \"${fastq_file}.metadata.json\"\n\n  script:\n  \"\"\"\n  cicada metadata extract ${fastq_file} \\\n    --format json \\\n    --output ${fastq_file}.metadata.json\n  \"\"\"\n}\n\nworkflow {\n  // ... other processes ...\n\n  fastq_files = Channel.fromPath(params.input)\n  metadata = extract_metadata(fastq_files)\n\n  // Use metadata in downstream processes\n  metadata.view()\n}\n</code></pre>"},{"location":"METADATA_EXTRACTION/#snakemake-pipeline-integration","title":"Snakemake Pipeline Integration","text":"<pre><code># Snakefile\nrule extract_metadata:\n    input:\n        fastq=\"{sample}.fastq.gz\"\n    output:\n        metadata=\"{sample}.metadata.json\"\n    shell:\n        \"\"\"\n        cicada metadata extract {input.fastq} \\\n            --format json \\\n            --output {output.metadata}\n        \"\"\"\n\nrule summarize_metadata:\n    input:\n        metadata=expand(\"{sample}.metadata.json\", sample=SAMPLES)\n    output:\n        summary=\"summary.txt\"\n    run:\n        import json\n        total_reads = 0\n        for f in input.metadata:\n            with open(f) as fh:\n                meta = json.load(fh)\n                total_reads += meta['total_reads']\n\n        with open(output.summary, 'w') as out:\n            out.write(f\"Total reads: {total_reads}\\n\")\n</code></pre>"},{"location":"METADATA_EXTRACTION/#python-script-integration","title":"Python Script Integration","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"Extract and analyze metadata using Cicada.\"\"\"\n\nimport subprocess\nimport json\nfrom pathlib import Path\n\ndef extract_metadata(fastq_path):\n    \"\"\"Extract metadata using Cicada CLI.\"\"\"\n    result = subprocess.run(\n        ['cicada', 'metadata', 'extract', str(fastq_path), '--format', 'json'],\n        capture_output=True,\n        text=True,\n        check=True\n    )\n    return json.loads(result.stdout)\n\ndef analyze_quality(metadata):\n    \"\"\"Analyze quality metrics.\"\"\"\n    mean_q = metadata['mean_quality_score']\n\n    if mean_q &gt;= 35:\n        return \"Excellent\"\n    elif mean_q &gt;= 30:\n        return \"Good\"\n    elif mean_q &gt;= 25:\n        return \"Acceptable\"\n    else:\n        return \"Poor\"\n\ndef main():\n    \"\"\"Process all FASTQ files in directory.\"\"\"\n    fastq_files = Path('.').glob('*.fastq.gz')\n\n    results = []\n    for fastq in fastq_files:\n        meta = extract_metadata(fastq)\n        quality = analyze_quality(meta)\n\n        results.append({\n            'file': str(fastq),\n            'reads': meta['total_reads'],\n            'quality': quality,\n            'quality_score': meta['mean_quality_score']\n        })\n\n    # Print summary\n    print(f\"Processed {len(results)} files\")\n    for r in results:\n        print(f\"  {r['file']}: {r['reads']:,} reads, {r['quality']} quality ({r['quality_score']:.1f})\")\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Output: <pre><code>Processed 3 files\n  sample1.fastq.gz: 45,623,891 reads, Excellent quality (37.2)\n  sample2.fastq.gz: 42,891,234 reads, Excellent quality (36.8)\n  sample3.fastq.gz: 48,234,567 reads, Good quality (34.5)\n</code></pre></p>"},{"location":"METADATA_EXTRACTION/#metadata-schema","title":"Metadata Schema","text":""},{"location":"METADATA_EXTRACTION/#common-fields-all-formats","title":"Common Fields (All Formats)","text":"Field Type Description Example <code>format</code> string File format <code>\"FASTQ\"</code> <code>file_name</code> string Full file path <code>\"/data/sample.fastq.gz\"</code> <code>file_size</code> integer File size in bytes <code>2147483648</code> <code>compression</code> string Compression type <code>\"gzip\"</code>, <code>\"none\"</code> <code>instrument_type</code> string Instrument category <code>\"sequencing\"</code>, <code>\"microscopy\"</code> <code>data_type</code> string Data category <code>\"nucleotide_sequence\"</code>, <code>\"image\"</code> <code>extractor_name</code> string Extractor used <code>\"fastq\"</code> <code>schema_name</code> string Metadata schema version <code>\"fastq_v1\"</code>"},{"location":"METADATA_EXTRACTION/#fastq-specific-fields","title":"FASTQ-Specific Fields","text":"Field Type Description Range/Values <code>total_reads</code> integer Number of reads &gt; 0 <code>total_bases</code> integer Total nucleotides &gt; 0 <code>mean_read_length</code> integer Average read length &gt; 0 <code>min_read_length</code> integer Shortest read &gt; 0 <code>max_read_length</code> integer Longest read &gt; 0 <code>gc_content_percent</code> float GC content percentage 0-100 <code>mean_quality_score</code> float Average Phred score 0-93 <code>min_quality_score</code> integer Minimum Phred score 0-93 <code>max_quality_score</code> integer Maximum Phred score 0-93 <code>is_paired_end</code> boolean Paired-end detected <code>true</code>, <code>false</code> <code>read_pair</code> string Read pair identifier <code>\"R1\"</code>, <code>\"R2\"</code>, <code>\"1\"</code>, <code>\"2\"</code>"},{"location":"METADATA_EXTRACTION/#quality-score-interpretation","title":"Quality Score Interpretation","text":"<p>FASTQ uses Phred+33 quality scores:</p> Score Range Quality Interpretation 40-41 Excellent Error rate &lt; 0.01% 30-39 Very Good Error rate &lt; 0.1% 20-29 Good Error rate &lt; 1% 10-19 Marginal Error rate &lt; 10% 0-9 Poor Error rate &gt; 10%"},{"location":"METADATA_EXTRACTION/#troubleshooting","title":"Troubleshooting","text":""},{"location":"METADATA_EXTRACTION/#file-not-recognized","title":"File Not Recognized","text":"<p>Problem: <code>Error: no extractor found for file</code></p> <p>Solutions: 1. Check file extension is supported:    <pre><code># List supported formats\ncicada metadata list\n</code></pre></p> <ol> <li> <p>Force specific extractor:    <pre><code>cicada metadata extract file.txt --extractor fastq\n</code></pre></p> </li> <li> <p>Verify file is not corrupted:    <pre><code># For gzipped files\ngunzip -t file.fastq.gz\n\n# For plain files\nhead -4 file.fastq\n</code></pre></p> </li> </ol>"},{"location":"METADATA_EXTRACTION/#invalid-file-format","title":"Invalid File Format","text":"<p>Problem: <code>Error: invalid FASTQ format</code></p> <p>Common causes: - File is empty - Missing <code>@</code> header line - Missing <code>+</code> separator line - Sequence and quality lengths don't match</p> <p>Solution: Validate FASTQ format <pre><code># Check first record\nhead -4 file.fastq\n\n# Expected format:\n# @HEADER\n# SEQUENCE\n# +\n# QUALITY\n</code></pre></p>"},{"location":"METADATA_EXTRACTION/#large-file-processing","title":"Large File Processing","text":"<p>Problem: Extraction taking too long</p> <p>Solution: Cicada automatically samples large files - Processes up to 10,000 reads by default - Provides representative statistics - Fast even for 100+ GB files</p> <p>Manual control (future feature): <pre><code># Not yet implemented, but planned:\ncicada metadata extract file.fastq.gz --sample-size 5000\n</code></pre></p>"},{"location":"METADATA_EXTRACTION/#gzip-errors","title":"Gzip Errors","text":"<p>Problem: <code>Error: gzip: invalid compressed data</code></p> <p>Solutions: 1. Verify file integrity:    <pre><code>gunzip -t file.fastq.gz\n</code></pre></p> <ol> <li> <p>Re-download file if corrupted</p> </li> <li> <p>Check disk space:    <pre><code>df -h /path/to/file\n</code></pre></p> </li> </ol>"},{"location":"METADATA_EXTRACTION/#permission-denied","title":"Permission Denied","text":"<p>Problem: <code>Error: permission denied</code></p> <p>Solutions: 1. Check file permissions:    <pre><code>ls -l file.fastq.gz\n</code></pre></p> <ol> <li> <p>Add read permission:    <pre><code>chmod +r file.fastq.gz\n</code></pre></p> </li> <li> <p>Check directory permissions:    <pre><code>ls -ld $(dirname file.fastq.gz)\n</code></pre></p> </li> </ol>"},{"location":"METADATA_EXTRACTION/#best-practices","title":"Best Practices","text":""},{"location":"METADATA_EXTRACTION/#1-extract-metadata-early","title":"1. Extract Metadata Early","text":"<p>Extract metadata as soon as data is generated: - Easier to track while experiment is fresh - Can detect quality issues early - Enables immediate data discovery</p>"},{"location":"METADATA_EXTRACTION/#2-store-metadata-with-data","title":"2. Store Metadata with Data","text":"<p>Keep metadata files alongside data files: <pre><code>project/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 sample_R1.fastq.gz\n\u2502   \u251c\u2500\u2500 sample_R2.fastq.gz\n\u251c\u2500\u2500 metadata/\n\u2502   \u251c\u2500\u2500 sample_R1.metadata.json\n\u2502   \u2514\u2500\u2500 sample_R2.metadata.json\n</code></pre></p> <p>Or use sidecar files: <pre><code>project/\n\u251c\u2500\u2500 sample_R1.fastq.gz\n\u251c\u2500\u2500 sample_R1.fastq.gz.metadata.json\n\u251c\u2500\u2500 sample_R2.fastq.gz\n\u2514\u2500\u2500 sample_R2.fastq.gz.metadata.json\n</code></pre></p>"},{"location":"METADATA_EXTRACTION/#3-validate-against-presets","title":"3. Validate Against Presets","text":"<p>Always validate against instrument presets to ensure completeness: <pre><code>cicada metadata validate file.fastq.gz --preset illumina-novaseq\n</code></pre></p>"},{"location":"METADATA_EXTRACTION/#4-version-your-metadata","title":"4. Version Your Metadata","text":"<p>Track metadata schema versions: - Check <code>schema_name</code> field in output - Document which version was used - Update extraction when schemas change</p>"},{"location":"METADATA_EXTRACTION/#5-automate-extraction","title":"5. Automate Extraction","text":"<p>Integrate metadata extraction into your workflows: - Add to upload scripts - Include in pipeline stages - Trigger on file system events</p>"},{"location":"METADATA_EXTRACTION/#performance","title":"Performance","text":""},{"location":"METADATA_EXTRACTION/#benchmarks","title":"Benchmarks","text":"<p>Typical extraction performance (single-threaded):</p> File Size Reads Time Throughput 100 MB 500K 0.5s 200 MB/s 1 GB 5M 2s 500 MB/s 10 GB 50M 5s 2 GB/s (sampled) 100 GB 500M 5s 20 GB/s (sampled) <p>Note: Large files are automatically sampled (10,000 reads), providing fast representative statistics.</p>"},{"location":"METADATA_EXTRACTION/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Use local storage: Extract from local files, not network mounts</li> <li>SSD recommended: Faster I/O for large files</li> <li>Batch processing: Use parallel processing for multiple files</li> <li>Sampling: Automatic for large files, no configuration needed</li> </ol>"},{"location":"METADATA_EXTRACTION/#next-steps","title":"Next Steps","text":"<ul> <li>Validate metadata: See PRESETS.md</li> <li>Prepare for DOI: See DOI_WORKFLOW.md</li> <li>User scenarios: See USER_SCENARIOS_v0.2.0.md</li> <li>Integration testing: See ../INTEGRATION_TESTING.md</li> </ul>"},{"location":"METADATA_EXTRACTION/#support","title":"Support","text":"<p>Questions or Issues? - \ud83d\udcd6 Full documentation: README.md - \ud83d\udc1b Report bugs: GitHub Issues - \ud83d\udcac Discussions: GitHub Discussions - \ud83d\ude80 Feature requests: GitHub Issues</p>"},{"location":"PRESETS/","title":"Instrument Preset Guide","text":"<p>Status: v0.2.0 Documentation Audience: Lab managers, data curators, researchers validating metadata</p>"},{"location":"PRESETS/#overview","title":"Overview","text":"<p>Instrument presets provide pre-configured validation rules and metadata templates for specific scientific instruments. They ensure your data commons maintains high-quality, standardized metadata for effective data management.</p> <p>Why Use Presets?</p> <p>Presets are essential for maintaining data quality in your lab's data commons: - Ensure data is properly documented for future use - Catch missing or incorrect metadata early - Enable consistent data organization across the lab - Facilitate data sharing within and between labs - Meet requirements when publishing data (optional)</p> <p>Key Benefits:</p> <ul> <li>Data Quality Assurance: Catch incomplete or incorrect metadata before it becomes a problem</li> <li>Lab Standardization: Consistent metadata practices across your lab</li> <li>Automatic Validation: Instant feedback on metadata completeness</li> <li>Quality Scoring: Objective assessment (0-100 scale) to track improvement</li> <li>Time Savings: Pre-configured rules for common instruments</li> <li>Future-Proof: Well-documented data is usable years later</li> </ul> <p>Available Presets (v0.2.0):</p> <ul> <li>Microscopy: Zeiss LSM 880, Zeiss LSM 900, Zeiss LSM 980, Generic Microscopy</li> <li>Sequencing: Illumina NovaSeq, Illumina MiSeq, Illumina NextSeq, Generic Sequencing</li> </ul>"},{"location":"PRESETS/#quick-start","title":"Quick Start","text":""},{"location":"PRESETS/#validate-with-preset","title":"Validate with Preset","text":"<pre><code># Extract and validate FASTQ file with Illumina NovaSeq preset\ncicada metadata extract sample_R1.fastq.gz --preset illumina-novaseq\n\n# Validate existing metadata file\ncicada metadata validate metadata.json --preset illumina-novaseq\n</code></pre>"},{"location":"PRESETS/#list-available-presets","title":"List Available Presets","text":"<pre><code># List all presets\ncicada metadata preset list\n\n# Find Illumina presets\ncicada metadata preset list --manufacturer Illumina\n\n# Find microscopy presets\ncicada metadata preset list --type microscopy\n</code></pre>"},{"location":"PRESETS/#check-quality-score","title":"Check Quality Score","text":"<pre><code># Extract with preset validation to see quality score\ncicada metadata extract sample.fastq --preset illumina-novaseq --format yaml\n\n# Output includes:\n# quality_score: 78\n# validation_errors: []\n# validation_warnings: [\"Optional field 'description' not provided\"]\n</code></pre>"},{"location":"PRESETS/#available-presets","title":"Available Presets","text":""},{"location":"PRESETS/#illumina-novaseq","title":"Illumina NovaSeq","text":"<p>Preset ID: <code>illumina-novaseq</code></p> <p>High-throughput sequencing platform for large-scale genomics projects.</p> <p>Manufacturer: Illumina Instrument Type: Sequencing Platform: NovaSeq 6000</p> <p>Required Fields (60% of quality score): - <code>format</code> - File format (e.g., \"FASTQ\") - <code>instrument_manufacturer</code> - \"Illumina\" - <code>instrument_model</code> - \"NovaSeq 6000\" or compatible - <code>sequencing_platform</code> - \"Illumina\" - <code>total_reads</code> - Number of reads in file - <code>read_length</code> - Length of sequencing reads (bp)</p> <p>Optional Fields (40% of quality score): - <code>run_id</code> - Sequencing run identifier - <code>flowcell_id</code> - Flowcell identifier - <code>lane</code> - Lane number (1-4 for NovaSeq) - <code>barcode</code> - Sample barcode/index sequence - <code>quality_encoding</code> - Quality score encoding (e.g., \"Phred+33\") - <code>gc_content</code> - GC percentage - <code>mean_quality_score</code> - Average quality score - <code>is_paired_end</code> - Paired-end sequencing flag</p> <p>Best For: - Whole genome sequencing - Large RNA-seq projects - High-depth targeted sequencing</p>"},{"location":"PRESETS/#illumina-miseq","title":"Illumina MiSeq","text":"<p>Preset ID: <code>illumina-miseq</code></p> <p>Desktop sequencer for targeted gene sequencing and small genome applications.</p> <p>Manufacturer: Illumina Instrument Type: Sequencing Platform: MiSeq</p> <p>Required Fields: - <code>format</code> - <code>instrument_manufacturer</code> - \"Illumina\" - <code>instrument_model</code> - \"MiSeq\" - <code>sequencing_platform</code> - \"Illumina\" - <code>total_reads</code> - <code>read_length</code></p> <p>Optional Fields: - <code>run_id</code> - <code>flowcell_id</code> - <code>barcode</code> - <code>quality_encoding</code> - <code>gc_content</code> - <code>mean_quality_score</code> - <code>is_paired_end</code></p> <p>Best For: - Amplicon sequencing (16S, targeted genes) - Small genome sequencing - Validation studies</p>"},{"location":"PRESETS/#illumina-nextseq","title":"Illumina NextSeq","text":"<p>Preset ID: <code>illumina-nextseq</code></p> <p>Mid-throughput sequencer balancing speed and data output.</p> <p>Manufacturer: Illumina Instrument Type: Sequencing Platform: NextSeq 500/550/1000/2000</p> <p>Required Fields: - <code>format</code> - <code>instrument_manufacturer</code> - \"Illumina\" - <code>instrument_model</code> - \"NextSeq\" (any model) - <code>sequencing_platform</code> - \"Illumina\" - <code>total_reads</code> - <code>read_length</code></p> <p>Optional Fields: - <code>run_id</code> - <code>flowcell_id</code> - <code>barcode</code> - <code>quality_encoding</code> - <code>gc_content</code> - <code>mean_quality_score</code> - <code>is_paired_end</code></p> <p>Best For: - RNA-seq - Exome sequencing - Targeted panels</p>"},{"location":"PRESETS/#zeiss-lsm-880","title":"Zeiss LSM 880","text":"<p>Preset ID: <code>zeiss-lsm-880</code></p> <p>Laser scanning confocal microscope with spectral detection.</p> <p>Manufacturer: Zeiss Instrument Type: Microscopy Platform: LSM 880</p> <p>Required Fields (60% of quality score): - <code>format</code> - Image format (e.g., \"CZI\", \"TIFF\") - <code>instrument_manufacturer</code> - \"Zeiss\" - <code>instrument_model</code> - \"LSM 880\" - <code>microscopy_type</code> - \"Confocal\" - <code>image_width</code> - Width in pixels - <code>image_height</code> - Height in pixels</p> <p>Optional Fields (40% of quality score): - <code>bit_depth</code> - Bits per pixel - <code>pixel_size_x</code> - X dimension pixel size (\u03bcm) - <code>pixel_size_y</code> - Y dimension pixel size (\u03bcm) - <code>pixel_size_z</code> - Z dimension pixel size (\u03bcm) - <code>channels</code> - Number of imaging channels - <code>z_slices</code> - Number of Z-stack slices - <code>time_points</code> - Number of time points - <code>objective_magnification</code> - Objective lens magnification - <code>objective_na</code> - Numerical aperture - <code>excitation_wavelengths</code> - Laser wavelengths (nm) - <code>emission_wavelengths</code> - Detection wavelengths (nm)</p> <p>Best For: - Live cell imaging - Multi-channel fluorescence - Z-stack acquisition</p>"},{"location":"PRESETS/#zeiss-lsm-900","title":"Zeiss LSM 900","text":"<p>Preset ID: <code>zeiss-lsm-900</code></p> <p>Advanced confocal with Airyscan 2 for super-resolution imaging.</p> <p>Manufacturer: Zeiss Instrument Type: Microscopy Platform: LSM 900</p> <p>Required Fields: - <code>format</code> - <code>instrument_manufacturer</code> - \"Zeiss\" - <code>instrument_model</code> - \"LSM 900\" - <code>microscopy_type</code> - \"Confocal\" - <code>image_width</code> - <code>image_height</code></p> <p>Optional Fields: - <code>bit_depth</code> - <code>pixel_size_x</code> - <code>pixel_size_y</code> - <code>pixel_size_z</code> - <code>channels</code> - <code>z_slices</code> - <code>time_points</code> - <code>objective_magnification</code> - <code>objective_na</code> - <code>excitation_wavelengths</code> - <code>emission_wavelengths</code> - <code>airyscan_mode</code> - \"SR\" (super-resolution) or \"FAST\"</p> <p>Best For: - Super-resolution imaging - Thick tissue sections - High-speed live imaging</p>"},{"location":"PRESETS/#zeiss-lsm-980","title":"Zeiss LSM 980","text":"<p>Preset ID: <code>zeiss-lsm-980</code></p> <p>Flagship confocal with multiphoton and spectral unmixing capabilities.</p> <p>Manufacturer: Zeiss Instrument Type: Microscopy Platform: LSM 980</p> <p>Required Fields: - <code>format</code> - <code>instrument_manufacturer</code> - \"Zeiss\" - <code>instrument_model</code> - \"LSM 980\" - <code>microscopy_type</code> - \"Confocal\" - <code>image_width</code> - <code>image_height</code></p> <p>Optional Fields: - <code>bit_depth</code> - <code>pixel_size_x</code> - <code>pixel_size_y</code> - <code>pixel_size_z</code> - <code>channels</code> - <code>z_slices</code> - <code>time_points</code> - <code>objective_magnification</code> - <code>objective_na</code> - <code>excitation_wavelengths</code> - <code>emission_wavelengths</code> - <code>airyscan_mode</code> - <code>multiphoton</code> - Boolean flag for multiphoton mode</p> <p>Best For: - Deep tissue imaging - Spectral imaging - Multiphoton microscopy</p>"},{"location":"PRESETS/#generic-sequencing","title":"Generic Sequencing","text":"<p>Preset ID: <code>generic-sequencing</code></p> <p>Universal preset for any sequencing platform.</p> <p>Manufacturer: Any Instrument Type: Sequencing Platform: Generic</p> <p>Required Fields: - <code>format</code> - <code>sequencing_platform</code> - Platform name - <code>total_reads</code></p> <p>Optional Fields: - <code>read_length</code> - <code>is_paired_end</code> - <code>quality_encoding</code> - <code>gc_content</code> - <code>instrument_manufacturer</code> - <code>instrument_model</code></p> <p>Best For: - Non-Illumina platforms (PacBio, Oxford Nanopore, MGI) - Legacy data - Unknown instrument details</p>"},{"location":"PRESETS/#generic-microscopy","title":"Generic Microscopy","text":"<p>Preset ID: <code>generic-microscopy</code></p> <p>Universal preset for any microscopy platform.</p> <p>Manufacturer: Any Instrument Type: Microscopy Platform: Generic</p> <p>Required Fields: - <code>format</code> - <code>instrument_type</code> - \"microscopy\" - <code>image_width</code> - <code>image_height</code></p> <p>Optional Fields: - <code>bit_depth</code> - <code>pixel_size_x</code> - <code>pixel_size_y</code> - <code>channels</code> - <code>microscopy_type</code> - <code>instrument_manufacturer</code> - <code>instrument_model</code></p> <p>Best For: - Non-Zeiss microscopes (Nikon, Olympus, Leica) - Widefield/epifluorescence - Unknown instrument details</p>"},{"location":"PRESETS/#command-reference","title":"Command Reference","text":""},{"location":"PRESETS/#list-presets","title":"List Presets","text":"<pre><code>cicada metadata preset list [flags]\n</code></pre> <p>Flags: - <code>--manufacturer, -m</code> - Filter by manufacturer (e.g., \"Illumina\", \"Zeiss\") - <code>--type, -t</code> - Filter by instrument type (\"sequencing\", \"microscopy\") - <code>--format, -f</code> - Output format: json, yaml, or table (default: table)</p> <p>Examples:</p> <pre><code># List all presets in table format\ncicada metadata preset list\n\n# List Illumina presets\ncicada metadata preset list --manufacturer Illumina\n\n# List microscopy presets as JSON\ncicada metadata preset list --type microscopy --format json\n</code></pre>"},{"location":"PRESETS/#show-preset-details","title":"Show Preset Details","text":"<pre><code>cicada metadata preset show &lt;preset-id&gt; [flags]\n</code></pre> <p>Flags: - <code>--format, -f</code> - Output format: json, yaml, or table (default: yaml)</p> <p>Examples:</p> <pre><code># Show Illumina NovaSeq preset details\ncicada metadata preset show illumina-novaseq\n\n# Show as JSON\ncicada metadata preset show zeiss-lsm-880 --format json\n</code></pre>"},{"location":"PRESETS/#validate-with-preset_1","title":"Validate with Preset","text":"<pre><code>cicada metadata validate &lt;metadata-file&gt; --preset &lt;preset-id&gt; [flags]\n</code></pre> <p>Flags: - <code>--preset, -p</code> - Preset ID to validate against - <code>--strict</code> - Treat warnings as errors - <code>--format, -f</code> - Output format: json, yaml, or table (default: table)</p> <p>Examples:</p> <pre><code># Validate metadata file\ncicada metadata validate metadata.json --preset illumina-novaseq\n\n# Strict validation (warnings become errors)\ncicada metadata validate metadata.json --preset illumina-novaseq --strict\n</code></pre>"},{"location":"PRESETS/#extract-with-preset","title":"Extract with Preset","text":"<pre><code>cicada metadata extract &lt;file&gt; --preset &lt;preset-id&gt; [flags]\n</code></pre> <p>Automatically validates extracted metadata against preset requirements.</p> <p>Examples:</p> <pre><code># Extract and validate in one step\ncicada metadata extract sample.fastq.gz --preset illumina-novaseq\n\n# Extract with preset and save to file\ncicada metadata extract sample.fastq.gz \\\n  --preset illumina-novaseq \\\n  --format json \\\n  --output metadata.json\n</code></pre>"},{"location":"PRESETS/#quality-scoring","title":"Quality Scoring","text":"<p>Presets use a 0-100 quality score based on field completeness:</p> <ul> <li>60 points: Required fields (all must be present)</li> <li>40 points: Optional fields (proportional to completeness)</li> </ul>"},{"location":"PRESETS/#score-calculation","title":"Score Calculation","text":"<pre><code>Score = (Required Fields \u00d7 60) + (Optional Fields \u00d7 40)\n\nWhere:\n- Required Fields = 1.0 if all present, 0.0 otherwise\n- Optional Fields = (present optional fields) / (total optional fields)\n</code></pre>"},{"location":"PRESETS/#score-interpretation","title":"Score Interpretation","text":"Score Interpretation Publication Ready? 90-100 Excellent \u2705 Yes - High quality 75-89 Good \u2705 Yes - Acceptable 60-74 Adequate \u26a0\ufe0f Maybe - Missing recommended fields 0-59 Insufficient \u274c No - Missing required fields"},{"location":"PRESETS/#example-score-progression","title":"Example Score Progression","text":"<p>Minimal Metadata (Score: 60) <pre><code>format: FASTQ\ninstrument_manufacturer: Illumina\ninstrument_model: NovaSeq 6000\nsequencing_platform: Illumina\ntotal_reads: 1000000\nread_length: 150\n</code></pre> - \u2705 All 6 required fields present: 60 points - \u274c 0 of 8 optional fields: 0 points - Total: 60/100</p> <p>Basic Metadata (Score: 80) <pre><code>format: FASTQ\ninstrument_manufacturer: Illumina\ninstrument_model: NovaSeq 6000\nsequencing_platform: Illumina\ntotal_reads: 1000000\nread_length: 150\nrun_id: RUN_123\nquality_encoding: Phred+33\ngc_content: 52.3\nis_paired_end: true\n</code></pre> - \u2705 All 6 required fields: 60 points - \u2705 4 of 8 optional fields: 20 points - Total: 80/100</p> <p>Complete Metadata (Score: 100) <pre><code>format: FASTQ\ninstrument_manufacturer: Illumina\ninstrument_model: NovaSeq 6000\nsequencing_platform: Illumina\ntotal_reads: 1000000\nread_length: 150\nrun_id: RUN_123\nflowcell_id: FLOWCELL_456\nlane: 2\nbarcode: AGTCACTA\nquality_encoding: Phred+33\ngc_content: 52.3\nmean_quality_score: 36.8\nis_paired_end: true\n</code></pre> - \u2705 All 6 required fields: 60 points - \u2705 All 8 optional fields: 40 points - Total: 100/100</p>"},{"location":"PRESETS/#validation-workflow","title":"Validation Workflow","text":""},{"location":"PRESETS/#step-1-extract-metadata","title":"Step 1: Extract Metadata","text":"<pre><code># Extract without validation\ncicada metadata extract sample_R1.fastq.gz --output metadata.json\n</code></pre>"},{"location":"PRESETS/#step-2-choose-preset","title":"Step 2: Choose Preset","text":"<pre><code># List available presets\ncicada metadata preset list --type sequencing\n\n# Show preset requirements\ncicada metadata preset show illumina-novaseq\n</code></pre>"},{"location":"PRESETS/#step-3-validate","title":"Step 3: Validate","text":"<pre><code># Validate extracted metadata\ncicada metadata validate metadata.json --preset illumina-novaseq\n</code></pre> <p>Output: <pre><code>Validation Results\n==================\n\nPreset: illumina-novaseq (Illumina NovaSeq 6000)\nQuality Score: 80/100\n\n\u2705 Required Fields (6/6)\n  \u2713 format\n  \u2713 instrument_manufacturer\n  \u2713 instrument_model\n  \u2713 sequencing_platform\n  \u2713 total_reads\n  \u2713 read_length\n\n\u26a0\ufe0f Optional Fields (4/8)\n  \u2713 run_id\n  \u2713 quality_encoding\n  \u2713 gc_content\n  \u2713 is_paired_end\n  \u2717 flowcell_id - Not provided\n  \u2717 lane - Not provided\n  \u2717 barcode - Not provided\n  \u2717 mean_quality_score - Not provided\n\nRecommendation: Consider adding optional fields to improve quality score.\n</code></pre></p>"},{"location":"PRESETS/#step-4-enrich-metadata-optional","title":"Step 4: Enrich Metadata (Optional)","text":"<p>Create enrichment file with missing fields:</p> <p>enrich.yaml: <pre><code>flowcell_id: FLOWCELL_456\nlane: 2\nbarcode: AGTCACTA\nmean_quality_score: 36.8\n</code></pre></p> <pre><code># Re-validate with enrichment\ncicada metadata validate metadata.json \\\n  --preset illumina-novaseq \\\n  --enrich enrich.yaml\n</code></pre> <p>New Output: <pre><code>Quality Score: 100/100\n\u2705 All fields present\n</code></pre></p>"},{"location":"PRESETS/#finding-presets","title":"Finding Presets","text":""},{"location":"PRESETS/#by-manufacturer","title":"By Manufacturer","text":"<pre><code># Find all Illumina presets\ncicada metadata preset list --manufacturer Illumina\n\n# Output:\n# ID                  Manufacturer  Platform        Type\n# illumina-novaseq    Illumina      NovaSeq 6000    sequencing\n# illumina-miseq      Illumina      MiSeq           sequencing\n# illumina-nextseq    Illumina      NextSeq         sequencing\n</code></pre>"},{"location":"PRESETS/#by-instrument-type","title":"By Instrument Type","text":"<pre><code># Find all microscopy presets\ncicada metadata preset list --type microscopy\n\n# Output:\n# ID                  Manufacturer  Platform    Type\n# zeiss-lsm-880       Zeiss         LSM 880     microscopy\n# zeiss-lsm-900       Zeiss         LSM 900     microscopy\n# zeiss-lsm-980       Zeiss         LSM 980     microscopy\n# generic-microscopy  Any           Generic     microscopy\n</code></pre>"},{"location":"PRESETS/#programmatically-future-v030","title":"Programmatically (Future: v0.3.0+)","text":"<p>Python example: <pre><code>from cicada import PresetRegistry\n\nregistry = PresetRegistry()\n\n# Find by manufacturer\nillumina_presets = registry.find(manufacturer=\"Illumina\")\n\n# Find by type\nmicroscopy_presets = registry.find(instrument_type=\"microscopy\")\n\n# Find by both\nzeiss_microscopes = registry.find(\n    manufacturer=\"Zeiss\",\n    instrument_type=\"microscopy\"\n)\n\n# Get specific preset\npreset = registry.get(\"illumina-novaseq\")\nprint(f\"Required fields: {preset.required_fields}\")\n</code></pre></p>"},{"location":"PRESETS/#integration-examples","title":"Integration Examples","text":""},{"location":"PRESETS/#nextflow-pipeline","title":"Nextflow Pipeline","text":"<pre><code>process validate_metadata {\n    input:\n    path fastq\n    val preset_id\n\n    output:\n    path \"metadata.json\"\n\n    script:\n    \"\"\"\n    # Extract and validate with preset\n    cicada metadata extract ${fastq} \\\n      --preset ${preset_id} \\\n      --format json \\\n      --output metadata.json\n\n    # Check quality score\n    score=\\$(jq -r '.quality_score' metadata.json)\n    if (( \\$(echo \"\\$score &lt; 75\" | bc -l) )); then\n        echo \"Warning: Quality score \\$score is below recommended threshold\"\n        exit 1\n    fi\n    \"\"\"\n}\n\nworkflow {\n    fastqs = Channel.fromPath(\"data/*.fastq.gz\")\n    preset = \"illumina-novaseq\"\n\n    validate_metadata(fastqs, preset)\n}\n</code></pre>"},{"location":"PRESETS/#snakemake-workflow","title":"Snakemake Workflow","text":"<pre><code>rule validate_metadata:\n    input:\n        fastq=\"data/{sample}.fastq.gz\"\n    output:\n        metadata=\"metadata/{sample}.json\",\n        report=\"reports/{sample}_validation.txt\"\n    params:\n        preset=\"illumina-novaseq\",\n        min_score=75\n    shell:\n        \"\"\"\n        # Extract with preset\n        cicada metadata extract {input.fastq} \\\n          --preset {params.preset} \\\n          --format json \\\n          --output {output.metadata}\n\n        # Generate validation report\n        cicada metadata validate {output.metadata} \\\n          --preset {params.preset} \\\n          --format table &gt; {output.report}\n\n        # Check quality score\n        score=$(jq -r '.quality_score' {output.metadata})\n        if [ \"$score\" -lt \"{params.min_score}\" ]; then\n            echo \"Quality score $score below threshold {params.min_score}\"\n            exit 1\n        fi\n        \"\"\"\n</code></pre>"},{"location":"PRESETS/#python-script","title":"Python Script","text":"<pre><code>#!/usr/bin/env python3\nimport json\nimport subprocess\nimport sys\n\ndef validate_with_preset(fastq_file, preset_id, min_score=75):\n    \"\"\"Extract and validate FASTQ metadata with preset.\"\"\"\n\n    # Extract metadata\n    result = subprocess.run([\n        \"cicada\", \"metadata\", \"extract\", fastq_file,\n        \"--preset\", preset_id,\n        \"--format\", \"json\"\n    ], capture_output=True, text=True)\n\n    if result.returncode != 0:\n        print(f\"Extraction failed: {result.stderr}\", file=sys.stderr)\n        return False\n\n    # Parse metadata\n    metadata = json.loads(result.stdout)\n    score = metadata.get(\"quality_score\", 0)\n\n    # Check score\n    if score &lt; min_score:\n        print(f\"Quality score {score} below threshold {min_score}\")\n        print(f\"Missing fields: {metadata.get('validation_warnings', [])}\")\n        return False\n\n    print(f\"\u2705 Validation passed (score: {score})\")\n    return True\n\n# Usage\nif __name__ == \"__main__\":\n    success = validate_with_preset(\n        fastq_file=\"sample_R1.fastq.gz\",\n        preset_id=\"illumina-novaseq\",\n        min_score=75\n    )\n    sys.exit(0 if success else 1)\n</code></pre>"},{"location":"PRESETS/#bash-script","title":"Bash Script","text":"<pre><code>#!/bin/bash\n# Batch validation with presets\n\nPRESET=\"illumina-novaseq\"\nMIN_SCORE=75\nOUTPUT_DIR=\"validated_metadata\"\n\nmkdir -p \"$OUTPUT_DIR\"\n\nfor fastq in data/*.fastq.gz; do\n    basename=$(basename \"$fastq\" .fastq.gz)\n    echo \"Processing $basename...\"\n\n    # Extract with preset\n    cicada metadata extract \"$fastq\" \\\n      --preset \"$PRESET\" \\\n      --format json \\\n      --output \"$OUTPUT_DIR/${basename}.json\"\n\n    # Check quality score\n    score=$(jq -r '.quality_score' \"$OUTPUT_DIR/${basename}.json\")\n\n    if (( $(echo \"$score &lt; $MIN_SCORE\" | bc -l) )); then\n        echo \"\u274c $basename: Score $score below threshold\"\n        # Log warnings\n        jq -r '.validation_warnings[]' \"$OUTPUT_DIR/${basename}.json\" \\\n          &gt;&gt; \"$OUTPUT_DIR/warnings.log\"\n    else\n        echo \"\u2705 $basename: Score $score\"\n    fi\ndone\n\necho \"Validation complete. Check $OUTPUT_DIR/warnings.log for issues.\"\n</code></pre>"},{"location":"PRESETS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"PRESETS/#preset-not-found","title":"\"Preset not found\"","text":"<p>Error: <pre><code>Error: preset 'illumina-nova' not found\n</code></pre></p> <p>Solution: <pre><code># List available presets\ncicada metadata preset list\n\n# Use exact preset ID\ncicada metadata validate data.json --preset illumina-novaseq\n</code></pre></p>"},{"location":"PRESETS/#low-quality-score","title":"Low Quality Score","text":"<p>Issue: Quality score below 75</p> <p>Diagnosis: <pre><code># Show validation details\ncicada metadata validate metadata.json --preset illumina-novaseq\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Add missing required fields (if score &lt; 60):    <pre><code># Check which required fields are missing\ncicada metadata validate metadata.json --preset illumina-novaseq\n\n# Add fields via enrichment file\necho \"instrument_model: NovaSeq 6000\" &gt; enrich.yaml\ncicada metadata validate metadata.json \\\n  --preset illumina-novaseq \\\n  --enrich enrich.yaml\n</code></pre></p> </li> <li> <p>Add optional fields (if score 60-89):    <pre><code># enrich.yaml\nrun_id: RUN_12345\nflowcell_id: FC_67890\nlane: 2\nbarcode: AGTCACTA\nmean_quality_score: 36.5\n</code></pre></p> </li> </ol>"},{"location":"PRESETS/#field-type-mismatch","title":"Field Type Mismatch","text":"<p>Error: <pre><code>Validation Error: Field 'total_reads' expected type 'integer', got 'string'\n</code></pre></p> <p>Solution:</p> <p>Check metadata format: <pre><code># View metadata\ncat metadata.json | jq '.total_reads'\n\n# Should be: 1000000 (number)\n# Not: \"1000000\" (string)\n</code></pre></p> <p>Fix in enrichment file: <pre><code># Correct (YAML auto-converts)\ntotal_reads: 1000000\n\n# Incorrect\ntotal_reads: \"1000000\"\n</code></pre></p>"},{"location":"PRESETS/#strict-validation-failures","title":"Strict Validation Failures","text":"<p>Issue: Validation fails in strict mode but passes in lenient mode</p> <p>Command: <pre><code># Fails\ncicada metadata validate metadata.json --preset illumina-novaseq --strict\n\n# Passes\ncicada metadata validate metadata.json --preset illumina-novaseq\n</code></pre></p> <p>Solution:</p> <p>Strict mode treats warnings as errors. Add missing optional fields: <pre><code># See which optional fields are missing\ncicada metadata validate metadata.json --preset illumina-novaseq\n\n# Add recommended fields\ncat &gt; enrich.yaml &lt;&lt;EOF\nrun_id: RUN_123\nquality_encoding: Phred+33\nis_paired_end: true\nEOF\n\n# Re-validate with enrichment\ncicada metadata validate metadata.json \\\n  --preset illumina-novaseq \\\n  --enrich enrich.yaml \\\n  --strict\n</code></pre></p>"},{"location":"PRESETS/#wrong-preset-selected","title":"Wrong Preset Selected","text":"<p>Issue: Validation fails because wrong preset was used</p> <p>Diagnosis: <pre><code># File is from MiSeq, but validated with NovaSeq preset\ncicada metadata validate metadata.json --preset illumina-novaseq\n# Error: instrument_model \"MiSeq\" doesn't match preset \"NovaSeq 6000\"\n</code></pre></p> <p>Solution: <pre><code># Use correct preset\ncicada metadata validate metadata.json --preset illumina-miseq\n\n# Or use generic preset\ncicada metadata validate metadata.json --preset generic-sequencing\n</code></pre></p>"},{"location":"PRESETS/#best-practices","title":"Best Practices","text":""},{"location":"PRESETS/#1-use-specific-presets-when-possible","title":"1. Use Specific Presets When Possible","text":"<p>\u2705 Recommended: <pre><code>cicada metadata extract data.fastq --preset illumina-novaseq\n</code></pre></p> <p>\u274c Avoid (unless necessary): <pre><code>cicada metadata extract data.fastq --preset generic-sequencing\n</code></pre></p> <p>Specific presets provide better validation and higher quality scores.</p>"},{"location":"PRESETS/#2-validate-early","title":"2. Validate Early","text":"<p>Validate metadata immediately after extraction: <pre><code># Good workflow\ncicada metadata extract sample.fastq --preset illumina-novaseq\n# Reviews validation results immediately\n\n# Bad workflow\ncicada metadata extract sample.fastq\n# ... weeks later ...\ncicada doi prepare sample.fastq  # Validation fails at DOI preparation\n</code></pre></p>"},{"location":"PRESETS/#3-target-80-quality-scores","title":"3. Target 80+ Quality Scores","text":"<p>Aim for quality scores above 80 for publication: - 90-100: Excellent - include all optional fields - 80-89: Good - acceptable for most journals - 70-79: Adequate - may need additional fields - &lt; 70: Insufficient - add more metadata</p>"},{"location":"PRESETS/#4-maintain-consistency","title":"4. Maintain Consistency","text":"<p>Use the same preset for all files from the same instrument: <pre><code># Define preset once\nPRESET=\"illumina-novaseq\"\n\n# Use consistently\nfor file in *.fastq.gz; do\n    cicada metadata extract \"$file\" --preset \"$PRESET\"\ndone\n</code></pre></p>"},{"location":"PRESETS/#5-document-preset-choice","title":"5. Document Preset Choice","text":"<p>Include preset information in project documentation: <pre><code>## Metadata Standards\n\n- Instrument: Illumina NovaSeq 6000\n- Preset: `illumina-novaseq`\n- Minimum Quality Score: 80\n- Validation: All files validated before DOI preparation\n</code></pre></p>"},{"location":"PRESETS/#6-use-enrichment-files","title":"6. Use Enrichment Files","text":"<p>Create project-wide enrichment files: <pre><code># project_metadata.yaml\nrun_id: PROJECT_2025_001\nquality_encoding: Phred+33\ninstrument_manufacturer: Illumina\ninstrument_model: NovaSeq 6000\nsequencing_platform: Illumina\n</code></pre></p> <p>Apply to all files: <pre><code>cicada metadata extract sample.fastq \\\n  --preset illumina-novaseq \\\n  --enrich project_metadata.yaml\n</code></pre></p>"},{"location":"PRESETS/#future-enhancements-v030","title":"Future Enhancements (v0.3.0+)","text":"<p>Planned features for future releases:</p>"},{"location":"PRESETS/#custom-presets","title":"Custom Presets","text":"<p>Create your own presets: <pre><code># Create preset from template\ncicada metadata preset create \\\n  --name custom-nanopore \\\n  --type sequencing \\\n  --manufacturer \"Oxford Nanopore\" \\\n  --template nanopore-template.yaml\n\n# Use custom preset\ncicada metadata validate data.fastq --preset custom-nanopore\n</code></pre></p>"},{"location":"PRESETS/#preset-templates","title":"Preset Templates","text":"<p>Generate metadata templates: <pre><code># Generate template from preset\ncicada metadata preset template illumina-novaseq &gt; template.yaml\n\n# Fill in template\nvim template.yaml\n\n# Validate against preset\ncicada metadata validate template.yaml --preset illumina-novaseq\n</code></pre></p>"},{"location":"PRESETS/#preset-validation-rules","title":"Preset Validation Rules","text":"<p>Define custom validation rules: <pre><code># custom-preset.yaml\nname: custom-miseq\nmanufacturer: Illumina\ninstrument_model: MiSeq\nrequired_fields:\n  - format\n  - total_reads\noptional_fields:\n  - run_id\n  - barcode\nvalidation_rules:\n  read_length:\n    min: 50\n    max: 300\n  quality_score:\n    min: 20\n</code></pre></p>"},{"location":"PRESETS/#preset-importexport","title":"Preset Import/Export","text":"<p>Share presets across teams: <pre><code># Export preset\ncicada metadata preset export illumina-novaseq &gt; preset.yaml\n\n# Import preset\ncicada metadata preset import preset.yaml\n\n# Share with team\ngit add presets/illumina-novaseq.yaml\ngit commit -m \"Add NovaSeq preset for lab\"\n</code></pre></p>"},{"location":"PRESETS/#related-documentation","title":"Related Documentation","text":"<ul> <li>Metadata Extraction Guide: Extracting metadata from files</li> <li>DOI Workflow Guide: Preparing metadata for DOI registration</li> <li>User Scenarios: Real-world preset usage examples</li> <li>Integration Testing: Testing preset validation</li> </ul>"},{"location":"PRESETS/#support","title":"Support","text":"<p>For questions or issues with presets:</p> <ul> <li>Documentation: See guides above</li> <li>Issues: Report problems at https://github.com/scttfrdmn/cicada/issues</li> <li>Feature Requests: Suggest new presets or improvements</li> </ul>"},{"location":"PRESETS/#version-history","title":"Version History","text":"<ul> <li>v0.2.0 (Current): Initial release with 8 default presets</li> <li>v0.3.0 (Planned): Custom presets, templates, advanced validation rules</li> </ul>"},{"location":"PROVIDERS/","title":"DOI Provider Setup Guide","text":"<p>Note: Provider integration is an optional advanced feature for labs that need to publish datasets. Most Cicada usage involves core data management features (storage, sync, metadata extraction). This guide is only relevant if you need to mint DOIs for dataset publication.</p> <p>Current Status (v0.2.0): Framework for provider integration is implemented. Live API integration planned for v0.4.0+.</p> <p>Status: v0.2.0 Documentation (Framework Ready, Live APIs in v0.4.0+) Audience: Lab managers, data managers, researchers publishing datasets</p>"},{"location":"PROVIDERS/#overview","title":"Overview","text":"<p>For labs that need to publish datasets, Cicada will support multiple DOI registration providers. Each provider offers different features, pricing, and repository integration.</p> <p>Supported Providers (v0.2.0):</p> <ul> <li>DataCite: Direct DOI registration service for institutions</li> <li>Zenodo: Free repository with DOI assignment (CERN)</li> </ul> <p>Planned Providers (v0.3.0+): - Dryad - Figshare - Mendeley Data</p> <p>Key Concepts:</p> <ul> <li>DOI (Digital Object Identifier): Persistent identifier for datasets</li> <li>Sandbox Environment: Testing environment with fake DOIs for development</li> <li>Production Environment: Live environment creating real, permanent DOIs</li> <li>Repository: Storage service hosting dataset files</li> <li>Metadata Schema: DataCite Metadata Schema v4.5</li> </ul>"},{"location":"PROVIDERS/#quick-start","title":"Quick Start","text":""},{"location":"PROVIDERS/#option-1-zenodo-recommended-for-small-labs","title":"Option 1: Zenodo (Recommended for Small Labs)","text":"<p>Advantages: - Free (unlimited datasets) - Integrated repository (storage + DOI) - No institutional membership required - Sandbox available for testing</p> <p>Setup Time: 15 minutes</p> <ol> <li> <p>Create Sandbox Account (for testing):    <pre><code># Visit https://sandbox.zenodo.org\n# Sign up with GitHub, ORCID, or email\n</code></pre></p> </li> <li> <p>Generate API Token:</p> </li> <li>Settings \u2192 Applications \u2192 Personal access tokens</li> <li>New token \u2192 Name: \"Cicada\" \u2192 Scopes: <code>deposit:write</code>, <code>deposit:actions</code></li> <li> <p>Copy token (shown once)</p> </li> <li> <p>Configure Cicada:    <pre><code>cicada config set provider zenodo-sandbox\ncicada config set zenodo.token YOUR_TOKEN_HERE\n</code></pre></p> </li> <li> <p>Test Publication:    <pre><code>cicada doi prepare sample.fastq \\\n  --enrich metadata.yaml \\\n  --provider zenodo-sandbox \\\n  --upload\n</code></pre></p> </li> <li> <p>Production Setup (when ready):</p> </li> <li>Create account at https://zenodo.org (same process)</li> <li>Generate production token</li> <li>Configure: <code>cicada config set provider zenodo</code></li> </ol>"},{"location":"PROVIDERS/#option-2-datacite-for-institutions","title":"Option 2: DataCite (For Institutions)","text":"<p>Advantages: - Direct DOI control - Custom DOI prefix (e.g., 10.12345/...) - Metadata-only registration (host files separately) - Higher credibility with some journals</p> <p>Prerequisites: - Institutional DataCite membership ($$$) - Repository ID from your institution - API credentials from institution</p> <p>Setup Time: 1-2 hours (including institutional approval)</p> <ol> <li>Get Institutional Credentials:</li> <li>Contact your library or IT department</li> <li>Request: Repository ID, Username, Password</li> <li> <p>Ask about sandbox vs production access</p> </li> <li> <p>Configure Cicada:    <pre><code>cicada config set provider datacite-sandbox\ncicada config set datacite.repository_id YOUR_REPO_ID\ncicada config set datacite.username YOUR_USERNAME\ncicada config set datacite.password YOUR_PASSWORD\n</code></pre></p> </li> <li> <p>Test Registration:    <pre><code>cicada doi prepare sample.fastq \\\n  --enrich metadata.yaml \\\n  --provider datacite-sandbox\n</code></pre></p> </li> </ol>"},{"location":"PROVIDERS/#datacite-setup","title":"DataCite Setup","text":""},{"location":"PROVIDERS/#understanding-datacite","title":"Understanding DataCite","text":"<p>DataCite is a global DOI registration agency focused on research data. Unlike Zenodo, DataCite provides only DOI registration\u2014you must host files separately.</p> <p>When to Use DataCite: - Your institution has DataCite membership - You need custom DOI prefixes - You have separate file storage (institutional repository, S3) - You require direct control over DOI metadata</p> <p>Cost: - Institutional membership: $5,000-10,000/year (varies) - Per-DOI: Usually unlimited within membership</p>"},{"location":"PROVIDERS/#sandbox-environment","title":"Sandbox Environment","text":"<p>DataCite provides a test environment for development and validation.</p> <p>Sandbox Details: - URL: https://api.test.datacite.org - DOIs: Test prefix <code>10.82041</code> (not resolvable outside sandbox) - Purpose: Testing workflows without creating real DOIs - Persistence: Data may be deleted periodically</p>"},{"location":"PROVIDERS/#production-environment","title":"Production Environment","text":"<p>Production Details: - URL: https://api.datacite.org - DOIs: Your institution's prefix (e.g., <code>10.12345</code>) - Purpose: Creating permanent, citable DOIs - Persistence: DOIs are permanent and cannot be deleted</p>"},{"location":"PROVIDERS/#step-by-step-setup","title":"Step-by-Step Setup","text":""},{"location":"PROVIDERS/#1-obtain-credentials","title":"1. Obtain Credentials","text":"<p>Contact your institution's library or research data management team:</p> <p>Request Template: <pre><code>Subject: DataCite API Access for Research Data Management\n\nHello,\n\nI'm setting up Cicada (https://github.com/scttfrdmn/cicada) to automate\nDOI registration for our lab's research datasets.\n\nCould you please provide:\n1. DataCite Repository ID\n2. API username\n3. API password\n4. Sandbox credentials (for testing)\n5. DOI prefix assigned to our lab/department\n\nThank you,\n[Your Name]\n</code></pre></p> <p>What You'll Receive: - Repository ID: <code>INST.DEPT</code> (e.g., <code>MIT.BIO</code>) - Username: Usually email or <code>repoID.username</code> - Password: Generated password - DOI Prefix: <code>10.XXXXX</code></p>"},{"location":"PROVIDERS/#2-test-sandbox-access","title":"2. Test Sandbox Access","text":"<p>Verify credentials work:</p> <pre><code># Set sandbox configuration\nexport DATACITE_REPO_ID=\"INST.DEPT\"\nexport DATACITE_USERNAME=\"your_username\"\nexport DATACITE_PASSWORD=\"your_password\"\n\n# Test API access\ncurl -u \"$DATACITE_USERNAME:$DATACITE_PASSWORD\" \\\n  https://api.test.datacite.org/dois \\\n  | jq '.'\n\n# Should return JSON with your test DOIs\n</code></pre>"},{"location":"PROVIDERS/#3-configure-cicada-sandbox","title":"3. Configure Cicada (Sandbox)","text":"<pre><code># Configure for testing\ncicada config set provider datacite-sandbox\ncicada config set datacite.repository_id \"$DATACITE_REPO_ID\"\ncicada config set datacite.username \"$DATACITE_USERNAME\"\ncicada config set datacite.password \"$DATACITE_PASSWORD\"\n\n# Verify configuration\ncicada config list\n</code></pre> <p>Config File Location: <code>~/.config/cicada/config.yaml</code></p> <p>Config File Contents: <pre><code>provider: datacite-sandbox\ndatacite:\n  repository_id: INST.DEPT\n  username: your_username\n  password: your_password\n  sandbox: true\n</code></pre></p>"},{"location":"PROVIDERS/#4-test-doi-creation","title":"4. Test DOI Creation","text":"<p>Create a test DOI with sample data:</p> <pre><code># Create test FASTQ file\necho \"@SEQ_ID\nACGTACGTACGTACGT\n+\nIIIIIIIIIIIIIIII\" &gt; test.fastq\n\n# Create enrichment metadata\ncat &gt; enrich.yaml &lt;&lt;EOF\ntitle: \"Test Dataset for DataCite Integration\"\nauthors:\n  - name: Test Researcher\n    orcid: 0000-0001-2345-6789\n    affiliation: Test University\ndescription: \"This is a test dataset for validating DataCite integration\"\npublisher: Test University\npublication_year: 2025\nEOF\n\n# Prepare DOI (draft state)\ncicada doi prepare test.fastq \\\n  --enrich enrich.yaml \\\n  --provider datacite-sandbox \\\n  --output doi.json\n\n# Review DOI\ncat doi.json | jq '.doi'\n# Output: \"10.82041/test-dataset-12345\"\n</code></pre>"},{"location":"PROVIDERS/#5-publish-doi-sandbox","title":"5. Publish DOI (Sandbox)","text":"<p>\u26a0\ufe0f Important: In sandbox, DOIs are temporary. In production, they're permanent.</p> <pre><code># Publish DOI (makes it findable)\ncicada doi publish test.fastq \\\n  --metadata doi.json \\\n  --provider datacite-sandbox\n\n# Verify publication\ncicada doi status 10.82041/test-dataset-12345 \\\n  --provider datacite-sandbox\n\n# Output:\n# DOI: 10.82041/test-dataset-12345\n# State: findable\n# URL: https://handle.test.datacite.org/10.82041/test-dataset-12345\n</code></pre>"},{"location":"PROVIDERS/#6-configure-production","title":"6. Configure Production","text":"<p>\u26a0\ufe0f Only after successful sandbox testing</p> <pre><code># Switch to production\ncicada config set provider datacite\ncicada config set datacite.sandbox false\n\n# Production credentials (may be same as sandbox)\ncicada config set datacite.repository_id \"$PROD_REPO_ID\"\ncicada config set datacite.username \"$PROD_USERNAME\"\ncicada config set datacite.password \"$PROD_PASSWORD\"\n\n# Verify production access\ncicada doi list --provider datacite\n</code></pre>"},{"location":"PROVIDERS/#datacite-workflows","title":"DataCite Workflows","text":""},{"location":"PROVIDERS/#draft-review-publish","title":"Draft \u2192 Review \u2192 Publish","text":"<pre><code># Step 1: Create draft DOI\ncicada doi prepare dataset.fastq \\\n  --enrich metadata.yaml \\\n  --provider datacite \\\n  --state draft \\\n  --output doi.json\n\n# Step 2: Review metadata\ncat doi.json | jq '.'\n\n# Step 3: Update if needed\ncicada doi update 10.12345/dataset-001 \\\n  --metadata updated_metadata.yaml \\\n  --provider datacite\n\n# Step 4: Publish when ready\ncicada doi publish 10.12345/dataset-001 \\\n  --provider datacite\n</code></pre>"},{"location":"PROVIDERS/#metadata-only-registration","title":"Metadata-Only Registration","text":"<p>DataCite doesn't host files\u2014provide URL to where files are stored:</p> <pre><code># Prepare DOI with file URL\ncat &gt; enrich.yaml &lt;&lt;EOF\ntitle: \"My Dataset\"\nauthors:\n  - name: Researcher Name\ndescription: \"Dataset description\"\nurl: \"https://mylab.edu/datasets/dataset-001\"\nEOF\n\ncicada doi prepare dataset.fastq \\\n  --enrich enrich.yaml \\\n  --provider datacite \\\n  --no-upload  # Don't try to upload files\n</code></pre>"},{"location":"PROVIDERS/#zenodo-setup","title":"Zenodo Setup","text":""},{"location":"PROVIDERS/#understanding-zenodo","title":"Understanding Zenodo","text":"<p>Zenodo is a free, open-access repository operated by CERN. It provides both file storage and DOI registration.</p> <p>When to Use Zenodo: - Your lab doesn't have DataCite membership - You want free, unlimited dataset hosting - You need a simple, integrated solution - You publish openly and want broad discoverability</p> <p>Cost: - Free (up to 50 GB per dataset) - Larger datasets: Contact Zenodo for approval</p>"},{"location":"PROVIDERS/#sandbox-environment_1","title":"Sandbox Environment","text":"<p>Zenodo provides a complete sandbox for testing.</p> <p>Sandbox Details: - URL: https://sandbox.zenodo.org - DOIs: Test DOIs <code>10.5072/zenodo.XXXXXX</code> - Purpose: Full-featured testing environment - Storage: Same as production (50 GB per dataset) - Persistence: Data retained indefinitely</p>"},{"location":"PROVIDERS/#production-environment_1","title":"Production Environment","text":"<p>Production Details: - URL: https://zenodo.org - DOIs: Real DOIs <code>10.5281/zenodo.XXXXXX</code> - Indexing: Indexed by Google Scholar, OpenAIRE, DataCite - Persistence: Permanent (files and metadata cannot be deleted)</p>"},{"location":"PROVIDERS/#step-by-step-setup_1","title":"Step-by-Step Setup","text":""},{"location":"PROVIDERS/#1-create-sandbox-account","title":"1. Create Sandbox Account","text":"<p>Visit https://sandbox.zenodo.org:</p> <ol> <li>Click Sign up (top right)</li> <li>Choose authentication:</li> <li>GitHub (recommended for developers)</li> <li>ORCID (recommended for researchers)</li> <li>Email (basic signup)</li> <li>Complete profile:</li> <li>Full name</li> <li>Affiliation</li> <li>ORCID (if not using ORCID login)</li> </ol>"},{"location":"PROVIDERS/#2-generate-api-token-sandbox","title":"2. Generate API Token (Sandbox)","text":"<ol> <li>Click profile icon \u2192 Applications</li> <li>Scroll to Personal access tokens</li> <li>Click New token</li> <li>Configure:</li> <li>Name: <code>Cicada Testing</code></li> <li>Scopes:<ul> <li>\u2705 <code>deposit:write</code> (create and update deposits)</li> <li>\u2705 <code>deposit:actions</code> (publish deposits)</li> </ul> </li> <li>Click Create</li> <li>Copy token immediately (shown once)</li> </ol> <p>Token Format: <code>eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...</code> (long string)</p>"},{"location":"PROVIDERS/#3-test-api-access","title":"3. Test API Access","text":"<pre><code># Set token\nexport ZENODO_TOKEN=\"your_token_here\"\n\n# List your deposits\ncurl \"https://sandbox.zenodo.org/api/deposit/depositions\" \\\n  -H \"Authorization: Bearer $ZENODO_TOKEN\" \\\n  | jq '.'\n\n# Should return empty array: []\n</code></pre>"},{"location":"PROVIDERS/#4-configure-cicada-sandbox","title":"4. Configure Cicada (Sandbox)","text":"<pre><code># Configure sandbox\ncicada config set provider zenodo-sandbox\ncicada config set zenodo.token \"$ZENODO_TOKEN\"\ncicada config set zenodo.sandbox true\n\n# Verify\ncicada config list\n</code></pre> <p>Config File: <code>~/.config/cicada/config.yaml</code> <pre><code>provider: zenodo-sandbox\nzenodo:\n  token: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\n  sandbox: true\n</code></pre></p>"},{"location":"PROVIDERS/#5-create-test-upload","title":"5. Create Test Upload","text":"<pre><code># Create test data\necho \"@SEQ_ID\nACGTACGTACGTACGT\n+\nIIIIIIIIIIIIIIII\" &gt; test_R1.fastq\n\n# Create metadata\ncat &gt; metadata.yaml &lt;&lt;EOF\ntitle: \"Test Sequencing Dataset\"\nauthors:\n  - name: Your Name\n    orcid: 0000-0001-2345-6789\n    affiliation: Your University\ndescription: |\n  Test dataset for Zenodo integration testing.\n  This is a small FASTQ file for validation purposes.\nkeywords:\n  - test\n  - sequencing\n  - FASTQ\nupload_type: dataset\naccess_right: open\nlicense: cc-by-4.0\nEOF\n\n# Prepare and upload\ncicada doi prepare test_R1.fastq \\\n  --enrich metadata.yaml \\\n  --provider zenodo-sandbox \\\n  --upload \\\n  --output zenodo_response.json\n\n# Check response\ncat zenodo_response.json | jq '{doi, title, state, links}'\n</code></pre> <p>Response: <pre><code>{\n  \"doi\": \"10.5072/zenodo.123456\",\n  \"title\": \"Test Sequencing Dataset\",\n  \"state\": \"unsubmitted\",\n  \"links\": {\n    \"record\": \"https://sandbox.zenodo.org/record/123456\",\n    \"bucket\": \"https://sandbox.zenodo.org/api/files/...\",\n    \"publish\": \"https://sandbox.zenodo.org/api/deposit/depositions/123456/actions/publish\"\n  }\n}\n</code></pre></p>"},{"location":"PROVIDERS/#6-publish-deposition","title":"6. Publish Deposition","text":"<pre><code># Publish to make DOI active\ncicada doi publish 10.5072/zenodo.123456 \\\n  --provider zenodo-sandbox\n\n# Visit record in browser\nopen \"https://sandbox.zenodo.org/record/123456\"\n</code></pre>"},{"location":"PROVIDERS/#7-production-setup","title":"7. Production Setup","text":"<p>\u26a0\ufe0f Only after successful sandbox testing</p> <ol> <li>Create Production Account: https://zenodo.org (same process)</li> <li>Generate Production Token: Same steps as sandbox</li> <li>Configure Cicada:    <pre><code>cicada config set provider zenodo\ncicado config set zenodo.token \"$PROD_TOKEN\"\ncicada config set zenodo.sandbox false\n</code></pre></li> </ol>"},{"location":"PROVIDERS/#zenodo-workflows","title":"Zenodo Workflows","text":""},{"location":"PROVIDERS/#complete-upload-workflow","title":"Complete Upload Workflow","text":"<pre><code># Step 1: Prepare files\ncicada metadata extract sample_R1.fastq.gz --output metadata.json\ncicada metadata extract sample_R2.fastq.gz --output metadata.json --append\n\n# Step 2: Create enrichment\ncat &gt; enrich.yaml &lt;&lt;EOF\ntitle: \"Bacterial genome sequencing - Sample 042\"\nauthors:\n  - name: Dr. Jane Smith\n    orcid: 0000-0002-1234-5678\n    affiliation: Department of Microbiology, State University\ndescription: |\n  Whole genome sequencing of antibiotic-resistant E. coli strain 042.\n  Sequenced on Illumina NovaSeq 6000, 2x150bp paired-end reads.\nkeywords:\n  - whole genome sequencing\n  - E. coli\n  - antibiotic resistance\nupload_type: dataset\naccess_right: open\nlicense: cc-by-4.0\nEOF\n\n# Step 3: Prepare DOI with both files\ncicada doi prepare sample_R1.fastq.gz sample_R2.fastq.gz \\\n  --enrich enrich.yaml \\\n  --provider zenodo \\\n  --upload \\\n  --output doi_response.json\n\n# Step 4: Review\ncat doi_response.json | jq '.'\n\n# Step 5: Publish\ndoi=$(jq -r '.doi' doi_response.json)\ncicada doi publish \"$doi\" --provider zenodo\n\n# Step 6: Get URL\necho \"Dataset published: https://zenodo.org/doi/$doi\"\n</code></pre>"},{"location":"PROVIDERS/#update-published-record","title":"Update Published Record","text":"<pre><code># Create new version\ncicada doi version 10.5281/zenodo.123456 \\\n  --provider zenodo \\\n  --output new_version.json\n\n# Upload new files\nnew_doi=$(jq -r '.doi' new_version.json)\ncicada doi upload \"$new_doi\" updated_data.fastq \\\n  --provider zenodo\n\n# Publish new version\ncicada doi publish \"$new_doi\" --provider zenodo\n</code></pre>"},{"location":"PROVIDERS/#provider-comparison","title":"Provider Comparison","text":"Feature DataCite Zenodo Dryad (v0.3.0) Figshare (v0.3.0) Cost Institutional membership Free $120/dataset Free (limited) Storage None (metadata only) 50 GB/dataset Unlimited 20 GB free DOI Prefix Custom (10.XXXXX) Fixed (10.5281) Fixed (10.5061) Fixed (10.6084) Sandbox \u2705 Yes \u2705 Yes \u2705 Yes \u274c No File Hosting \u274c No \u2705 Yes \u2705 Yes \u2705 Yes API Access \u2705 Full \u2705 Full \u2705 Full \u2705 Full Setup Time 1-2 hours 15 minutes 30 minutes 30 minutes Best For Institutions Small labs Publications Figures/datasets"},{"location":"PROVIDERS/#choosing-a-provider","title":"Choosing a Provider","text":"<p>Choose DataCite if: - Your institution has membership - You need custom DOI prefix - You host files separately (S3, institutional repository) - You want direct control over metadata</p> <p>Choose Zenodo if: - You want free, simple solution - You need integrated file hosting - You publish open access - You're a small lab or individual researcher</p> <p>Choose Dryad if (v0.3.0): - You're publishing in journals requiring Dryad (e.g., Evolution, Am Nat) - You have large datasets (&gt; 50 GB) - You want curated, journal-integrated submission</p> <p>Choose Figshare if (v0.3.0): - You have figures, posters, presentations (not just data) - You want institutional Figshare integration - You need file versioning and previews</p>"},{"location":"PROVIDERS/#configuration-management","title":"Configuration Management","text":""},{"location":"PROVIDERS/#configuration-file-location","title":"Configuration File Location","text":"<ul> <li>Linux/macOS: <code>~/.config/cicada/config.yaml</code></li> <li>Windows: <code>%APPDATA%\\cicada\\config.yaml</code></li> </ul>"},{"location":"PROVIDERS/#configuration-commands","title":"Configuration Commands","text":"<pre><code># View all settings\ncicada config list\n\n# Set individual values\ncicada config set provider zenodo\ncicada config set zenodo.token \"your_token\"\n\n# Get specific value\ncicada config get provider\n\n# Reset to defaults\ncicada config reset\n\n# Edit config file directly\nvim ~/.config/cicada/config.yaml\n</code></pre>"},{"location":"PROVIDERS/#multi-provider-configuration","title":"Multi-Provider Configuration","text":"<p>Configure multiple providers and switch as needed:</p> <p>config.yaml: <pre><code># Active provider\nprovider: zenodo\n\n# DataCite (production)\ndatacite:\n  repository_id: MIT.BIO\n  username: mit_bio_user\n  password: secret_password\n  sandbox: false\n\n# DataCite (sandbox)\ndatacite_sandbox:\n  repository_id: MIT.BIO\n  username: mit_bio_user\n  password: sandbox_password\n  sandbox: true\n\n# Zenodo (production)\nzenodo:\n  token: prod_token_here\n  sandbox: false\n\n# Zenodo (sandbox)\nzenodo_sandbox:\n  token: sandbox_token_here\n  sandbox: true\n</code></pre></p> <p>Switch providers: <pre><code># Use Zenodo sandbox\ncicada config set provider zenodo-sandbox\n\n# Use DataCite production\ncicada config set provider datacite\n\n# Use Zenodo production\ncicada config set provider zenodo\n</code></pre></p>"},{"location":"PROVIDERS/#environment-variables","title":"Environment Variables","text":"<p>Override config file with environment variables:</p> <pre><code># Provider\nexport CICADA_PROVIDER=zenodo-sandbox\n\n# Zenodo\nexport ZENODO_TOKEN=your_token\nexport ZENODO_SANDBOX=true\n\n# DataCite\nexport DATACITE_REPOSITORY_ID=INST.DEPT\nexport DATACITE_USERNAME=username\nexport DATACITE_PASSWORD=password\nexport DATACITE_SANDBOX=true\n\n# Run command\ncicada doi prepare data.fastq --enrich metadata.yaml --upload\n</code></pre> <p>Environment variables take precedence over config file.</p>"},{"location":"PROVIDERS/#testing-workflows","title":"Testing Workflows","text":""},{"location":"PROVIDERS/#pre-production-testing-checklist","title":"Pre-Production Testing Checklist","text":"<p>Before publishing real DOIs, test thoroughly in sandbox:</p> <ul> <li> <p> API Access: Verify credentials work   <pre><code>cicada doi list --provider zenodo-sandbox\n</code></pre></p> </li> <li> <p> Metadata Validation: Check quality score   <pre><code>cicada doi validate data.fastq --enrich metadata.yaml\n</code></pre></p> </li> <li> <p> Draft Creation: Create draft DOI   <pre><code>cicada doi prepare data.fastq --enrich metadata.yaml --provider zenodo-sandbox\n</code></pre></p> </li> <li> <p> File Upload: Upload test files (Zenodo only)   <pre><code>cicada doi prepare data.fastq --enrich metadata.yaml --upload --provider zenodo-sandbox\n</code></pre></p> </li> <li> <p> Publication: Publish test DOI   <pre><code>cicada doi publish 10.5072/zenodo.123456 --provider zenodo-sandbox\n</code></pre></p> </li> <li> <p> URL Resolution: Verify DOI resolves   <pre><code>open \"https://sandbox.zenodo.org/doi/10.5072/zenodo.123456\"\n</code></pre></p> </li> <li> <p> Metadata Display: Check all fields display correctly</p> </li> <li> <p> File Download: Download files and verify integrity</p> </li> </ul>"},{"location":"PROVIDERS/#automated-testing","title":"Automated Testing","text":"<p>Create test script for CI/CD:</p> <pre><code>#!/bin/bash\n# test_doi_workflow.sh\n\nset -e  # Exit on error\n\nPROVIDER=\"zenodo-sandbox\"\nTEST_FILE=\"test_data.fastq\"\nMETADATA=\"test_metadata.yaml\"\n\necho \"Testing DOI workflow with $PROVIDER...\"\n\n# Step 1: Validate\necho \"1. Validating metadata...\"\ncicada doi validate \"$TEST_FILE\" --enrich \"$METADATA\"\n\n# Step 2: Prepare\necho \"2. Creating draft DOI...\"\ncicada doi prepare \"$TEST_FILE\" \\\n  --enrich \"$METADATA\" \\\n  --provider \"$PROVIDER\" \\\n  --upload \\\n  --output response.json\n\n# Step 3: Extract DOI\nDOI=$(jq -r '.doi' response.json)\necho \"Created DOI: $DOI\"\n\n# Step 4: Publish\necho \"3. Publishing DOI...\"\ncicada doi publish \"$DOI\" --provider \"$PROVIDER\"\n\n# Step 5: Verify\necho \"4. Verifying publication...\"\ncicada doi status \"$DOI\" --provider \"$PROVIDER\"\n\necho \"\u2705 Test workflow completed successfully\"\n</code></pre>"},{"location":"PROVIDERS/#publishing-workflows","title":"Publishing Workflows","text":""},{"location":"PROVIDERS/#workflow-1-pre-publication-dataset","title":"Workflow 1: Pre-Publication Dataset","text":"<p>Publish dataset before paper submission:</p> <pre><code># Step 1: Extract and validate metadata\ncicada metadata extract data_R1.fastq.gz --preset illumina-novaseq\ncicada metadata extract data_R2.fastq.gz --preset illumina-novaseq\n\n# Step 2: Create comprehensive metadata\ncat &gt; publication_metadata.yaml &lt;&lt;EOF\ntitle: \"Genome-wide association study of antibiotic resistance in E. coli\"\nauthors:\n  - name: Dr. Jane Smith\n    orcid: 0000-0002-1234-5678\n    affiliation: State University\n  - name: Dr. John Doe\n    orcid: 0000-0003-9876-5432\n    affiliation: State University\ndescription: |\n  Raw sequencing data supporting our manuscript \"Mechanisms of antibiotic\n  resistance evolution in E. coli\" submitted to Nature Microbiology.\n\n  Data includes whole genome sequencing of 50 clinical isolates...\nkeywords:\n  - GWAS\n  - E. coli\n  - antibiotic resistance\n  - whole genome sequencing\nrelated_identifiers:\n  - identifier: \"10.1101/2025.01.123456\"  # bioRxiv preprint\n    relation: IsSupplementTo\n    type: DOI\nEOF\n\n# Step 3: Prepare DOI\ncicada doi prepare data_*.fastq.gz \\\n  --enrich publication_metadata.yaml \\\n  --provider zenodo \\\n  --upload \\\n  --output dataset_doi.json\n\n# Step 4: Review metadata\ncat dataset_doi.json | jq '.'\n\n# Step 5: Publish\ndoi=$(jq -r '.doi' dataset_doi.json)\ncicada doi publish \"$doi\" --provider zenodo\n\n# Step 6: Include DOI in manuscript\necho \"Data Availability: Dataset available at https://zenodo.org/doi/$doi\"\n</code></pre>"},{"location":"PROVIDERS/#workflow-2-supplementary-data-for-published-paper","title":"Workflow 2: Supplementary Data for Published Paper","text":"<p>Add DOI after paper acceptance:</p> <pre><code># Prepare metadata with paper DOI\ncat &gt; supplementary_metadata.yaml &lt;&lt;EOF\ntitle: \"Supplementary Data: [Paper Title]\"\nauthors:\n  - name: Paper Author 1\n  - name: Paper Author 2\ndescription: \"Supplementary dataset for our paper published in [Journal]\"\nrelated_identifiers:\n  - identifier: \"10.1234/journal.2025.5678\"  # Paper DOI\n    relation: IsSupplementTo\n    type: DOI\npublication_year: 2025\nEOF\n\n# Create DOI\ncicada doi prepare supplementary_data.zip \\\n  --enrich supplementary_metadata.yaml \\\n  --provider zenodo \\\n  --upload\n\n# Publish immediately (paper already accepted)\ncicada doi publish [DOI] --provider zenodo\n</code></pre>"},{"location":"PROVIDERS/#workflow-3-dataset-series","title":"Workflow 3: Dataset Series","text":"<p>Create related datasets with version control:</p> <pre><code># Version 1: Raw data\ncicada doi prepare raw_data_v1.tar.gz \\\n  --enrich metadata_v1.yaml \\\n  --provider zenodo \\\n  --upload \\\n  --output v1_doi.json\n\nv1_doi=$(jq -r '.doi' v1_doi.json)\n\n# Version 2: Processed data (link to v1)\ncat &gt; metadata_v2.yaml &lt;&lt;EOF\ntitle: \"Processed Data - Version 2\"\ndescription: \"Processed version of raw data\"\nrelated_identifiers:\n  - identifier: \"$v1_doi\"\n    relation: IsNewVersionOf\n    type: DOI\nEOF\n\ncicada doi prepare processed_data_v2.tar.gz \\\n  --enrich metadata_v2.yaml \\\n  --provider zenodo \\\n  --upload\n</code></pre>"},{"location":"PROVIDERS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"PROVIDERS/#authentication-failures","title":"Authentication Failures","text":"<p>Error: <code>401 Unauthorized</code></p> <p>Zenodo Solutions: <pre><code># Check token is set\ncicada config get zenodo.token\n\n# Regenerate token at https://zenodo.org (Settings \u2192 Applications)\ncicada config set zenodo.token \"new_token_here\"\n\n# Test authentication\ncurl \"https://zenodo.org/api/deposit/depositions\" \\\n  -H \"Authorization: Bearer $(cicada config get zenodo.token)\"\n</code></pre></p> <p>DataCite Solutions: <pre><code># Verify credentials with institution\ncicada config get datacite.repository_id\ncicada config get datacite.username\n\n# Test API access\ncurl -u \"username:password\" https://api.datacite.org/dois\n</code></pre></p>"},{"location":"PROVIDERS/#upload-failures","title":"Upload Failures","text":"<p>Error: <code>File upload failed: Connection timeout</code></p> <p>Solutions:</p> <ol> <li> <p>Check file size (Zenodo limit: 50 GB)    <pre><code>ls -lh data.fastq.gz\n</code></pre></p> </li> <li> <p>Compress files <pre><code>gzip -9 data.fastq  # Maximum compression\n</code></pre></p> </li> <li> <p>Split large files <pre><code>split -b 10G data.tar.gz data_part_\n</code></pre></p> </li> <li> <p>Upload separately <pre><code># Create deposition first\ncicada doi prepare data.fastq --enrich metadata.yaml --no-upload\n\n# Upload files individually\ncicada doi upload [DOI] data_part_aa\ncicada doi upload [DOI] data_part_ab\n</code></pre></p> </li> </ol>"},{"location":"PROVIDERS/#validation-failures","title":"Validation Failures","text":"<p>Error: <code>Validation failed: Missing required field 'publisher'</code></p> <p>Solution:</p> <p>Add missing fields to enrichment file:</p> <pre><code># Required by DataCite\npublisher: \"Your Institution\"\npublication_year: 2025\nresource_type: Dataset\n\n# Recommended\ndescription: \"Detailed description of dataset\"\nsubjects: [\"Biology\", \"Genomics\"]\n</code></pre>"},{"location":"PROVIDERS/#doi-already-exists","title":"DOI Already Exists","text":"<p>Error: <code>DOI 10.5281/zenodo.123456 already exists</code></p> <p>Solution:</p> <p>Either update existing DOI or create new version:</p> <pre><code># Option 1: Update existing\ncicada doi update 10.5281/zenodo.123456 \\\n  --metadata new_metadata.yaml \\\n  --provider zenodo\n\n# Option 2: Create new version\ncicada doi version 10.5281/zenodo.123456 \\\n  --provider zenodo\n</code></pre>"},{"location":"PROVIDERS/#best-practices","title":"Best Practices","text":""},{"location":"PROVIDERS/#1-always-test-in-sandbox-first","title":"1. Always Test in Sandbox First","text":"<pre><code># \u274c Don't do this first time\ncicada doi prepare data.fastq --provider zenodo --upload\n\n# \u2705 Do this instead\ncicada doi prepare data.fastq --provider zenodo-sandbox --upload\n</code></pre>"},{"location":"PROVIDERS/#2-validate-before-preparation","title":"2. Validate Before Preparation","text":"<pre><code># Check quality score first\ncicada doi validate data.fastq --enrich metadata.yaml\n\n# If score &lt; 75, add more metadata\n# If score &gt;= 80, proceed with preparation\n</code></pre>"},{"location":"PROVIDERS/#3-use-version-control-for-metadata","title":"3. Use Version Control for Metadata","text":"<pre><code># Store metadata in git\ngit add metadata/\ngit commit -m \"Add DOI metadata for Dataset 042\"\n</code></pre>"},{"location":"PROVIDERS/#4-document-dois","title":"4. Document DOIs","text":"<p>Keep a lab registry:</p> <p>dois.md: <pre><code># Lab DOI Registry\n\n| Dataset | DOI | Published | Paper |\n|---------|-----|-----------|-------|\n| WGS E. coli 042 | 10.5281/zenodo.123456 | 2025-01-15 | Nature Micro |\n| RNA-seq time series | 10.5281/zenodo.123457 | 2025-02-01 | Cell |\n</code></pre></p>"},{"location":"PROVIDERS/#5-include-data-availability-statements","title":"5. Include Data Availability Statements","text":"<p>Standard text for papers:</p> <pre><code>Data Availability: Raw sequencing data have been deposited in Zenodo\nunder accession code 10.5281/zenodo.123456.\n</code></pre>"},{"location":"PROVIDERS/#6-monitor-usage","title":"6. Monitor Usage","text":"<p>Check dataset views and downloads:</p> <pre><code># Zenodo: View statistics at record page\nopen \"https://zenodo.org/record/123456\"\n\n# DataCite: Check usage statistics\ncicada doi stats 10.12345/dataset-001 --provider datacite\n</code></pre>"},{"location":"PROVIDERS/#related-documentation","title":"Related Documentation","text":"<ul> <li>DOI Workflow Guide: Preparing metadata for DOI registration</li> <li>Metadata Extraction Guide: Extracting metadata from files</li> <li>User Scenarios: Real-world publishing workflows</li> </ul>"},{"location":"PROVIDERS/#support","title":"Support","text":"<p>For provider-specific issues:</p> <ul> <li>Zenodo: https://zenodo.org/support</li> <li>DataCite: https://support.datacite.org</li> <li>Cicada: https://github.com/scttfrdmn/cicada/issues</li> </ul>"},{"location":"PROVIDERS/#version-history","title":"Version History","text":"<ul> <li>v0.2.0 (Current): DataCite and Zenodo support</li> <li>v0.3.0 (Planned): Dryad, Figshare, Mendeley Data support</li> </ul>"},{"location":"ROADMAP_v0.2.0/","title":"Cicada v0.2.0 - Metadata &amp; Intelligence","text":"<p>Target: Q1 2026 Focus: Instrument-aware data management with metadata extraction and validation Project Management: GitHub Projects</p>"},{"location":"ROADMAP_v0.2.0/#vision","title":"Vision","text":"<p>Cicada is a dormant data commons platform for academic research labs. Version 0.1.0 established the foundational storage and sync layer. Version 0.2.0 builds on that foundation by adding instrument awareness and metadata intelligence - critical capabilities for a true data commons.</p> <p>This release transforms Cicada into an intelligent research data pipeline that: - Automatically extracts and preserves instrument metadata during data ingestion - Understands diverse scientific instruments through pluggable extractors - Validates file integrity and metadata completeness - Enables FAIR-compliant data management with rich, searchable metadata - Supports DOI minting for data publication (DataCite/Zenodo)</p> <p>These capabilities move Cicada closer to the full data commons vision: federated storage with metadata, access control, compute-to-data, collaboration primitives, and data publication.</p>"},{"location":"ROADMAP_v0.2.0/#core-features","title":"Core Features","text":""},{"location":"ROADMAP_v0.2.0/#1-metadata-extraction-framework","title":"1. Metadata Extraction Framework","text":"<p>Goal: Automatically extract and preserve instrument metadata during sync operations.</p>"},{"location":"ROADMAP_v0.2.0/#11-metadata-extractor-interface","title":"1.1 Metadata Extractor Interface","text":"<pre><code>type Extractor interface {\n    // CanHandle returns true if extractor supports this file\n    CanHandle(filename string) bool\n\n    // Extract extracts metadata from file path\n    Extract(filepath string) (*Metadata, error)\n\n    // ExtractFromReader extracts from stream (for S3\u2192Local)\n    ExtractFromReader(r io.Reader, filename string) (*Metadata, error)\n\n    // Validate checks file integrity\n    Validate(filepath string) error\n\n    // Name returns extractor name\n    Name() string\n\n    // SupportedFormats returns file extensions\n    SupportedFormats() []string\n}\n\ntype Metadata struct {\n    // Common fields (all instruments)\n    Format           string                 `json:\"format\"`\n    InstrumentType   string                 `json:\"instrument_type\"`\n    InstrumentModel  string                 `json:\"instrument_model\"`\n    Manufacturer     string                 `json:\"manufacturer\"`\n    AcquisitionDate  time.Time              `json:\"acquisition_date\"`\n    Operator         string                 `json:\"operator,omitempty\"`\n    FileSize         int64                  `json:\"file_size\"`\n    Checksum         string                 `json:\"checksum\"`\n\n    // Instrument-specific fields\n    Microscopy       *MicroscopyMetadata    `json:\"microscopy,omitempty\"`\n    Sequencing       *SequencingMetadata    `json:\"sequencing,omitempty\"`\n    MassSpec         *MassSpecMetadata      `json:\"mass_spec,omitempty\"`\n    FlowCytometry    *FlowCytometryMetadata `json:\"flow_cytometry,omitempty\"`\n\n    // Custom fields\n    Custom           map[string]interface{} `json:\"custom,omitempty\"`\n\n    // Extraction metadata\n    ExtractedAt      time.Time              `json:\"extracted_at\"`\n    ExtractorVersion string                 `json:\"extractor_version\"`\n}\n\ntype MicroscopyMetadata struct {\n    Objective        ObjectiveInfo          `json:\"objective\"`\n    Channels         []ChannelInfo          `json:\"channels\"`\n    Dimensions       ImageDimensions        `json:\"dimensions\"`\n    PixelSize        PixelSize              `json:\"pixel_size\"`\n    ExposureTimes    []float64              `json:\"exposure_times_ms\"`\n    IlluminationType string                 `json:\"illumination_type\"` // confocal, widefield, etc.\n}\n\ntype ObjectiveInfo struct {\n    Magnification float64 `json:\"magnification\"`\n    NA            float64 `json:\"numerical_aperture\"`\n    Immersion     string  `json:\"immersion\"` // oil, water, air\n    WorkingDist   float64 `json:\"working_distance_um,omitempty\"`\n}\n\ntype ChannelInfo struct {\n    Name             string  `json:\"name\"`\n    Wavelength       int     `json:\"wavelength_nm\"`\n    ExposureTime     float64 `json:\"exposure_time_ms\"`\n    EmissionFilter   string  `json:\"emission_filter,omitempty\"`\n    ExcitationFilter string  `json:\"excitation_filter,omitempty\"`\n}\n\ntype ImageDimensions struct {\n    Width  int `json:\"width\"`\n    Height int `json:\"height\"`\n    Depth  int `json:\"depth\"`  // Z-stack\n    Time   int `json:\"time\"`   // Time series\n    Channels int `json:\"channels\"`\n}\n\ntype PixelSize struct {\n    X float64 `json:\"x_um\"`\n    Y float64 `json:\"y_um\"`\n    Z float64 `json:\"z_um,omitempty\"`\n}\n\ntype SequencingMetadata struct {\n    Platform       string `json:\"platform\"`        // Illumina, Nanopore, PacBio\n    Instrument     string `json:\"instrument\"`      // NovaSeq 6000, MinION, etc.\n    RunID          string `json:\"run_id\"`\n    FlowcellID     string `json:\"flowcell_id\"`\n    ReadLength     int    `json:\"read_length\"`\n    IsPaired       bool   `json:\"is_paired\"`\n    ReadCount      int64  `json:\"read_count,omitempty\"`\n    QualityEncoding string `json:\"quality_encoding\"` // Phred+33, Phred+64\n}\n</code></pre>"},{"location":"ROADMAP_v0.2.0/#12-priority-extractors-v020","title":"1.2 Priority Extractors (v0.2.0)","text":"<p>Microscopy: 1. Zeiss CZI (Confocal microscopy)    - Library: github.com/ome/bioformats (via CGo) or custom parser    - Extracts: Instrument, objective, channels, dimensions, pixel size    - Validation: CZI file structure integrity</p> <ol> <li>OME-TIFF (Open Microscopy Environment)</li> <li>Library: encoding/xml for OME-XML parsing</li> <li>Extracts: Full OME metadata specification</li> <li>Validation: OME-XML schema compliance</li> </ol> <p>Sequencing: 3. FASTQ (Raw sequencing data)    - Library: Custom parser    - Extracts: Read count, quality encoding, sequence length distribution    - Validation: FASTQ format integrity, quality score range</p> <p>Future (v0.3.0+): - Nikon ND2 - Leica LIF - BAM/CRAM - mzML (mass spec) - FCS (flow cytometry)</p>"},{"location":"ROADMAP_v0.2.0/#13-cli-integration","title":"1.3 CLI Integration","text":"<pre><code># Enable metadata extraction during sync\ncicada sync --extract-metadata /data/microscope s3://lab-data/\n\n# Extract metadata from existing files\ncicada metadata extract /data/microscope/*.czi\n\n# Show metadata for a file\ncicada metadata show /data/microscope/sample_001.czi\n\n# Validate file integrity\ncicada metadata validate /data/microscope/*.czi\n\n# Export metadata catalog\ncicada metadata export s3://lab-data/ --format json &gt; catalog.json\n</code></pre>"},{"location":"ROADMAP_v0.2.0/#14-storage-options","title":"1.4 Storage Options","text":"<p>Option A: S3 Object Metadata (Default) - Store as S3 object tags (10 tags max) - Queryable via S3 API - Limitations: Limited to 10 key-value pairs</p> <p>Option B: Sidecar JSON Files - Store as <code>.metadata.json</code> alongside each file - Full metadata preservation - Easy to read/process</p> <p>Option C: Central Catalog - Store in S3 as <code>s3://bucket/.cicada/metadata-catalog.json</code> - Searchable index of all files - Enables cross-file queries</p> <p>Recommendation: Hybrid approach - Critical fields \u2192 S3 object tags (instrument, date, format) - Full metadata \u2192 Sidecar JSON - Searchable index \u2192 Central catalog (updated incrementally)</p>"},{"location":"ROADMAP_v0.2.0/#2-instrument-presets-system","title":"2. Instrument Presets System","text":"<p>Goal: Simplify configuration for common lab instruments with pre-configured settings.</p>"},{"location":"ROADMAP_v0.2.0/#21-preset-definition","title":"2.1 Preset Definition","text":"<pre><code># presets/zeiss-confocal.yaml\nname: \"Zeiss Confocal Microscope\"\ndescription: \"Zeiss LSM series confocal microscopes (LSM 880, 900, 980)\"\nversion: \"1.0\"\n\n# Instrument detection\ndetection:\n  file_extensions:\n    - .czi\n  file_patterns:\n    - \"*.czi\"\n  magic_bytes:\n    offset: 0\n    bytes: \"ZISRAWFILE\"  # CZI file signature\n\n# Sync configuration\nsync:\n  debounce_seconds: 30      # CZI files are large, need time to write\n  min_age_seconds: 60       # Wait for complete write\n  concurrency: 4            # Balance speed vs system load\n\n  # Suggested exclude patterns\n  exclude_patterns:\n    - \"*.tmp\"\n    - \"*.partial\"\n    - \"*_preview.jpg\"       # Zeiss preview images\n    - \"Experiment.czexp\"    # Experiment metadata file\n\n# Metadata extraction\nmetadata:\n  enabled: true\n  extractor: \"zeiss-czi\"\n  extract_on_sync: true\n\n  # Fields to extract\n  fields:\n    - instrument_model\n    - objective\n    - channels\n    - dimensions\n    - pixel_size\n    - acquisition_date\n    - operator\n\n  # Fields to store as S3 tags (max 10)\n  s3_tags:\n    - instrument_type: microscopy\n    - instrument_model: \"{metadata.instrument_model}\"\n    - format: czi\n    - acquisition_date: \"{metadata.acquisition_date}\"\n    - magnification: \"{metadata.microscopy.objective.magnification}x\"\n\n# Validation\nvalidation:\n  enabled: true\n  checks:\n    - file_integrity      # Verify CZI structure\n    - minimum_file_size: 1048576  # 1 MB minimum\n    - complete_metadata   # Ensure required metadata present\n\n# Notifications (optional)\nnotifications:\n  on_sync_complete:\n    message: \"Synced {file_count} CZI files ({total_size} GB)\"\n  on_validation_failure:\n    message: \"\u26a0\ufe0f  Validation failed: {file_path}\"\n    action: skip  # skip, retry, fail\n</code></pre>"},{"location":"ROADMAP_v0.2.0/#22-preset-library-structure","title":"2.2 Preset Library Structure","text":"<pre><code>presets/\n\u251c\u2500\u2500 microscopy/\n\u2502   \u251c\u2500\u2500 zeiss-confocal.yaml\n\u2502   \u251c\u2500\u2500 zeiss-lightsheet.yaml\n\u2502   \u251c\u2500\u2500 nikon-widefield.yaml\n\u2502   \u251c\u2500\u2500 leica-confocal.yaml\n\u2502   \u2514\u2500\u2500 olympus-spinning-disk.yaml\n\u251c\u2500\u2500 sequencing/\n\u2502   \u251c\u2500\u2500 illumina-novaseq.yaml\n\u2502   \u251c\u2500\u2500 illumina-miseq.yaml\n\u2502   \u251c\u2500\u2500 nanopore-minion.yaml\n\u2502   \u2514\u2500\u2500 pacbio-sequel.yaml\n\u251c\u2500\u2500 mass-spec/\n\u2502   \u251c\u2500\u2500 thermo-orbitrap.yaml\n\u2502   \u2514\u2500\u2500 bruker-maldi.yaml\n\u251c\u2500\u2500 flow-cytometry/\n\u2502   \u251c\u2500\u2500 bd-facs.yaml\n\u2502   \u2514\u2500\u2500 beckman-cytoflex.yaml\n\u2514\u2500\u2500 generic/\n    \u251c\u2500\u2500 large-files.yaml      # Generic preset for large files\n    \u251c\u2500\u2500 many-small-files.yaml # Generic preset for many small files\n    \u2514\u2500\u2500 custom-template.yaml  # Template for users to customize\n</code></pre>"},{"location":"ROADMAP_v0.2.0/#23-cli-commands","title":"2.3 CLI Commands","text":"<pre><code># List available presets\ncicada instrument list\n\n# Show preset details\ncicada instrument show zeiss-confocal\n\n# Interactive setup wizard\ncicada instrument setup\n\n# Apply specific preset\ncicada instrument setup zeiss-confocal \\\n  --path /mnt/zeiss/output \\\n  --destination s3://lab-data/microscopy\n\n# Auto-detect instrument type\ncicada instrument detect /mnt/zeiss/output\n\n# Create custom preset from existing watch\ncicada instrument export my-custom-preset \\\n  --from-watch /mnt/zeiss/output-123456\n</code></pre>"},{"location":"ROADMAP_v0.2.0/#24-auto-detection-logic","title":"2.4 Auto-Detection Logic","text":"<pre><code>// Auto-detect instrument type from files\nfunc DetectInstrument(path string) (*Preset, error) {\n    // 1. Check file extensions\n    files, _ := ioutil.ReadDir(path)\n    extensions := countExtensions(files)\n\n    // 2. Read magic bytes from sample files\n    magicBytes := readMagicBytes(files[0:5])\n\n    // 3. Match against preset detection rules\n    for _, preset := range presets {\n        if preset.Matches(extensions, magicBytes) {\n            return preset, nil\n        }\n    }\n\n    return nil, ErrNoPresetMatch\n}\n</code></pre>"},{"location":"ROADMAP_v0.2.0/#3-pluggable-doi-provider-system","title":"3. Pluggable DOI Provider System","text":"<p>Goal: Support multiple DOI providers (DataCite, Zenodo) or disable DOI minting entirely.</p>"},{"location":"ROADMAP_v0.2.0/#31-provider-interface","title":"3.1 Provider Interface","text":"<pre><code>// DOIProvider interface for pluggable DOI minting\ntype DOIProvider interface {\n    // Name returns provider name (datacite, zenodo, etc.)\n    Name() string\n\n    // Mint creates a new DOI for a dataset\n    Mint(ctx context.Context, dataset *Dataset) (*DOI, error)\n\n    // Update updates DOI metadata\n    Update(ctx context.Context, doi string, dataset *Dataset) error\n\n    // Get retrieves DOI information\n    Get(ctx context.Context, doi string) (*DOI, error)\n\n    // Validate checks if dataset metadata is valid for this provider\n    Validate(dataset *Dataset) error\n\n    // EstimateCost returns estimated cost for minting (some providers charge)\n    EstimateCost(dataset *Dataset) (float64, string, error)\n}\n\n// DOIConfig for provider selection\ntype DOIConfig struct {\n    Provider     string                 `yaml:\"provider\"` // datacite, zenodo, off\n    Enabled      bool                   `yaml:\"enabled\"`\n\n    // DataCite specific\n    DataCite     *DataCiteConfig        `yaml:\"datacite,omitempty\"`\n\n    // Zenodo specific\n    Zenodo       *ZenodoConfig          `yaml:\"zenodo,omitempty\"`\n\n    // Common settings\n    DefaultLicense   string             `yaml:\"default_license\"`\n    DefaultPublisher string             `yaml:\"default_publisher\"`\n    AutoPublish      bool               `yaml:\"auto_publish\"` // Auto-mint DOI on publish command\n}\n\ntype DataCiteConfig struct {\n    RepositoryID string  `yaml:\"repository_id\"`\n    Password     string  `yaml:\"password\"`\n    Prefix       string  `yaml:\"prefix\"`        // e.g., \"10.12345\"\n    TestMode     bool    `yaml:\"test_mode\"`     // Use test environment\n    BaseURL      string  `yaml:\"base_url\"`\n}\n\ntype ZenodoConfig struct {\n    AccessToken  string  `yaml:\"access_token\"`\n    Sandbox      bool    `yaml:\"sandbox\"`       // Use sandbox environment\n    Community    string  `yaml:\"community\"`     // Zenodo community ID\n}\n</code></pre>"},{"location":"ROADMAP_v0.2.0/#32-provider-implementations","title":"3.2 Provider Implementations","text":"<p>DataCite Provider (Priority): <pre><code>type DataCiteProvider struct {\n    client *DataCiteClient\n    config *DataCiteConfig\n}\n\nfunc (p *DataCiteProvider) Mint(ctx context.Context, dataset *Dataset) (*DOI, error) {\n    // 1. Validate dataset metadata\n    if err := p.Validate(dataset); err != nil {\n        return nil, fmt.Errorf(\"validation failed: %w\", err)\n    }\n\n    // 2. Generate DOI suffix\n    suffix := generateDOISuffix(dataset)\n    doiString := fmt.Sprintf(\"%s/%s\", p.config.Prefix, suffix)\n\n    // 3. Prepare DataCite metadata XML\n    metadata := buildDataCiteMetadata(dataset, doiString)\n\n    // 4. Register with DataCite\n    resp, err := p.client.CreateDOI(ctx, metadata)\n    if err != nil {\n        return nil, fmt.Errorf(\"datacite registration failed: %w\", err)\n    }\n\n    return &amp;DOI{\n        DOI:       doiString,\n        URL:       dataset.URL,\n        State:     \"draft\",\n        Provider:  \"datacite\",\n        CreatedAt: time.Now(),\n    }, nil\n}\n</code></pre></p> <p>Zenodo Provider (Future): <pre><code>type ZenodoProvider struct {\n    client *ZenodoClient\n    config *ZenodoConfig\n}\n\nfunc (p *ZenodoProvider) Mint(ctx context.Context, dataset *Dataset) (*DOI, error) {\n    // Zenodo auto-generates DOI\n    // 1. Create deposition\n    // 2. Upload files (optional)\n    // 3. Publish to get DOI\n\n    deposition, err := p.client.CreateDeposition(ctx, dataset)\n    if err != nil {\n        return nil, err\n    }\n\n    return &amp;DOI{\n        DOI:      deposition.DOI,\n        URL:      deposition.URL,\n        Provider: \"zenodo\",\n    }, nil\n}\n</code></pre></p> <p>Disabled Provider: <pre><code>type DisabledProvider struct{}\n\nfunc (p *DisabledProvider) Mint(ctx context.Context, dataset *Dataset) (*DOI, error) {\n    return nil, fmt.Errorf(\"DOI minting is disabled\")\n}\n\nfunc (p *DisabledProvider) Name() string {\n    return \"disabled\"\n}\n</code></pre></p>"},{"location":"ROADMAP_v0.2.0/#33-configuration","title":"3.3 Configuration","text":"<pre><code># ~/.cicada/config.yaml\nversion: \"1\"\n\naws:\n  profile: default\n  region: us-west-2\n\n# DOI configuration\ndoi:\n  provider: datacite  # datacite, zenodo, off\n  enabled: true\n\n  # Provider-specific configs\n  datacite:\n    repository_id: \"INST.LAB\"\n    password: \"${DATACITE_PASSWORD}\"  # From environment\n    prefix: \"10.12345\"\n    test_mode: false\n\n  zenodo:\n    access_token: \"${ZENODO_TOKEN}\"\n    sandbox: false\n    community: \"my-institution\"\n\n  # Common settings\n  default_license: \"CC-BY-4.0\"\n  default_publisher: \"Rodriguez Lab, University\"\n  auto_publish: false  # Require explicit publish command\n</code></pre>"},{"location":"ROADMAP_v0.2.0/#34-cli-commands","title":"3.4 CLI Commands","text":"<pre><code># Configure DOI provider\ncicada config set doi.provider datacite\ncicada config set doi.datacite.repository_id INST.LAB\ncicada config set doi.datacite.prefix 10.12345\n\n# Disable DOI minting\ncicada config set doi.provider off\n\n# Publish dataset with DOI\ncicada publish s3://lab-data/experiment-2025-11 \\\n  --title \"Neuronal differentiation RNA-seq\" \\\n  --authors \"Maria Rodriguez, Alex Thompson\" \\\n  --license CC-BY-4.0 \\\n  --description \"RNA-seq time series of neuronal differentiation\"\n\n# Dry run (preview DOI metadata without minting)\ncicada publish --dry-run s3://lab-data/experiment-2025-11 \\\n  --title \"...\"\n\n# Update existing DOI\ncicada publish --update 10.12345/exp.2025.789 \\\n  --add-author \"James Park\"\n\n# List published datasets\ncicada publish list\n\n# Show DOI details\ncicada publish show 10.12345/exp.2025.789\n</code></pre>"},{"location":"ROADMAP_v0.2.0/#implementation-plan","title":"Implementation Plan","text":""},{"location":"ROADMAP_v0.2.0/#phase-1-core-infrastructure-weeks-1-2","title":"Phase 1: Core Infrastructure (Weeks 1-2)","text":"<ol> <li>Metadata Package Refactor</li> <li>Define <code>Metadata</code> struct with common fields</li> <li>Implement <code>Extractor</code> interface</li> <li>Create registry system</li> <li> <p>Add basic validation framework</p> </li> <li> <p>Storage Layer</p> </li> <li>S3 object tagging integration</li> <li>Sidecar JSON writer</li> <li>Central catalog management</li> <li>Metadata query interface</li> </ol>"},{"location":"ROADMAP_v0.2.0/#phase-2-first-extractor-zeiss-czi-weeks-3-4","title":"Phase 2: First Extractor - Zeiss CZI (Weeks 3-4)","text":"<ol> <li>CZI Parser</li> <li>Research CZI file format</li> <li>Evaluate libraries (bioformats, pylibCZI bindings)</li> <li>Implement extractor</li> <li> <p>Add validation</p> </li> <li> <p>Testing</p> </li> <li>Unit tests with sample CZI files</li> <li>Integration tests with S3</li> <li>Performance benchmarks</li> </ol>"},{"location":"ROADMAP_v0.2.0/#phase-3-additional-extractors-weeks-5-6","title":"Phase 3: Additional Extractors (Weeks 5-6)","text":"<ol> <li>OME-TIFF Extractor</li> <li>XML parsing for OME metadata</li> <li> <p>Schema validation</p> </li> <li> <p>FASTQ Extractor</p> </li> <li>Format parsing</li> <li>Quality encoding detection</li> <li>Read count estimation</li> </ol>"},{"location":"ROADMAP_v0.2.0/#phase-4-instrument-presets-weeks-7-8","title":"Phase 4: Instrument Presets (Weeks 7-8)","text":"<ol> <li>Preset System</li> <li>YAML preset format</li> <li>Preset loader</li> <li>Auto-detection logic</li> <li> <p>CLI commands</p> </li> <li> <p>Initial Presets</p> </li> <li>Zeiss confocal</li> <li>Illumina sequencers</li> <li>Generic templates</li> </ol>"},{"location":"ROADMAP_v0.2.0/#phase-5-doi-provider-system-weeks-9-10","title":"Phase 5: DOI Provider System (Weeks 9-10)","text":"<ol> <li>Provider Interface</li> <li>Define interface</li> <li>Implement provider registry</li> <li> <p>Configuration system</p> </li> <li> <p>DataCite Provider</p> </li> <li>API client</li> <li>Metadata mapping</li> <li> <p>Error handling</p> </li> <li> <p>CLI Integration</p> </li> <li>Publish commands</li> <li>Interactive workflows</li> </ol>"},{"location":"ROADMAP_v0.2.0/#phase-6-documentation-testing-weeks-11-12","title":"Phase 6: Documentation &amp; Testing (Weeks 11-12)","text":"<ol> <li>Documentation</li> <li>Update USER_SCENARIOS with metadata examples</li> <li>API documentation</li> <li> <p>Provider setup guides</p> </li> <li> <p>Integration Testing</p> </li> <li>End-to-end workflows</li> <li>Multi-instrument tests</li> <li> <p>DOI minting tests (test mode)</p> </li> <li> <p>Performance Testing</p> </li> <li>Large file metadata extraction</li> <li>Concurrent extraction performance</li> <li>S3 tagging overhead</li> </ol>"},{"location":"ROADMAP_v0.2.0/#success-metrics","title":"Success Metrics","text":""},{"location":"ROADMAP_v0.2.0/#technical-metrics","title":"Technical Metrics","text":"<ul> <li>Metadata extraction: &lt; 5s for typical files (&lt; 1GB)</li> <li>Sync overhead: &lt; 10% performance impact with metadata enabled</li> <li>Validation accuracy: 99%+ detection of corrupt files</li> <li>Preset accuracy: 95%+ correct auto-detection</li> </ul>"},{"location":"ROADMAP_v0.2.0/#user-metrics","title":"User Metrics","text":"<ul> <li>Setup time reduction: 5 min \u2192 1 min with presets</li> <li>Metadata completeness: 80%+ of required fields extracted</li> <li>DOI minting success: 99%+ successful mints</li> <li>User satisfaction: Positive feedback from beta users</li> </ul>"},{"location":"ROADMAP_v0.2.0/#testing-strategy","title":"Testing Strategy","text":""},{"location":"ROADMAP_v0.2.0/#unit-tests","title":"Unit Tests","text":"<ul> <li>Each extractor independently tested</li> <li>Sample files for each format</li> <li>Edge cases (corrupt files, partial metadata)</li> </ul>"},{"location":"ROADMAP_v0.2.0/#integration-tests","title":"Integration Tests","text":"<ul> <li>Full sync with metadata extraction</li> <li>S3 storage of metadata</li> <li>DOI minting (test mode)</li> </ul>"},{"location":"ROADMAP_v0.2.0/#user-acceptance-testing","title":"User Acceptance Testing","text":"<ul> <li>Beta test with 3-5 labs</li> <li>Different instrument types</li> <li>Real-world workloads</li> </ul>"},{"location":"ROADMAP_v0.2.0/#documentation-deliverables","title":"Documentation Deliverables","text":"<ol> <li>User Guide: Metadata extraction and DOI minting</li> <li>Developer Guide: Creating custom extractors</li> <li>Administrator Guide: DOI provider setup</li> <li>Preset Library: Documentation for each preset</li> <li>Updated USER_SCENARIOS: Metadata-aware walkthroughs</li> </ol>"},{"location":"ROADMAP_v0.2.0/#risks-mitigations","title":"Risks &amp; Mitigations","text":""},{"location":"ROADMAP_v0.2.0/#risk-1-czi-parser-complexity","title":"Risk 1: CZI Parser Complexity","text":"<p>Mitigation: Use bioformats library (battle-tested) or partner with OME team</p>"},{"location":"ROADMAP_v0.2.0/#risk-2-s3-tagging-limitations","title":"Risk 2: S3 Tagging Limitations","text":"<p>Mitigation: Hybrid approach with sidecar files for full metadata</p>"},{"location":"ROADMAP_v0.2.0/#risk-3-doi-provider-changes","title":"Risk 3: DOI Provider Changes","text":"<p>Mitigation: Abstract interface, versioned API clients</p>"},{"location":"ROADMAP_v0.2.0/#risk-4-performance-impact","title":"Risk 4: Performance Impact","text":"<p>Mitigation: Optional metadata extraction, async processing, caching</p>"},{"location":"ROADMAP_v0.2.0/#risk-5-legalcompliance","title":"Risk 5: Legal/Compliance","text":"<p>Mitigation: Clear licensing terms, terms of service for DOI minting</p>"},{"location":"ROADMAP_v0.2.0/#future-considerations-v030","title":"Future Considerations (v0.3.0+)","text":"<ul> <li>More extractors: Nikon ND2, Leica LIF, BAM, mzML, FCS</li> <li>Metadata search: Query interface for finding files by metadata</li> <li>Batch operations: Extract metadata from entire S3 buckets</li> <li>Visualization: Web UI for browsing metadata</li> <li>AI/ML integration: Automatic quality assessment, anomaly detection</li> <li>Workflow integration: Integration with pipeline tools (Nextflow, Snakemake)</li> </ul>"},{"location":"ROADMAP_v0.2.0/#success-criteria-for-v020-release","title":"Success Criteria for v0.2.0 Release","text":"<ul> <li>\u2705 3+ extractors fully implemented (CZI, OME-TIFF, FASTQ)</li> <li>\u2705 Metadata extraction working in sync workflows</li> <li>\u2705 5+ instrument presets available</li> <li>\u2705 DataCite provider fully functional</li> <li>\u2705 Documentation complete</li> <li>\u2705 80%+ test coverage for new features</li> <li>\u2705 Beta tested by 3+ labs</li> <li>\u2705 No regressions in v0.1.0 functionality</li> </ul>"},{"location":"ROADMAP_v0.2.0_PLAN/","title":"Cicada v0.2.0 Implementation Plan","text":"<p>Target: Q1 2026 (14 weeks) Focus: Metadata extraction + Instrument awareness + DOI minting + Basic multi-user</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#milestones","title":"Milestones","text":""},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-1-metadata-foundation-weeks-1-2","title":"Milestone 1: Metadata Foundation (Weeks 1-2)","text":"<p>Due: Week 2 Goal: Core metadata infrastructure in place</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-2-first-extractor-working-weeks-3-4","title":"Milestone 2: First Extractor Working (Weeks 3-4)","text":"<p>Due: Week 4 Goal: Zeiss CZI extraction functional end-to-end</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-3-extraction-library-complete-weeks-5-6","title":"Milestone 3: Extraction Library Complete (Weeks 5-6)","text":"<p>Due: Week 6 Goal: OME-TIFF and FASTQ extractors working</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-4-instrument-presets-weeks-7-8","title":"Milestone 4: Instrument Presets (Weeks 7-8)","text":"<p>Due: Week 8 Goal: Preset system with 5+ instruments</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-5-doi-provider-system-weeks-9-10","title":"Milestone 5: DOI Provider System (Weeks 9-10)","text":"<p>Due: Week 10 Goal: DataCite provider functional</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-6-multi-user-foundation-weeks-11-12","title":"Milestone 6: Multi-User Foundation (Weeks 11-12)","text":"<p>Due: Week 12 Goal: Basic IAM automation for 2-3 users</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-7-documentation-release-weeks-13-14","title":"Milestone 7: Documentation &amp; Release (Weeks 13-14)","text":"<p>Due: Week 14 Goal: v0.2.0 released with complete docs</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-breakdown-by-milestone","title":"Issue Breakdown by Milestone","text":""},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-1-metadata-foundation-weeks-1-2_1","title":"Milestone 1: Metadata Foundation (Weeks 1-2)","text":""},{"location":"ROADMAP_v0.2.0_PLAN/#issue-1-define-metadata-core-types","title":"Issue #1: Define Metadata Core Types","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>metadata</code> Assignee: TBD Estimate: 2 days</p> <p>Description: Define core metadata data structures in Go.</p> <p>Tasks: - [ ] Create <code>internal/metadata/types.go</code> - [ ] Define <code>Metadata</code> struct with common fields - [ ] Define instrument-specific structs (<code>MicroscopyMetadata</code>, <code>SequencingMetadata</code>, etc.) - [ ] Add JSON/YAML tags - [ ] Write unit tests for serialization - [ ] Document all fields with comments</p> <p>Acceptance Criteria: - All types compile and pass tests - 100% test coverage for type definitions - Documentation generated with godoc</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-2-implement-extractor-interface","title":"Issue #2: Implement Extractor Interface","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>metadata</code> Assignee: TBD Estimate: 2 days</p> <p>Description: Define and implement the pluggable extractor interface.</p> <p>Tasks: - [ ] Create <code>internal/metadata/extractor.go</code> - [ ] Define <code>Extractor</code> interface - [ ] Implement <code>ExtractorRegistry</code> with registration system - [ ] Add <code>GenericExtractor</code> as fallback - [ ] Write unit tests for registry - [ ] Add example extractor stub</p> <p>Acceptance Criteria: - Interface is well-documented - Registry can register/lookup extractors - GenericExtractor works for any file - Tests cover all registry operations</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-3-s3-object-tagging-integration","title":"Issue #3: S3 Object Tagging Integration","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>metadata</code>, <code>s3</code> Assignee: TBD Estimate: 3 days</p> <p>Description: Add support for storing metadata as S3 object tags.</p> <p>Tasks: - [ ] Update <code>internal/sync/s3.go</code> with tagging support - [ ] Implement <code>PutObjectTagging()</code> wrapper - [ ] Implement <code>GetObjectTagging()</code> wrapper - [ ] Add tag-to-metadata conversion functions - [ ] Handle 10-tag limit (prioritize critical fields) - [ ] Write integration tests with LocalStack - [ ] Document S3 permissions needed</p> <p>Acceptance Criteria: - Tags written during upload - Tags readable during list operations - Tests verify tag persistence - Documentation includes IAM policy examples</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-4-sidecar-json-file-writer","title":"Issue #4: Sidecar JSON File Writer","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>metadata</code> Assignee: TBD Estimate: 2 days</p> <p>Description: Implement sidecar JSON file creation for full metadata storage.</p> <p>Tasks: - [ ] Create <code>internal/metadata/storage.go</code> - [ ] Implement <code>WriteSidecarJSON(metadata, filepath)</code> - [ ] Support both local and S3 destinations - [ ] Use <code>.metadata.json</code> suffix convention - [ ] Pretty-print JSON for readability - [ ] Write unit tests - [ ] Document sidecar file format</p> <p>Acceptance Criteria: - Sidecar files created alongside data files - JSON is valid and readable - Works for both local and S3 paths - Tests verify file contents</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-5-central-catalog-management","title":"Issue #5: Central Catalog Management","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>metadata</code> Assignee: TBD Estimate: 3 days</p> <p>Description: Implement central metadata catalog for searchable index.</p> <p>Tasks: - [ ] Design catalog JSON schema - [ ] Implement incremental catalog updates - [ ] Store catalog at <code>s3://bucket/.cicada/metadata-catalog.json</code> - [ ] Add <code>AppendToCatalog(metadata)</code> function - [ ] Add <code>ReadCatalog()</code> function - [ ] Handle concurrent writes (optimistic locking) - [ ] Write integration tests - [ ] Document catalog format</p> <p>Acceptance Criteria: - Catalog updates incrementally - Catalog is queryable JSON - Concurrent updates handled gracefully - Tests verify catalog integrity</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-2-first-extractor-working-weeks-3-4_1","title":"Milestone 2: First Extractor Working (Weeks 3-4)","text":""},{"location":"ROADMAP_v0.2.0_PLAN/#issue-6-research-czi-file-format","title":"Issue #6: Research CZI File Format","text":"<p>Labels: <code>research</code>, <code>v0.2.0</code>, <code>metadata</code>, <code>microscopy</code> Assignee: TBD Estimate: 1 day</p> <p>Description: Research Zeiss CZI file format and available parsing libraries.</p> <p>Tasks: - [ ] Review CZI file format specification - [ ] Evaluate <code>pylibCZIrw</code> (Python library) - [ ] Evaluate <code>bioformats</code> (Java library) - [ ] Evaluate Go CGo bindings possibility - [ ] Test sample CZI files - [ ] Document recommended approach - [ ] Create proof-of-concept</p> <p>Acceptance Criteria: - Decision documented on library choice - Sample CZI files obtained for testing - POC demonstrates basic metadata extraction</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-7-implement-zeiss-czi-extractor","title":"Issue #7: Implement Zeiss CZI Extractor","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>metadata</code>, <code>microscopy</code> Assignee: TBD Estimate: 5 days</p> <p>Description: Implement full Zeiss CZI metadata extractor.</p> <p>Tasks: - [ ] Create <code>internal/metadata/zeiss.go</code> - [ ] Implement <code>ZeissExtractor</code> struct - [ ] Parse CZI header and metadata sections - [ ] Extract instrument, objective, channels, dimensions - [ ] Extract pixel size and acquisition date - [ ] Handle errors gracefully - [ ] Write comprehensive unit tests - [ ] Add integration test with real CZI files - [ ] Document supported CZI versions</p> <p>Acceptance Criteria: - Extracts all required metadata fields - Handles corrupt/partial files gracefully - &lt;5s extraction time for typical files - 90%+ test coverage - Works with LSM 880, 900, 980 files</p> <p>Related: Depends on #6</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-8-czi-validation","title":"Issue #8: CZI Validation","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>metadata</code>, <code>microscopy</code> Assignee: TBD Estimate: 2 days</p> <p>Description: Implement CZI file validation logic.</p> <p>Tasks: - [ ] Verify CZI magic bytes (<code>ZISRAWFILE</code>) - [ ] Check file structure integrity - [ ] Validate metadata completeness - [ ] Add <code>Validate()</code> method to <code>ZeissExtractor</code> - [ ] Write unit tests with corrupt files - [ ] Document validation errors</p> <p>Acceptance Criteria: - Detects corrupt CZI files - Clear error messages - Tests cover common corruption cases - 99%+ accuracy on test corpus</p> <p>Related: Depends on #7</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-9-cli-integration-for-metadata-extraction","title":"Issue #9: CLI Integration for Metadata Extraction","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>cli</code>, <code>metadata</code> Assignee: TBD Estimate: 3 days</p> <p>Description: Add CLI commands for metadata extraction.</p> <p>Tasks: - [ ] Create <code>cmd/cicada/metadata.go</code> - [ ] Add <code>cicada metadata extract &lt;path&gt;</code> command - [ ] Add <code>cicada metadata show &lt;path&gt;</code> command - [ ] Add <code>cicada metadata validate &lt;path&gt;</code> command - [ ] Add <code>--format</code> flag (json, yaml, table) - [ ] Add <code>--extractor</code> flag to force specific extractor - [ ] Add colored output for validation results - [ ] Update <code>cicada sync</code> with <code>--extract-metadata</code> flag - [ ] Write CLI tests - [ ] Update documentation</p> <p>Acceptance Criteria: - All commands work as documented - Output is clear and actionable - Sync integration works seamlessly - Tests verify CLI behavior</p> <p>Related: Depends on #7</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-3-extraction-library-complete-weeks-5-6_1","title":"Milestone 3: Extraction Library Complete (Weeks 5-6)","text":""},{"location":"ROADMAP_v0.2.0_PLAN/#issue-10-implement-ome-tiff-extractor","title":"Issue #10: Implement OME-TIFF Extractor","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>metadata</code>, <code>microscopy</code> Assignee: TBD Estimate: 4 days</p> <p>Description: Implement OME-TIFF metadata extractor.</p> <p>Tasks: - [ ] Create <code>internal/metadata/ometiff.go</code> - [ ] Parse TIFF tags to find OME-XML - [ ] Parse OME-XML metadata - [ ] Map OME schema to Cicada metadata - [ ] Handle multiple images in one file - [ ] Write unit tests with sample files - [ ] Document OME-TIFF support</p> <p>Acceptance Criteria: - Extracts full OME metadata - Handles BigTIFF format - &lt;3s extraction for typical files - 90%+ test coverage</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-11-implement-fastq-extractor","title":"Issue #11: Implement FASTQ Extractor","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>metadata</code>, <code>sequencing</code> Assignee: TBD Estimate: 4 days</p> <p>Description: Implement FASTQ metadata extractor.</p> <p>Tasks: - [ ] Create <code>internal/metadata/fastq.go</code> - [ ] Parse FASTQ format (4-line records) - [ ] Detect quality encoding (Phred+33/Phred+64) - [ ] Sample reads to estimate total count - [ ] Calculate quality score statistics - [ ] Detect read length distribution - [ ] Handle gzipped FASTQ files - [ ] Write unit tests - [ ] Document FASTQ support</p> <p>Acceptance Criteria: - Correctly detects quality encoding - Accurate read count estimation - &lt;10s for 1GB file - 90%+ test coverage</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-12-performance-benchmarking","title":"Issue #12: Performance Benchmarking","text":"<p>Labels: <code>testing</code>, <code>v0.2.0</code>, <code>metadata</code>, <code>performance</code> Assignee: TBD Estimate: 2 days</p> <p>Description: Create performance benchmarks for all extractors.</p> <p>Tasks: - [ ] Create <code>internal/metadata/bench_test.go</code> - [ ] Benchmark CZI extraction (100MB, 1GB, 5GB files) - [ ] Benchmark OME-TIFF extraction - [ ] Benchmark FASTQ extraction - [ ] Document performance targets - [ ] Add CI benchmark job - [ ] Create performance regression tests</p> <p>Acceptance Criteria: - All benchmarks under target times - CI runs benchmarks on PRs - Performance regression caught automatically</p> <p>Related: Depends on #7, #10, #11</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-4-instrument-presets-weeks-7-8_1","title":"Milestone 4: Instrument Presets (Weeks 7-8)","text":""},{"location":"ROADMAP_v0.2.0_PLAN/#issue-13-design-preset-yaml-format","title":"Issue #13: Design Preset YAML Format","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>presets</code> Assignee: TBD Estimate: 1 day</p> <p>Description: Design and document preset YAML schema.</p> <p>Tasks: - [ ] Define preset YAML structure - [ ] Add detection rules (extensions, magic bytes) - [ ] Add sync configuration fields - [ ] Add metadata extraction settings - [ ] Add validation rules - [ ] Document schema with examples - [ ] Create JSON schema for validation</p> <p>Acceptance Criteria: - Schema is well-documented - Examples cover common use cases - JSON schema validates presets</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-14-implement-preset-loader","title":"Issue #14: Implement Preset Loader","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>presets</code> Assignee: TBD Estimate: 3 days</p> <p>Description: Implement preset loading and management system.</p> <p>Tasks: - [ ] Create <code>internal/preset/loader.go</code> - [ ] Implement <code>LoadPreset(path)</code> function - [ ] Implement preset validation - [ ] Add embedded preset support - [ ] Create preset registry - [ ] Write unit tests - [ ] Document preset API</p> <p>Acceptance Criteria: - Loads presets from YAML files - Validates preset structure - Registry manages all presets - Tests verify loading logic</p> <p>Related: Depends on #13</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-15-implement-auto-detection","title":"Issue #15: Implement Auto-Detection","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>presets</code> Assignee: TBD Estimate: 3 days</p> <p>Description: Implement instrument auto-detection logic.</p> <p>Tasks: - [ ] Create <code>internal/preset/detect.go</code> - [ ] Implement file extension matching - [ ] Implement magic byte detection - [ ] Score confidence of matches - [ ] Return best match with confidence level - [ ] Write unit tests with sample files - [ ] Document detection algorithm</p> <p>Acceptance Criteria: - 95%+ detection accuracy - Clear confidence scores - Handles ambiguous cases gracefully - Tests cover all presets</p> <p>Related: Depends on #14</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-16-create-initial-presets","title":"Issue #16: Create Initial Presets","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>presets</code>, <code>microscopy</code>, <code>sequencing</code> Assignee: TBD Estimate: 4 days</p> <p>Description: Create initial preset library for common instruments.</p> <p>Tasks: - [ ] Create <code>presets/microscopy/zeiss-confocal.yaml</code> - [ ] Create <code>presets/microscopy/zeiss-lightsheet.yaml</code> - [ ] Create <code>presets/sequencing/illumina-novaseq.yaml</code> - [ ] Create <code>presets/sequencing/illumina-miseq.yaml</code> - [ ] Create <code>presets/generic/large-files.yaml</code> - [ ] Create <code>presets/generic/many-small-files.yaml</code> - [ ] Create <code>presets/README.md</code> with documentation - [ ] Test each preset with real data - [ ] Document preset usage</p> <p>Acceptance Criteria: - 6+ presets created - Each preset tested with real instrument data - Documentation explains all fields - Presets follow consistent style</p> <p>Related: Depends on #13</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-17-preset-cli-commands","title":"Issue #17: Preset CLI Commands","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>cli</code>, <code>presets</code> Assignee: TBD Estimate: 3 days</p> <p>Description: Add CLI commands for preset management.</p> <p>Tasks: - [ ] Create <code>cmd/cicada/instrument.go</code> - [ ] Add <code>cicada instrument list</code> command - [ ] Add <code>cicada instrument show &lt;preset&gt;</code> command - [ ] Add <code>cicada instrument detect &lt;path&gt;</code> command - [ ] Add <code>cicada instrument setup &lt;preset&gt;</code> command - [ ] Add interactive setup wizard - [ ] Write CLI tests - [ ] Update documentation</p> <p>Acceptance Criteria: - All commands work as documented - Interactive wizard is user-friendly - Tests verify CLI behavior - Documentation includes examples</p> <p>Related: Depends on #14, #15, #16</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-5-doi-provider-system-weeks-9-10_1","title":"Milestone 5: DOI Provider System (Weeks 9-10)","text":""},{"location":"ROADMAP_v0.2.0_PLAN/#issue-18-define-doi-provider-interface","title":"Issue #18: Define DOI Provider Interface","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>doi</code> Assignee: TBD Estimate: 2 days</p> <p>Description: Define pluggable DOI provider interface.</p> <p>Tasks: - [ ] Create <code>internal/doi/provider.go</code> - [ ] Define <code>Provider</code> interface - [ ] Define <code>Dataset</code> struct with required fields - [ ] Define <code>DOI</code> struct - [ ] Create <code>ProviderRegistry</code> - [ ] Add configuration structs - [ ] Write unit tests - [ ] Document interface design</p> <p>Acceptance Criteria: - Interface supports multiple providers - Dataset struct covers DataCite/Zenodo needs - Registry manages providers - Tests verify interface contracts</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-19-implement-disabled-provider","title":"Issue #19: Implement Disabled Provider","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>doi</code> Assignee: TBD Estimate: 1 day</p> <p>Description: Implement \"disabled\" DOI provider for when minting is off.</p> <p>Tasks: - [ ] Create <code>internal/doi/disabled.go</code> - [ ] Implement <code>DisabledProvider</code> struct - [ ] Return clear error messages - [ ] Write unit tests - [ ] Document usage</p> <p>Acceptance Criteria: - Returns helpful error when disabled - Tests verify error messages - Documentation explains configuration</p> <p>Related: Depends on #18</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-20-implement-datacite-provider-stub","title":"Issue #20: Implement DataCite Provider (Stub)","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>doi</code>, <code>datacite</code> Assignee: TBD Estimate: 5 days</p> <p>Description: Implement DataCite provider with basic functionality.</p> <p>Tasks: - [ ] Create <code>internal/doi/datacite.go</code> - [ ] Implement <code>DataCiteProvider</code> struct - [ ] Add DataCite API client - [ ] Implement <code>Mint()</code> method - [ ] Generate DataCite metadata XML - [ ] Add test mode support - [ ] Write unit tests with mock API - [ ] Document DataCite setup</p> <p>Acceptance Criteria: - Can mint DOI in test mode - Metadata XML validates against schema - Error handling is robust - Tests verify API interactions - Documentation includes credential setup</p> <p>Related: Depends on #18</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-21-doi-configuration-system","title":"Issue #21: DOI Configuration System","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>doi</code>, <code>config</code> Assignee: TBD Estimate: 2 days</p> <p>Description: Add DOI configuration to Cicada config system.</p> <p>Tasks: - [ ] Update <code>internal/config/config.go</code> - [ ] Add <code>DOI</code> configuration section - [ ] Support provider selection (datacite, zenodo, off) - [ ] Add provider-specific config sections - [ ] Support environment variable substitution - [ ] Write unit tests - [ ] Document configuration</p> <p>Acceptance Criteria: - Config supports all providers - Environment variables work - Validation catches misconfigurations - Tests verify all config paths</p> <p>Related: Depends on #18</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-22-doi-cli-commands","title":"Issue #22: DOI CLI Commands","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>cli</code>, <code>doi</code> Assignee: TBD Estimate: 3 days</p> <p>Description: Add CLI commands for DOI management.</p> <p>Tasks: - [ ] Create <code>cmd/cicada/publish.go</code> - [ ] Add <code>cicada publish &lt;path&gt;</code> command - [ ] Add interactive metadata prompts - [ ] Add <code>--dry-run</code> flag - [ ] Add <code>cicada publish list</code> command - [ ] Add <code>cicada publish show &lt;doi&gt;</code> command - [ ] Write CLI tests - [ ] Update documentation</p> <p>Acceptance Criteria: - Can mint DOI via CLI - Interactive prompts are clear - Dry-run shows preview - Tests verify CLI behavior</p> <p>Related: Depends on #20, #21</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-6-multi-user-foundation-weeks-11-12_1","title":"Milestone 6: Multi-User Foundation (Weeks 11-12)","text":""},{"location":"ROADMAP_v0.2.0_PLAN/#issue-23-iam-user-creation-commands","title":"Issue #23: IAM User Creation Commands","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>auth</code>, <code>iam</code> Assignee: TBD Estimate: 3 days</p> <p>Description: Add CLI commands for creating IAM users.</p> <p>Tasks: - [ ] Create <code>cmd/cicada/user.go</code> - [ ] Add <code>cicada user add &lt;name&gt;</code> command - [ ] Create IAM user via AWS SDK - [ ] Generate access keys - [ ] Store credentials securely - [ ] Add <code>cicada user list</code> command - [ ] Add <code>cicada user remove &lt;name&gt;</code> command - [ ] Write CLI tests - [ ] Document user management</p> <p>Acceptance Criteria: - Creates IAM users successfully - Credentials stored securely - Tests verify user creation - Documentation includes IAM prerequisites</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-24-path-based-iam-policy-generation","title":"Issue #24: Path-Based IAM Policy Generation","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>auth</code>, <code>iam</code> Assignee: TBD Estimate: 4 days</p> <p>Description: Implement automatic IAM policy generation for path-based access.</p> <p>Tasks: - [ ] Create <code>internal/auth/policy.go</code> - [ ] Implement policy template system - [ ] Generate policies for S3 path prefixes - [ ] Support read-only vs read-write access - [ ] Attach policies to users - [ ] Add <code>cicada user grant &lt;user&gt; &lt;path&gt;</code> command - [ ] Add <code>cicada user revoke &lt;user&gt; &lt;path&gt;</code> command - [ ] Write unit tests - [ ] Document policy generation</p> <p>Acceptance Criteria: - Policies follow least-privilege principle - Path-based access works correctly - Tests verify policy generation - Documentation explains access patterns</p> <p>Related: Depends on #23</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-25-project-management-system","title":"Issue #25: Project Management System","text":"<p>Labels: <code>enhancement</code>, <code>v0.2.0</code>, <code>projects</code> Assignee: TBD Estimate: 3 days</p> <p>Description: Add basic project management for organizing data.</p> <p>Tasks: - [ ] Create <code>internal/config/project.go</code> - [ ] Define <code>Project</code> struct - [ ] Add <code>cicada project create &lt;name&gt;</code> command - [ ] Add <code>cicada project list</code> command - [ ] Associate S3 paths with projects - [ ] Store projects in config - [ ] Write unit tests - [ ] Document project usage</p> <p>Acceptance Criteria: - Projects can be created and listed - Projects map to S3 paths - Config persists projects - Tests verify project management - Documentation includes examples</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-26-multi-user-documentation","title":"Issue #26: Multi-User Documentation","text":"<p>Labels: <code>documentation</code>, <code>v0.2.0</code>, <code>auth</code> Assignee: TBD Estimate: 2 days</p> <p>Description: Create comprehensive multi-user setup guide.</p> <p>Tasks: - [ ] Create <code>docs/MULTI_USER_SETUP.md</code> - [ ] Document IAM prerequisites - [ ] Provide step-by-step setup guide - [ ] Include example scenarios (2-3 users) - [ ] Document common pitfalls - [ ] Add troubleshooting section - [ ] Include security best practices</p> <p>Acceptance Criteria: - Guide enables self-service setup - All commands documented - Examples are realistic - Security considerations covered</p> <p>Related: Depends on #23, #24, #25</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#milestone-7-documentation-release-weeks-13-14_1","title":"Milestone 7: Documentation &amp; Release (Weeks 13-14)","text":""},{"location":"ROADMAP_v0.2.0_PLAN/#issue-27-update-user_scenarios-for-v020","title":"Issue #27: Update USER_SCENARIOS for v0.2.0","text":"<p>Labels: <code>documentation</code>, <code>v0.2.0</code> Assignee: TBD Estimate: 2 days</p> <p>Description: Update USER_SCENARIOS.md with v0.2.0 features.</p> <p>Tasks: - [ ] Add metadata extraction to scenarios - [ ] Add instrument preset usage - [ ] Add DOI minting example - [ ] Add multi-user collaboration example - [ ] Update all command examples - [ ] Review for accuracy</p> <p>Acceptance Criteria: - All scenarios updated - New features demonstrated - Commands are correct - Examples are realistic</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-28-create-v020-migration-guide","title":"Issue #28: Create v0.2.0 Migration Guide","text":"<p>Labels: <code>documentation</code>, <code>v0.2.0</code> Assignee: TBD Estimate: 1 day</p> <p>Description: Create migration guide for v0.1.0 \u2192 v0.2.0.</p> <p>Tasks: - [ ] Create <code>docs/MIGRATION_v0.2.0.md</code> - [ ] Document breaking changes (if any) - [ ] Document new configuration options - [ ] Provide upgrade steps - [ ] Include rollback instructions</p> <p>Acceptance Criteria: - Guide is clear and complete - All breaking changes documented - Upgrade path is straightforward</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-29-integration-testing-suite","title":"Issue #29: Integration Testing Suite","text":"<p>Labels: <code>testing</code>, <code>v0.2.0</code> Assignee: TBD Estimate: 4 days</p> <p>Description: Create comprehensive integration test suite.</p> <p>Tasks: - [ ] Create <code>internal/integration/metadata_test.go</code> - [ ] Test end-to-end metadata extraction during sync - [ ] Test preset auto-detection - [ ] Test DOI minting (test mode) - [ ] Test multi-user scenarios - [ ] Add CI integration test job - [ ] Document test setup</p> <p>Acceptance Criteria: - All major workflows tested - Tests run in CI - Tests catch regressions - Documentation explains test setup</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-30-performance-load-testing","title":"Issue #30: Performance &amp; Load Testing","text":"<p>Labels: <code>testing</code>, <code>v0.2.0</code>, <code>performance</code> Assignee: TBD Estimate: 2 days</p> <p>Description: Validate performance targets for v0.2.0.</p> <p>Tasks: - [ ] Test metadata extraction with 1000 files - [ ] Test sync overhead with metadata enabled - [ ] Test catalog update performance - [ ] Test concurrent extraction - [ ] Document performance results - [ ] Identify bottlenecks</p> <p>Acceptance Criteria: - All performance targets met - No regressions from v0.1.0 - Bottlenecks documented - Results inform v0.3.0 optimization</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-31-release-preparation","title":"Issue #31: Release Preparation","text":"<p>Labels: <code>release</code>, <code>v0.2.0</code> Assignee: TBD Estimate: 2 days</p> <p>Description: Prepare v0.2.0 for release.</p> <p>Tasks: - [ ] Update CHANGELOG.md - [ ] Update version in all files - [ ] Create release notes - [ ] Build binaries for all platforms - [ ] Test release binaries - [ ] Create GitHub release - [ ] Update README.md badges - [ ] Announce release</p> <p>Acceptance Criteria: - CHANGELOG is complete - All binaries built and tested - GitHub release created - Documentation updated</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#issue-32-v020-announcement-outreach","title":"Issue #32: v0.2.0 Announcement &amp; Outreach","text":"<p>Labels: <code>release</code>, <code>v0.2.0</code>, <code>community</code> Assignee: TBD Estimate: 1 day</p> <p>Description: Announce v0.2.0 release to community.</p> <p>Tasks: - [ ] Write release blog post - [ ] Create demo video - [ ] Post to relevant communities - [ ] Update project website - [ ] Notify early adopters</p> <p>Acceptance Criteria: - Blog post published - Demo video available - Community notified - Website updated</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#summary","title":"Summary","text":"<p>Total Issues: 32 Total Duration: 14 weeks Dependencies: Sequential with some parallel work possible</p>"},{"location":"ROADMAP_v0.2.0_PLAN/#critical-path","title":"Critical Path:","text":"<ol> <li>Metadata Foundation (Issues #1-5) \u2192 2 weeks</li> <li>First Extractor (Issues #6-9) \u2192 2 weeks</li> <li>Extraction Library (Issues #10-12) \u2192 2 weeks</li> <li>Instrument Presets (Issues #13-17) \u2192 2 weeks</li> <li>DOI Providers (Issues #18-22) \u2192 2 weeks</li> <li>Multi-User (Issues #23-26) \u2192 2 weeks</li> <li>Documentation &amp; Release (Issues #27-32) \u2192 2 weeks</li> </ol>"},{"location":"ROADMAP_v0.2.0_PLAN/#parallelization-opportunities","title":"Parallelization Opportunities:","text":"<ul> <li>Issues #10 and #11 can be done in parallel (Week 5-6)</li> <li>Issues #18-20 can be split among multiple developers (Week 9-10)</li> <li>Documentation issues #27-28 can be done while testing (Week 13-14)</li> </ul>"},{"location":"ROADMAP_v0.2.0_SUMMARY/","title":"Cicada v0.2.0 - Implementation Plan Summary","text":"<p>Status: Planning Complete \u2705 Target Release: Q1 2026 (14 weeks) GitHub Milestone: Phase 2: Metadata &amp; FAIR</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#overview","title":"Overview","text":"<p>Version 0.2.0 builds on v0.1.0's foundational storage and sync layer by adding metadata intelligence and instrument awareness - critical capabilities for Cicada's vision as a dormant data commons platform.</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#key-additions","title":"Key Additions","text":"<ol> <li>Metadata Extraction Framework: Automatically extract and preserve instrument metadata</li> <li>Instrument Presets: Simplified configuration for common lab instruments</li> <li>DOI Provider System: Pluggable DOI minting (DataCite/Zenodo)</li> <li>Basic Multi-User Support: IAM automation for 2-3 users</li> </ol>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#milestones-timeline","title":"Milestones &amp; Timeline","text":""},{"location":"ROADMAP_v0.2.0_SUMMARY/#milestone-1-metadata-foundation-weeks-1-2","title":"Milestone 1: Metadata Foundation (Weeks 1-2)","text":"<p>Goal: Core metadata infrastructure in place</p> <p>Issues: - #17 Define Metadata Core Types - #18 Implement Extractor Interface - #19 S3 Object Tagging Integration</p> <p>Deliverable: Metadata types, extractor interface, S3 tag storage</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#milestone-2-first-extractor-working-weeks-3-4","title":"Milestone 2: First Extractor Working (Weeks 3-4)","text":"<p>Goal: Zeiss CZI extraction functional end-to-end</p> <p>Issues: - #20 Implement Zeiss CZI Extractor - #21 CLI Integration for Metadata Extraction</p> <p>Deliverable: Working CZI extractor with CLI commands</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#milestone-3-extraction-library-complete-weeks-5-6","title":"Milestone 3: Extraction Library Complete (Weeks 5-6)","text":"<p>Goal: OME-TIFF and FASTQ extractors working</p> <p>Tasks (to be broken into issues): - Implement OME-TIFF extractor - Implement FASTQ extractor - Performance benchmarking</p> <p>Deliverable: 3 production extractors (CZI, OME-TIFF, FASTQ)</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#milestone-4-instrument-presets-weeks-7-8","title":"Milestone 4: Instrument Presets (Weeks 7-8)","text":"<p>Goal: Preset system with 5+ instruments</p> <p>Issues: - #22 Instrument Preset System</p> <p>Deliverable: Auto-detection, preset library, CLI commands</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#milestone-5-doi-provider-system-weeks-9-10","title":"Milestone 5: DOI Provider System (Weeks 9-10)","text":"<p>Goal: DataCite provider functional</p> <p>Issues: - #23 Pluggable DOI Provider System</p> <p>Deliverable: DOI minting with DataCite test mode</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#milestone-6-multi-user-foundation-weeks-11-12","title":"Milestone 6: Multi-User Foundation (Weeks 11-12)","text":"<p>Goal: Basic IAM automation for 2-3 users</p> <p>Issues: - #24 Basic Multi-User Support</p> <p>Deliverable: IAM user creation, path-based access control</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#milestone-7-documentation-release-weeks-13-14","title":"Milestone 7: Documentation &amp; Release (Weeks 13-14)","text":"<p>Goal: v0.2.0 released with complete docs</p> <p>Issues: - #25 Documentation &amp; Testing</p> <p>Deliverable: Updated docs, migration guide, v0.2.0 release</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#success-metrics","title":"Success Metrics","text":""},{"location":"ROADMAP_v0.2.0_SUMMARY/#technical-metrics","title":"Technical Metrics","text":"<ul> <li>Metadata extraction: &lt; 5s for typical files (&lt; 1GB)</li> <li>Sync overhead: &lt; 10% performance impact with metadata enabled</li> <li>Validation accuracy: 99%+ detection of corrupt files</li> <li>Preset accuracy: 95%+ correct auto-detection</li> </ul>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#user-metrics","title":"User Metrics","text":"<ul> <li>Setup time reduction: 5 min \u2192 1 min with presets</li> <li>Metadata completeness: 80%+ of required fields extracted</li> <li>DOI minting success: 99%+ successful mints</li> <li>User satisfaction: Positive feedback from beta users</li> </ul>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#documentation","title":"Documentation","text":""},{"location":"ROADMAP_v0.2.0_SUMMARY/#planning-documents","title":"Planning Documents","text":"<ul> <li>ROADMAP_v0.2.0.md: Feature specifications</li> <li>ROADMAP_v0.2.0_PLAN.md: Detailed 32-issue breakdown</li> <li>This Document: Executive summary</li> </ul>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#reference-code","title":"Reference Code","text":"<p>Already implemented (POC/framework): - <code>internal/metadata/extractor.go</code> - Extractor POC with Zeiss CZI basic implementation - <code>internal/doi/provider.go</code> - Provider interface - <code>internal/doi/providers_example.go</code> - DataCite/Zenodo stubs - <code>presets/</code> - Example preset YAML files</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#project-management","title":"Project Management","text":""},{"location":"ROADMAP_v0.2.0_SUMMARY/#github-organization","title":"GitHub Organization","text":"<p>Milestones: - Primary: Phase 2: Metadata &amp; FAIR - Multi-user: Phase 3: Web UI &amp; User Management</p> <p>Labels: - <code>v0.2.0</code> - Version 0.2.0 features - <code>type: feature</code> - New features - <code>area: metadata</code> - Metadata system - <code>area: cli</code> - CLI commands - <code>priority: critical/high/medium/low</code> - Priority levels</p> <p>Issues: 9 consolidated issues (see milestone)</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#github-projects","title":"GitHub Projects","text":"<p>Track v0.2.0 development in GitHub Projects.</p> <p>Suggested Views: 1. By Milestone: Group issues by milestone 1-7 2. By Priority: Critical \u2192 High \u2192 Medium \u2192 Low 3. Kanban: Backlog \u2192 In Progress \u2192 Review \u2192 Done</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#dependencies-prerequisites","title":"Dependencies &amp; Prerequisites","text":""},{"location":"ROADMAP_v0.2.0_SUMMARY/#technical-dependencies","title":"Technical Dependencies","text":"<ul> <li>Go 1.23+</li> <li>AWS SDK for Go v2</li> <li>CZI parsing library (to be evaluated in #20)</li> <li>XML parsing for OME-TIFF</li> </ul>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#aws-permissions","title":"AWS Permissions","text":"<p>Existing v0.1.0 permissions plus: - <code>s3:PutObjectTagging</code> - For metadata tags - <code>s3:GetObjectTagging</code> - For reading metadata - <code>iam:CreateUser</code> - For multi-user (optional) - <code>iam:AttachUserPolicy</code> - For multi-user (optional)</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#risk-mitigation","title":"Risk Mitigation","text":""},{"location":"ROADMAP_v0.2.0_SUMMARY/#risk-1-czi-parser-complexity","title":"Risk 1: CZI Parser Complexity","text":"<p>Mitigation: Use battle-tested bioformats library or collaborate with OME team</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#risk-2-s3-tagging-limitations-10-tags-max","title":"Risk 2: S3 Tagging Limitations (10 tags max)","text":"<p>Mitigation: Hybrid approach - critical fields in tags, full metadata in sidecar JSON</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#risk-3-doi-provider-api-changes","title":"Risk 3: DOI Provider API Changes","text":"<p>Mitigation: Abstracted provider interface, versioned API clients</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#risk-4-performance-impact","title":"Risk 4: Performance Impact","text":"<p>Mitigation: Optional metadata extraction, async processing, caching</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#risk-5-scope-creep","title":"Risk 5: Scope Creep","text":"<p>Mitigation: Strict milestone-based development, defer features to v0.3.0</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#communication-updates","title":"Communication &amp; Updates","text":""},{"location":"ROADMAP_v0.2.0_SUMMARY/#weekly-updates","title":"Weekly Updates","text":"<p>Post progress updates to: - GitHub Discussions - Project README - Community channels</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#blockers","title":"Blockers","text":"<p>Report blockers immediately in: - GitHub issue comments - Project standup - Slack/Discord (if applicable)</p>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#next-steps","title":"Next Steps","text":"<ol> <li>\u2705 Planning Complete - Documentation and issues created</li> <li>\ud83d\udd1c Milestone 1 - Start with issue #17 (Define Metadata Core Types)</li> <li>\ud83d\udd1c Team Assignment - Assign issues to developers</li> <li>\ud83d\udd1c Sprint Planning - Set up 2-week sprints aligned with milestones</li> </ol>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#resources","title":"Resources","text":""},{"location":"ROADMAP_v0.2.0_SUMMARY/#planning-documents_1","title":"Planning Documents","text":"<ul> <li>Full specification: planning/PROJECT-SUMMARY.md</li> <li>Comprehensive roadmap: planning/ROADMAP.md</li> <li>User scenarios: USER_SCENARIOS.md</li> </ul>"},{"location":"ROADMAP_v0.2.0_SUMMARY/#community","title":"Community","text":"<ul> <li>GitHub: https://github.com/scttfrdmn/cicada</li> <li>Issues: https://github.com/scttfrdmn/cicada/issues</li> <li>Milestones: https://github.com/scttfrdmn/cicada/milestones</li> </ul> <p>Last Updated: 2025-11-23 Status: Ready to begin development \u2705</p>"},{"location":"ROADMAP_v0.3.0/","title":"Cicada v0.3.0 Roadmap","text":"<p>Target Release: Q2 2025 (April-June 2025) Status: Planning Phase Previous Release: v0.2.0 (January 2025)</p>"},{"location":"ROADMAP_v0.3.0/#overview","title":"Overview","text":"<p>Cicada v0.3.0 focuses on production-ready DOI registration and expanded file format support. This release transforms v0.2.0's metadata extraction and DOI preparation capabilities from preparation tools into complete end-to-end workflows that integrate with live repository services.</p>"},{"location":"ROADMAP_v0.3.0/#key-themes","title":"Key Themes","text":"<ol> <li>Live Provider Integration - Connect to real DataCite/Zenodo/Dryad APIs</li> <li>Format Expansion - Support microscopy and bioinformatics file formats</li> <li>User Customization - Custom presets and interactive workflows</li> <li>Production Readiness - Caching, error recovery, monitoring</li> </ol>"},{"location":"ROADMAP_v0.3.0/#goals","title":"Goals","text":""},{"location":"ROADMAP_v0.3.0/#primary-goals-must-have","title":"Primary Goals (Must Have)","text":"<ol> <li>Complete DOI Workflow - Users can mint real DOIs from command line</li> <li>Microscopy Support - CZI and OME-TIFF metadata extraction</li> <li>Provider Integration - At least DataCite and Zenodo working in sandbox and production</li> </ol>"},{"location":"ROADMAP_v0.3.0/#secondary-goals-should-have","title":"Secondary Goals (Should Have)","text":"<ol> <li>Custom Presets - Users can define lab-specific validation rules</li> <li>Interactive Editing - TUI for metadata review and enrichment</li> <li>Bioinformatics Formats - BAM/SAM and VCF support</li> </ol>"},{"location":"ROADMAP_v0.3.0/#stretch-goals-nice-to-have","title":"Stretch Goals (Nice to Have)","text":"<ol> <li>Additional Providers - Dryad and Figshare integration</li> <li>API Server Mode - REST API for programmatic access</li> <li>Metadata Caching - Performance optimization for large archives</li> </ol>"},{"location":"ROADMAP_v0.3.0/#milestones","title":"Milestones","text":""},{"location":"ROADMAP_v0.3.0/#milestone-1-provider-integration-foundation-3-4-weeks","title":"Milestone 1: Provider Integration Foundation (3-4 weeks)","text":"<p>Goal: Implement live DataCite and Zenodo API integrations</p> <p>Features: - DataCite API client (sandbox and production) - Zenodo API client (sandbox and production) - Authentication and credential management - Error handling and retry logic - API rate limiting and backoff - Configuration management for providers</p> <p>Deliverables: - <code>internal/doi/datacite_client.go</code> - Complete DataCite API implementation - <code>internal/doi/zenodo_client.go</code> - Complete Zenodo API implementation - <code>internal/doi/provider_config.go</code> - Provider configuration - Provider integration tests with sandbox APIs - Provider setup documentation</p> <p>Success Criteria: - [ ] Can create draft DOI in DataCite sandbox - [ ] Can publish DOI in DataCite sandbox - [ ] Can upload files and create DOI in Zenodo sandbox - [ ] Can update existing DOIs - [ ] Error handling works for common failure scenarios - [ ] Configuration management is user-friendly</p> <p>Complexity: HIGH (API integration, authentication, error handling)</p>"},{"location":"ROADMAP_v0.3.0/#milestone-2-microscopy-file-formats-2-3-weeks","title":"Milestone 2: Microscopy File Formats (2-3 weeks)","text":"<p>Goal: Extract metadata from CZI and OME-TIFF files</p> <p>Features: - CZI metadata extraction (dimensions, channels, objectives, timestamps) - OME-TIFF metadata extraction (OME-XML parsing) - Integration with Zeiss presets - Multi-dimensional image handling (Z-stacks, time series, channels) - Performance optimization for large images</p> <p>Deliverables: - <code>internal/metadata/czi_extractor.go</code> - Complete CZI implementation - <code>internal/metadata/ome_tiff_extractor.go</code> - Complete OME-TIFF implementation - Integration tests with real microscopy files - Updated preset validation for microscopy metadata - Microscopy format documentation</p> <p>Success Criteria: - [ ] Can extract metadata from Zeiss CZI files - [ ] Can extract metadata from OME-TIFF files - [ ] Preset validation works with extracted microscopy metadata - [ ] Performance: &lt; 5 seconds for typical microscopy files (&lt; 1 GB) - [ ] Handles multi-channel, Z-stack, time-series images</p> <p>Complexity: MEDIUM-HIGH (Binary format parsing, XML parsing, domain knowledge)</p>"},{"location":"ROADMAP_v0.3.0/#milestone-3-custom-preset-system-2-weeks","title":"Milestone 3: Custom Preset System (2 weeks)","text":"<p>Goal: Enable users to create and manage custom presets</p> <p>Features: - Preset creation from templates or scratch - YAML-based preset definition format - Preset validation rules (field types, ranges, patterns) - Preset import/export - Preset inheritance (extend existing presets) - Custom quality scoring weights</p> <p>Deliverables: - <code>internal/metadata/preset_builder.go</code> - Preset creation and validation - <code>cicada metadata preset create</code> - CLI command - <code>cicada metadata preset import/export</code> - CLI commands - Preset definition schema and examples - Custom preset documentation</p> <p>Success Criteria: - [ ] Users can create presets from YAML files - [ ] Custom presets validate correctly - [ ] Can inherit from built-in presets - [ ] Import/export works for sharing presets - [ ] Quality scoring reflects custom field weights</p> <p>Complexity: MEDIUM (Schema design, validation logic, CLI integration)</p>"},{"location":"ROADMAP_v0.3.0/#milestone-4-interactive-metadata-editing-2-3-weeks","title":"Milestone 4: Interactive Metadata Editing (2-3 weeks)","text":"<p>Goal: Provide interactive UI for metadata review and enrichment</p> <p>Features: - Terminal UI (TUI) for metadata editing - Form-based enrichment with validation - Side-by-side comparison (extracted vs enriched) - Quality score real-time updates - Field help and examples - Validation feedback</p> <p>Deliverables: - <code>internal/ui/metadata_editor.go</code> - TUI implementation - <code>cicada metadata edit</code> - CLI command - Integration with DOI workflow - Interactive editing documentation</p> <p>Success Criteria: - [ ] TUI works on Linux, macOS, Windows - [ ] Can edit all metadata fields interactively - [ ] Validation provides helpful feedback - [ ] Quality score updates in real-time - [ ] Can save and resume editing sessions</p> <p>Complexity: MEDIUM-HIGH (TUI library integration, UX design, validation)</p>"},{"location":"ROADMAP_v0.3.0/#milestone-5-bioinformatics-formats-2-weeks","title":"Milestone 5: Bioinformatics Formats (2 weeks)","text":"<p>Goal: Support BAM/SAM alignment and VCF variant files</p> <p>Features: - BAM/SAM metadata extraction (alignment statistics, references) - VCF metadata extraction (sample info, variant stats) - Integration with generic-sequencing preset - Performance optimization (BAM indexing)</p> <p>Deliverables: - <code>internal/metadata/bam_extractor.go</code> - BAM/SAM implementation - <code>internal/metadata/vcf_extractor.go</code> - VCF implementation - Integration tests with real bioinformatics files - Bioinformatics format documentation</p> <p>Success Criteria: - [ ] Can extract metadata from BAM/SAM files - [ ] Can extract metadata from VCF files - [ ] Performance: &lt; 1 second for typical files - [ ] Handles compressed formats (.bam.gz, .vcf.gz)</p> <p>Complexity: MEDIUM (Binary format parsing, domain knowledge)</p>"},{"location":"ROADMAP_v0.3.0/#milestone-6-production-readiness-2-weeks","title":"Milestone 6: Production Readiness (2 weeks)","text":"<p>Goal: Make v0.3.0 production-ready with caching, monitoring, and error recovery</p> <p>Features: - Metadata caching (avoid re-extraction) - API response caching - Retry logic with exponential backoff - Detailed logging and monitoring - Error recovery and rollback - Batch operation progress tracking</p> <p>Deliverables: - <code>internal/cache/metadata_cache.go</code> - Caching implementation - Enhanced logging throughout codebase - Batch operation commands - Production deployment documentation</p> <p>Success Criteria: - [ ] Metadata cache works correctly - [ ] Cached metadata significantly improves performance - [ ] Retry logic prevents transient failures - [ ] Logging provides useful debugging information - [ ] Batch operations track progress</p> <p>Complexity: MEDIUM (Caching strategy, error handling, logging)</p>"},{"location":"ROADMAP_v0.3.0/#milestone-7-additional-providers-stretch-2-3-weeks","title":"Milestone 7: Additional Providers (Stretch, 2-3 weeks)","text":"<p>Goal: Add Dryad and Figshare provider support</p> <p>Features: - Dryad API client - Figshare API client - Provider-specific metadata mapping - Multi-provider workflow</p> <p>Deliverables: - <code>internal/doi/dryad_client.go</code> - Dryad implementation - <code>internal/doi/figshare_client.go</code> - Figshare implementation - Provider comparison documentation</p> <p>Success Criteria: - [ ] Can create and publish DOIs on Dryad - [ ] Can create and publish DOIs on Figshare - [ ] Provider selection is seamless</p> <p>Complexity: MEDIUM (Similar to DataCite/Zenodo but different APIs)</p>"},{"location":"ROADMAP_v0.3.0/#milestone-8-api-server-mode-stretch-2-3-weeks","title":"Milestone 8: API Server Mode (Stretch, 2-3 weeks)","text":"<p>Goal: Provide REST API for programmatic access</p> <p>Features: - REST API server - OpenAPI/Swagger documentation - Authentication and authorization - Rate limiting - API client libraries (Go, Python)</p> <p>Deliverables: - <code>internal/api/server.go</code> - API server - <code>cicada serve</code> - CLI command - OpenAPI specification - API documentation and examples</p> <p>Success Criteria: - [ ] REST API covers all CLI functionality - [ ] API documentation is comprehensive - [ ] Authentication works correctly - [ ] Performance meets requirements</p> <p>Complexity: HIGH (API design, authentication, documentation)</p>"},{"location":"ROADMAP_v0.3.0/#timeline","title":"Timeline","text":""},{"location":"ROADMAP_v0.3.0/#conservative-estimate-5-6-months","title":"Conservative Estimate (5-6 months)","text":"Milestone Duration Target Completion 1. Provider Integration 4 weeks End of February 2025 2. Microscopy Formats 3 weeks Mid-March 2025 3. Custom Presets 2 weeks End of March 2025 4. Interactive Editing 3 weeks Mid-April 2025 5. Bioinformatics Formats 2 weeks End of April 2025 6. Production Readiness 2 weeks Mid-May 2025 v0.3.0 Release - End of May 2025"},{"location":"ROADMAP_v0.3.0/#aggressive-estimate-3-4-months","title":"Aggressive Estimate (3-4 months)","text":"<p>Focus on core features only (Milestones 1-3, 6):</p> Milestone Duration Target Completion 1. Provider Integration 3 weeks Mid-February 2025 2. Microscopy Formats 2 weeks End of February 2025 3. Custom Presets 1.5 weeks Mid-March 2025 6. Production Readiness 1.5 weeks End of March 2025 v0.3.0 Release - Early April 2025 <p>Defer Milestones 4, 5, 7, 8 to v0.3.1 or v0.4.0</p>"},{"location":"ROADMAP_v0.3.0/#dependencies","title":"Dependencies","text":""},{"location":"ROADMAP_v0.3.0/#external-dependencies","title":"External Dependencies","text":"<ol> <li>Provider Accounts/Credentials:</li> <li>DataCite sandbox account (free)</li> <li>DataCite production account (requires institutional membership)</li> <li>Zenodo sandbox account (free)</li> <li>Zenodo production account (free)</li> <li>Dryad account (paid per dataset)</li> <li> <p>Figshare account (free tier available)</p> </li> <li> <p>Test Data:</p> </li> <li>Real CZI files from Zeiss microscopes</li> <li>Real OME-TIFF files</li> <li>Real BAM/SAM files</li> <li> <p>Real VCF files</p> </li> <li> <p>Libraries:</p> </li> <li>CZI parsing library (may need to implement)</li> <li>OME-XML parsing library (Go XML)</li> <li>BAM/SAM parsing library (biogo/hts)</li> <li>VCF parsing library (available)</li> <li>TUI library (bubbletea or tview)</li> </ol>"},{"location":"ROADMAP_v0.3.0/#internal-dependencies","title":"Internal Dependencies","text":"<ul> <li>v0.2.0 must be released and stable</li> <li>Metadata extraction framework is extensible</li> <li>Provider registry supports multiple providers</li> <li>Preset system is extensible</li> </ul>"},{"location":"ROADMAP_v0.3.0/#feature-priorities","title":"Feature Priorities","text":""},{"location":"ROADMAP_v0.3.0/#high-priority-core-v030","title":"High Priority (Core v0.3.0)","text":"<ol> <li>Provider Integration - Without this, DOI workflow is incomplete</li> <li>Microscopy Formats - Target users need this for lab data</li> <li>Production Readiness - Required for reliable production use</li> </ol>"},{"location":"ROADMAP_v0.3.0/#medium-priority-should-include","title":"Medium Priority (Should Include)","text":"<ol> <li>Custom Presets - Users request this for lab-specific workflows</li> <li>Interactive Editing - Improves UX significantly</li> <li>Bioinformatics Formats - Expands user base</li> </ol>"},{"location":"ROADMAP_v0.3.0/#low-priority-can-defer","title":"Low Priority (Can Defer)","text":"<ol> <li>Additional Providers - Nice to have but not blocking</li> <li>API Server Mode - Advanced feature for later</li> </ol>"},{"location":"ROADMAP_v0.3.0/#success-metrics","title":"Success Metrics","text":""},{"location":"ROADMAP_v0.3.0/#adoption-metrics","title":"Adoption Metrics","text":"<ul> <li> 50+ users adopt v0.3.0 within 3 months</li> <li> 10+ labs use DOI minting in production</li> <li> 100+ DOIs minted through Cicada</li> </ul>"},{"location":"ROADMAP_v0.3.0/#technical-metrics","title":"Technical Metrics","text":"<ul> <li> Provider integration test coverage &gt; 80%</li> <li> Format extraction accuracy &gt; 95%</li> <li> API uptime &gt; 99.5% (for provider APIs)</li> <li> Average metadata extraction time &lt; 5 seconds (all formats)</li> </ul>"},{"location":"ROADMAP_v0.3.0/#user-satisfaction","title":"User Satisfaction","text":"<ul> <li> User satisfaction score &gt; \u2158</li> <li> Documentation findability &gt; \u2158</li> <li> DOI workflow completion rate &gt; 90%</li> <li> Feature requests &lt; 10% of feedback (vs bugs)</li> </ul>"},{"location":"ROADMAP_v0.3.0/#risk-assessment","title":"Risk Assessment","text":""},{"location":"ROADMAP_v0.3.0/#high-risk","title":"High Risk","text":"<ol> <li>Provider API Changes - DataCite/Zenodo might change APIs</li> <li> <p>Mitigation: Version API clients, monitor changelog, integration tests</p> </li> <li> <p>Authentication Complexity - Provider auth might be difficult</p> </li> <li> <p>Mitigation: Start with sandbox, clear documentation, examples</p> </li> <li> <p>Binary Format Parsing - CZI/BAM parsing might be complex</p> </li> <li>Mitigation: Use existing libraries where possible, limit scope</li> </ol>"},{"location":"ROADMAP_v0.3.0/#medium-risk","title":"Medium Risk","text":"<ol> <li>Performance - Large microscopy files might be slow</li> <li> <p>Mitigation: Benchmark early, optimize, consider sampling</p> </li> <li> <p>TUI Complexity - Interactive UI might be complex</p> </li> <li> <p>Mitigation: Use proven library (bubbletea), start simple</p> </li> <li> <p>Custom Preset Abuse - Users might create invalid presets</p> </li> <li>Mitigation: Strong validation, good error messages, examples</li> </ol>"},{"location":"ROADMAP_v0.3.0/#low-risk","title":"Low Risk","text":"<ol> <li>Timeline Slippage - Features might take longer</li> <li> <p>Mitigation: Conservative estimates, prioritize core features</p> </li> <li> <p>Dependency Issues - External libraries might have bugs</p> </li> <li>Mitigation: Vet libraries early, have fallback options</li> </ol>"},{"location":"ROADMAP_v0.3.0/#breaking-changes","title":"Breaking Changes","text":""},{"location":"ROADMAP_v0.3.0/#potential-breaking-changes","title":"Potential Breaking Changes","text":"<ol> <li>Provider Configuration Format - May need to change config structure</li> <li> <p>Migration: Automatic config migration, clear migration guide</p> </li> <li> <p>Metadata Schema Evolution - New formats might require schema changes</p> </li> <li> <p>Migration: Backward compatible schema, version metadata</p> </li> <li> <p>CLI Flag Changes - May rename or reorganize flags</p> </li> <li>Migration: Deprecation warnings, maintain aliases</li> </ol>"},{"location":"ROADMAP_v0.3.0/#compatibility-promise","title":"Compatibility Promise","text":"<p>Goal: Maintain backward compatibility with v0.2.0 where possible.</p> <ul> <li>All v0.2.0 commands continue to work</li> <li>v0.2.0 config files remain valid</li> <li>v0.2.0 metadata format remains supported</li> <li>Clear migration paths for any breaking changes</li> </ul>"},{"location":"ROADMAP_v0.3.0/#user-stories","title":"User Stories","text":""},{"location":"ROADMAP_v0.3.0/#story-1-postdoc-publishes-sequencing-dataset","title":"Story 1: Postdoc Publishes Sequencing Dataset","text":"<p>As a postdoc researcher I want to mint a DOI for my FASTQ files and deposit them in Zenodo So that I can cite the dataset in my paper</p> <p>Acceptance Criteria: - Can extract metadata from FASTQ files - Can enrich with author/description - Can upload files to Zenodo - Can mint DOI in one command - Receives permanent DOI URL</p> <p>v0.3.0 Solution: <pre><code># Extract and prepare\ncicada doi prepare sample_R1.fastq.gz sample_R2.fastq.gz \\\n  --enrich metadata.yaml \\\n  --provider zenodo\n\n# Upload and mint DOI (NEW in v0.3.0)\ncicada doi publish \\\n  --files sample_R1.fastq.gz,sample_R2.fastq.gz \\\n  --provider zenodo\n\n# Output: DOI: 10.5281/zenodo.123456\n# URL: https://zenodo.org/record/123456\n</code></pre></p>"},{"location":"ROADMAP_v0.3.0/#story-2-lab-manager-extracts-microscopy-metadata","title":"Story 2: Lab Manager Extracts Microscopy Metadata","text":"<p>As a lab manager I want to extract metadata from CZI files from our Zeiss LSM 880 So that I can catalog our microscopy data</p> <p>Acceptance Criteria: - Can extract metadata from CZI files - Validates against Zeiss LSM 880 preset - Captures all imaging parameters - Fast enough for batch processing</p> <p>v0.3.0 Solution: <pre><code># Extract from CZI file (NEW in v0.3.0)\ncicada metadata extract image_001.czi \\\n  --preset zeiss-lsm-880 \\\n  --format json \\\n  --output metadata.json\n\n# Batch processing\nfind . -name \"*.czi\" | parallel -j 8 \\\n  cicada metadata extract {} --preset zeiss-lsm-880\n</code></pre></p>"},{"location":"ROADMAP_v0.3.0/#story-3-bioinformatician-creates-custom-preset","title":"Story 3: Bioinformatician Creates Custom Preset","text":"<p>As a bioinformatician I want to create a custom preset for our lab's RNA-seq pipeline So that all lab members validate metadata consistently</p> <p>Acceptance Criteria: - Can define custom required/optional fields - Can specify validation rules - Can share preset with team - Preset validates correctly</p> <p>v0.3.0 Solution: <pre><code># Create custom preset (NEW in v0.3.0)\ncicada metadata preset create lab-rnaseq \\\n  --template generic-sequencing \\\n  --add-required pipeline_version \\\n  --add-optional read_length_min \\\n  --add-optional read_length_max\n\n# Edit preset definition\nvim ~/.cicada/presets/lab-rnaseq.yaml\n\n# Share with team\ncicada metadata preset export lab-rnaseq &gt; lab-rnaseq.yaml\n# Team imports: cicada metadata preset import lab-rnaseq.yaml\n\n# Use custom preset\ncicada metadata extract sample.fastq --preset lab-rnaseq\n</code></pre></p>"},{"location":"ROADMAP_v0.3.0/#story-4-data-curator-uses-interactive-editor","title":"Story 4: Data Curator Uses Interactive Editor","text":"<p>As a data curator I want to review and enrich metadata interactively So that I can ensure high-quality metadata before DOI minting</p> <p>Acceptance Criteria: - Can see extracted metadata - Can edit fields interactively - Can see quality score in real-time - Can validate before saving</p> <p>v0.3.0 Solution: <pre><code># Interactive editing (NEW in v0.3.0)\ncicada metadata edit sample.fastq\n\n# Opens TUI showing:\n# - Extracted metadata (read-only)\n# - Editable enrichment fields\n# - Real-time quality score\n# - Validation feedback\n# - Save/cancel options\n</code></pre></p>"},{"location":"ROADMAP_v0.3.0/#story-5-research-scientist-publishes-to-datacite","title":"Story 5: Research Scientist Publishes to DataCite","text":"<p>As a research scientist I want to mint a DOI through my institution's DataCite membership So that I get a DOI with my institution's prefix</p> <p>Acceptance Criteria: - Can configure DataCite credentials - Can mint DOI in production - Gets institutional DOI prefix - Can provide landing page URL</p> <p>v0.3.0 Solution: <pre><code># Configure DataCite (one-time setup)\ncicada config set provider datacite\ncicada config set datacite.repository_id MIT.BIO\ncicada config set datacite.username my_username\ncicada config set datacite.password my_password\n\n# Mint DOI (NEW in v0.3.0)\ncicada doi publish \\\n  --metadata doi-ready.json \\\n  --landing-page https://mylab.mit.edu/data/dataset-001 \\\n  --provider datacite\n\n# Output: DOI: 10.12345/dataset-001\n</code></pre></p>"},{"location":"ROADMAP_v0.3.0/#documentation-plan","title":"Documentation Plan","text":""},{"location":"ROADMAP_v0.3.0/#new-documentation-v030","title":"New Documentation (v0.3.0)","text":"<ol> <li>Provider Integration Guide</li> <li>DataCite sandbox/production setup</li> <li>Zenodo sandbox/production setup</li> <li>Dryad setup</li> <li>Figshare setup</li> <li>Authentication and credentials</li> <li> <p>Troubleshooting provider issues</p> </li> <li> <p>Microscopy Formats Guide</p> </li> <li>CZI metadata extraction</li> <li>OME-TIFF metadata extraction</li> <li>Zeiss preset usage</li> <li> <p>Multi-dimensional image handling</p> </li> <li> <p>Custom Preset Guide</p> </li> <li>Creating custom presets</li> <li>Preset definition format</li> <li>Validation rules</li> <li>Sharing presets</li> <li> <p>Best practices</p> </li> <li> <p>Interactive Editing Guide</p> </li> <li>TUI navigation</li> <li>Editing workflows</li> <li>Keyboard shortcuts</li> <li> <p>Tips and tricks</p> </li> <li> <p>Production Deployment Guide</p> </li> <li>Configuration management</li> <li>Caching strategies</li> <li>Error handling</li> <li>Monitoring and logging</li> <li>Backup and recovery</li> </ol>"},{"location":"ROADMAP_v0.3.0/#updated-documentation","title":"Updated Documentation","text":"<ul> <li>User Scenarios v0.3.0 (update with new features)</li> <li>README.md (add v0.3.0 features)</li> <li>CHANGELOG.md (track changes)</li> <li>Integration Testing Guide (new test patterns)</li> </ul>"},{"location":"ROADMAP_v0.3.0/#testing-strategy","title":"Testing Strategy","text":""},{"location":"ROADMAP_v0.3.0/#integration-tests","title":"Integration Tests","text":"<ul> <li> Provider integration tests (sandbox APIs)</li> <li> CZI extraction integration tests</li> <li> OME-TIFF extraction integration tests</li> <li> BAM/SAM extraction integration tests</li> <li> VCF extraction integration tests</li> <li> Custom preset integration tests</li> <li> Interactive UI integration tests (if possible)</li> <li> End-to-end DOI workflow tests</li> </ul>"},{"location":"ROADMAP_v0.3.0/#unit-tests","title":"Unit Tests","text":"<ul> <li> Provider client unit tests (mocked APIs)</li> <li> Format parser unit tests</li> <li> Custom preset validation unit tests</li> <li> Cache implementation unit tests</li> </ul>"},{"location":"ROADMAP_v0.3.0/#manual-tests","title":"Manual Tests","text":"<ul> <li> Test with real provider accounts</li> <li> Test with real microscopy files</li> <li> Test UI on different terminals</li> <li> Test on Linux, macOS, Windows</li> </ul>"},{"location":"ROADMAP_v0.3.0/#performance-tests","title":"Performance Tests","text":"<ul> <li> Large CZI file extraction (&lt; 5 seconds)</li> <li> Batch processing (1000 files)</li> <li> API rate limiting</li> <li> Cache hit rate</li> </ul>"},{"location":"ROADMAP_v0.3.0/#open-questions","title":"Open Questions","text":"<ol> <li>Provider Priority: Should we focus on DataCite or Zenodo first?</li> <li> <p>Suggestion: Zenodo (easier auth, integrated storage, free)</p> </li> <li> <p>TUI Library: Which library for interactive editing?</p> </li> <li>Options: bubbletea (modern, powerful) vs tview (mature, simpler)</li> <li> <p>Suggestion: bubbletea (better long-term)</p> </li> <li> <p>CZI Parsing: Use existing library or implement from scratch?</p> </li> <li>Options: Port libCZI vs implement minimal reader</li> <li> <p>Suggestion: Implement minimal reader (lighter dependency)</p> </li> <li> <p>Caching Strategy: What to cache and where?</p> </li> <li>Options: File-based vs SQLite vs memory</li> <li> <p>Suggestion: File-based for simplicity (JSON files in ~/.cicada/cache/)</p> </li> <li> <p>Custom Preset Format: YAML or JSON?</p> </li> <li> <p>Suggestion: YAML (more user-friendly, supports comments)</p> </li> <li> <p>Backward Compatibility: How strict?</p> </li> <li>Suggestion: Strict for CLI, flexible for config (with migration)</li> </ol>"},{"location":"ROADMAP_v0.3.0/#next-steps","title":"Next Steps","text":"<ol> <li>Validate Roadmap - Get feedback from target users</li> <li>Create GitHub Milestones - Set up project tracking</li> <li>Break Down Milestone 1 - Create detailed task list for provider integration</li> <li>Set Up Provider Accounts - Get sandbox credentials for testing</li> <li>Gather Test Data - Collect sample CZI, OME-TIFF, BAM, VCF files</li> <li>Start Development - Begin Milestone 1 (Provider Integration)</li> </ol>"},{"location":"ROADMAP_v0.3.0/#version-history","title":"Version History","text":"<ul> <li>v1.0 (2025-01-23): Initial v0.3.0 roadmap created after v0.2.0 release</li> </ul>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/","title":"v0.3.0 Milestone 1: Provider Integration Foundation","text":"<p>Status: Not Started Target: 3-4 weeks (February 2025) Priority: CRITICAL</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#overview","title":"Overview","text":"<p>Milestone 1 implements live DataCite and Zenodo API integrations, enabling users to mint real DOIs from Cicada. This is the most critical feature for v0.3.0 as it completes the DOI workflow that was prepared in v0.2.0.</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#goals","title":"Goals","text":"<ul> <li> Users can mint DOIs in DataCite sandbox</li> <li> Users can mint DOIs in DataCite production</li> <li> Users can upload files and mint DOIs in Zenodo sandbox</li> <li> Users can upload files and mint DOIs in Zenodo production</li> <li> Error handling covers common failure scenarios</li> <li> Configuration management is straightforward</li> <li> Comprehensive documentation for setup and usage</li> </ul>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#task-breakdown","title":"Task Breakdown","text":""},{"location":"ROADMAP_v0.3.0_MILESTONE1/#phase-1-foundation-week-1","title":"Phase 1: Foundation (Week 1)","text":""},{"location":"ROADMAP_v0.3.0_MILESTONE1/#task-11-provider-configuration-system","title":"Task 1.1: Provider Configuration System","text":"<p>Complexity: LOW Estimated Time: 2 days Dependencies: None</p> <p>Description: Create configuration system for managing provider credentials and settings.</p> <p>Implementation:</p> <ol> <li> <p>Config Structure (<code>internal/doi/provider_config.go</code>):    <pre><code>type ProviderConfig struct {\n    Active   string                    // Active provider name\n    Providers map[string]ProviderCreds // Provider-specific credentials\n}\n\ntype ProviderCreds struct {\n    Type       string            // \"datacite\", \"zenodo\", etc.\n    Sandbox    bool              // Use sandbox environment\n    BaseURL    string            // API base URL\n    Credentials map[string]string // Provider-specific credentials\n}\n</code></pre></p> </li> <li> <p>Configuration Commands:</p> </li> <li><code>cicada config set provider.active zenodo</code></li> <li><code>cicada config set provider.zenodo.token TOKEN</code></li> <li><code>cicada config set provider.datacite.repository_id REPO_ID</code></li> <li><code>cicada config set provider.datacite.username USER</code></li> <li> <p><code>cicada config set provider.datacite.password PASS</code></p> </li> <li> <p>Security:</p> </li> <li>Encrypt passwords in config file</li> <li>Support environment variables for secrets</li> <li>Warn if secrets in plaintext</li> </ol> <p>Deliverables: - [ ] <code>internal/doi/provider_config.go</code> (config structures) - [ ] Config get/set methods - [ ] Environment variable support - [ ] Unit tests for config management</p> <p>Acceptance Criteria: - [ ] Can configure multiple providers - [ ] Credentials are securely stored - [ ] Environment variables override config file - [ ] Config validation catches errors</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#task-12-datacite-api-client-core-infrastructure","title":"Task 1.2: DataCite API Client - Core Infrastructure","text":"<p>Complexity: MEDIUM Estimated Time: 3 days Dependencies: Task 1.1</p> <p>Description: Implement core DataCite API client with authentication and basic DOI operations.</p> <p>Implementation:</p> <ol> <li> <p>Client Structure (<code>internal/doi/datacite_client.go</code>):    <pre><code>type DataCiteClient struct {\n    baseURL      string\n    repositoryID string\n    username     string\n    password     string\n    httpClient   *http.Client\n}\n\nfunc NewDataCiteClient(config *ProviderCreds) (*DataCiteClient, error)\nfunc (c *DataCiteClient) CreateDOI(metadata *Dataset) (*DOI, error)\nfunc (c *DataCiteClient) UpdateDOI(doi string, metadata *Dataset) error\nfunc (c *DataCiteClient) PublishDOI(doi string) error\nfunc (c *DataCiteClient) GetDOI(doi string) (*DOI, error)\nfunc (c *DataCiteClient) ListDOIs(filter *DOIFilter) ([]*DOI, error)\n</code></pre></p> </li> <li> <p>Authentication:</p> </li> <li>HTTP Basic Auth (username:password)</li> <li>Set User-Agent header</li> <li> <p>Handle 401/403 responses</p> </li> <li> <p>API Endpoints:</p> </li> <li><code>POST /dois</code> - Create draft DOI</li> <li><code>PUT /dois/{id}</code> - Update DOI</li> <li><code>PUT /dois/{id}/actions/publish</code> - Publish DOI</li> <li><code>GET /dois/{id}</code> - Get DOI details</li> <li> <p><code>GET /dois</code> - List DOIs</p> </li> <li> <p>Request/Response Handling:</p> </li> <li>JSON serialization/deserialization</li> <li>Error response parsing</li> <li>API version headers</li> </ol> <p>Deliverables: - [ ] <code>internal/doi/datacite_client.go</code> (client implementation) - [ ] <code>internal/doi/datacite_types.go</code> (API request/response types) - [ ] Unit tests with mocked HTTP responses</p> <p>Acceptance Criteria: - [ ] Client authenticates successfully - [ ] Can serialize Dataset to DataCite JSON - [ ] Can deserialize API responses - [ ] Error handling covers API error responses</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#task-13-zenodo-api-client-core-infrastructure","title":"Task 1.3: Zenodo API Client - Core Infrastructure","text":"<p>Complexity: MEDIUM Estimated Time: 3 days Dependencies: Task 1.1</p> <p>Description: Implement core Zenodo API client with token authentication and basic operations.</p> <p>Implementation:</p> <ol> <li> <p>Client Structure (<code>internal/doi/zenodo_client.go</code>):    <pre><code>type ZenodoClient struct {\n    baseURL    string\n    token      string\n    httpClient *http.Client\n}\n\nfunc NewZenodoClient(config *ProviderCreds) (*ZenodoClient, error)\nfunc (c *ZenodoClient) CreateDeposition() (*Deposition, error)\nfunc (c *ZenodoClient) UploadFile(depID string, file io.Reader, filename string) error\nfunc (c *ZenodoClient) UpdateMetadata(depID string, metadata *Dataset) error\nfunc (c *ZenodoClient) PublishDeposition(depID string) (*DOI, error)\nfunc (c *ZenodoClient) GetDeposition(depID string) (*Deposition, error)\n</code></pre></p> </li> <li> <p>Authentication:</p> </li> <li>Bearer token authentication</li> <li> <p>Token in query param or header</p> </li> <li> <p>API Endpoints:</p> </li> <li><code>POST /api/deposit/depositions</code> - Create deposition</li> <li><code>POST /api/deposit/depositions/{id}/files</code> - Upload file</li> <li><code>PUT /api/deposit/depositions/{id}</code> - Update metadata</li> <li><code>POST /api/deposit/depositions/{id}/actions/publish</code> - Publish</li> <li> <p><code>GET /api/deposit/depositions/{id}</code> - Get deposition</p> </li> <li> <p>File Upload:</p> </li> <li>Multipart form data</li> <li>Progress tracking (optional)</li> <li>Checksum validation</li> </ol> <p>Deliverables: - [ ] <code>internal/doi/zenodo_client.go</code> (client implementation) - [ ] <code>internal/doi/zenodo_types.go</code> (API request/response types) - [ ] Unit tests with mocked HTTP responses</p> <p>Acceptance Criteria: - [ ] Client authenticates with token - [ ] Can create depositions - [ ] Can upload files - [ ] Can update metadata and publish</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#phase-2-integration-week-2","title":"Phase 2: Integration (Week 2)","text":""},{"location":"ROADMAP_v0.3.0_MILESTONE1/#task-21-provider-registry-enhancement","title":"Task 2.1: Provider Registry Enhancement","text":"<p>Complexity: LOW Estimated Time: 1 day Dependencies: Tasks 1.2, 1.3</p> <p>Description: Enhance existing provider registry to support live API clients.</p> <p>Implementation:</p> <ol> <li> <p>Update Provider Interface (<code>internal/doi/provider.go</code>):    <pre><code>type Provider interface {\n    Name() string\n    Validate(dataset *Dataset) error\n    Prepare(dataset *Dataset) (*PrepareResult, error)\n    CreateDOI(dataset *Dataset) (*DOI, error)      // NEW\n    PublishDOI(doi string) error                    // NEW\n    UploadFiles(doi string, files []string) error   // NEW (Zenodo)\n    GetDOI(doi string) (*DOI, error)                // NEW\n}\n</code></pre></p> </li> <li> <p>Update Registry:</p> </li> <li>Register DataCite provider</li> <li>Register Zenodo provider</li> <li> <p>Provider selection from config</p> </li> <li> <p>Factory Pattern:    <pre><code>func (r *ProviderRegistry) GetProvider(name string) (Provider, error)\nfunc NewDataCiteProvider(config *ProviderCreds) *DataCiteProvider\nfunc NewZenodoProvider(config *ProviderCreds) *ZenodoProvider\n</code></pre></p> </li> </ol> <p>Deliverables: - [ ] Updated <code>internal/doi/provider.go</code> - [ ] Provider factory implementation - [ ] Integration tests</p> <p>Acceptance Criteria: - [ ] Can get DataCite provider from registry - [ ] Can get Zenodo provider from registry - [ ] Provider selection respects configuration</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#task-22-datacite-metadata-mapping","title":"Task 2.2: DataCite Metadata Mapping","text":"<p>Complexity: MEDIUM Estimated Time: 2 days Dependencies: Task 2.1</p> <p>Description: Map Cicada Dataset to DataCite JSON format (v4.5 schema).</p> <p>Implementation:</p> <ol> <li> <p>Mapping Function (<code>internal/doi/datacite_mapper.go</code>):    <pre><code>func MapToDataCite(dataset *Dataset) (*DataCiteMetadata, error)\n</code></pre></p> </li> <li> <p>DataCite JSON Schema:    <pre><code>{\n  \"data\": {\n    \"type\": \"dois\",\n    \"attributes\": {\n      \"prefix\": \"10.12345\",\n      \"titles\": [{\"title\": \"Dataset Title\"}],\n      \"creators\": [{\"name\": \"Author Name\", \"nameType\": \"Personal\"}],\n      \"publisher\": \"Publisher Name\",\n      \"publicationYear\": 2025,\n      \"types\": {\"resourceTypeGeneral\": \"Dataset\"},\n      \"descriptions\": [{\"description\": \"...\", \"descriptionType\": \"Abstract\"}],\n      ...\n    }\n  }\n}\n</code></pre></p> </li> <li> <p>Handle All Fields:</p> </li> <li>Required: title, creators, publisher, publicationYear, resourceType</li> <li> <p>Recommended: subjects, contributors, dates, relatedIdentifiers, descriptions, geoLocations, fundingReferences</p> </li> <li> <p>Validation:</p> </li> <li>Ensure all required fields present</li> <li>Validate field formats (ORCID, DOI, etc.)</li> <li>Check constraints (year range, etc.)</li> </ol> <p>Deliverables: - [ ] <code>internal/doi/datacite_mapper.go</code> - [ ] DataCite JSON schema types - [ ] Mapper unit tests with examples - [ ] Validation tests</p> <p>Acceptance Criteria: - [ ] Maps all Dataset fields correctly - [ ] Produces valid DataCite JSON - [ ] Handles optional fields gracefully - [ ] Validation catches common errors</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#task-23-zenodo-metadata-mapping","title":"Task 2.3: Zenodo Metadata Mapping","text":"<p>Complexity: MEDIUM Estimated Time: 2 days Dependencies: Task 2.1</p> <p>Description: Map Cicada Dataset to Zenodo JSON format.</p> <p>Implementation:</p> <ol> <li> <p>Mapping Function (<code>internal/doi/zenodo_mapper.go</code>):    <pre><code>func MapToZenodo(dataset *Dataset) (*ZenodoMetadata, error)\n</code></pre></p> </li> <li> <p>Zenodo JSON Schema:    <pre><code>{\n  \"metadata\": {\n    \"title\": \"Dataset Title\",\n    \"upload_type\": \"dataset\",\n    \"description\": \"Dataset description\",\n    \"creators\": [{\"name\": \"Author Name\", \"orcid\": \"0000-0001-2345-6789\"}],\n    \"keywords\": [\"keyword1\", \"keyword2\"],\n    \"access_right\": \"open\",\n    \"license\": \"cc-by-4.0\",\n    ...\n  }\n}\n</code></pre></p> </li> <li> <p>Handle All Fields:</p> </li> <li>Required: title, upload_type, description, creators</li> <li> <p>Optional: keywords, notes, access_right, license, communities, grants</p> </li> <li> <p>License Mapping:</p> </li> <li>Map common license IDs to Zenodo format</li> <li>Default to CC-BY-4.0</li> </ol> <p>Deliverables: - [ ] <code>internal/doi/zenodo_mapper.go</code> - [ ] Zenodo JSON schema types - [ ] Mapper unit tests - [ ] License mapping table</p> <p>Acceptance Criteria: - [ ] Maps all Dataset fields correctly - [ ] Produces valid Zenodo JSON - [ ] License mapping works - [ ] Handles optional fields</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#phase-3-cli-integration-week-3","title":"Phase 3: CLI Integration (Week 3)","text":""},{"location":"ROADMAP_v0.3.0_MILESTONE1/#task-31-doi-publish-command","title":"Task 3.1: DOI Publish Command","text":"<p>Complexity: MEDIUM Estimated Time: 2 days Dependencies: Phase 2</p> <p>Description: Implement <code>cicada doi publish</code> command for minting DOIs.</p> <p>Implementation:</p> <ol> <li> <p>Command Structure (<code>internal/cli/doi.go</code>):    <pre><code>cicada doi publish [files...] [flags]\n\nFlags:\n  --metadata FILE        Metadata file (from doi prepare)\n  --provider STRING      Provider: datacite, zenodo (default: from config)\n  --files FILE[,FILE]    Files to upload (Zenodo only)\n  --landing-page URL     Landing page URL (DataCite)\n  --dry-run              Show what would be published\n</code></pre></p> </li> <li> <p>Workflow:</p> </li> <li>Load metadata file</li> <li>Select provider from config</li> <li>Call provider.CreateDOI()</li> <li>Upload files (if Zenodo)</li> <li>Call provider.PublishDOI()</li> <li> <p>Display DOI and URL</p> </li> <li> <p>Output:    <pre><code>Creating DOI...\nUploading files... (if Zenodo)\n\u2713 File 1 uploaded (5.2 MB)\n\u2713 File 2 uploaded (3.8 MB)\nPublishing DOI...\n\n\u2713 DOI Published!\nDOI: 10.5281/zenodo.123456\nURL: https://zenodo.org/record/123456\n</code></pre></p> </li> </ol> <p>Deliverables: - [ ] Update <code>internal/cli/doi.go</code> with publish command - [ ] Progress display - [ ] Error handling - [ ] Integration tests</p> <p>Acceptance Criteria: - [ ] Can publish DOI to DataCite - [ ] Can publish DOI to Zenodo with file upload - [ ] Progress is displayed - [ ] Errors are handled gracefully</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#task-32-doi-status-command","title":"Task 3.2: DOI Status Command","text":"<p>Complexity: LOW Estimated Time: 1 day Dependencies: Task 3.1</p> <p>Description: Implement <code>cicada doi status</code> command for checking DOI state.</p> <p>Implementation:</p> <ol> <li> <p>Command Structure:    <pre><code>cicada doi status &lt;doi&gt; [flags]\n\nFlags:\n  --provider STRING    Provider (optional, auto-detect from DOI)\n  --format STRING      Output format: table, json, yaml\n</code></pre></p> </li> <li> <p>Workflow:</p> </li> <li>Parse DOI</li> <li>Detect provider (from DOI prefix)</li> <li>Call provider.GetDOI()</li> <li> <p>Display status</p> </li> <li> <p>Output:    <pre><code>DOI: 10.5281/zenodo.123456\nStatus: Published\nTitle: Dataset Title\nAuthors: Jane Smith, John Doe\nPublished: 2025-01-15\nURL: https://zenodo.org/record/123456\n</code></pre></p> </li> </ol> <p>Deliverables: - [ ] <code>cicada doi status</code> command - [ ] DOI parsing and provider detection - [ ] Output formatting</p> <p>Acceptance Criteria: - [ ] Can check DataCite DOI status - [ ] Can check Zenodo DOI status - [ ] Auto-detects provider from DOI</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#task-33-doi-list-command","title":"Task 3.3: DOI List Command","text":"<p>Complexity: LOW Estimated Time: 1 day Dependencies: Task 3.1</p> <p>Description: Implement <code>cicada doi list</code> command for listing user's DOIs.</p> <p>Implementation:</p> <ol> <li> <p>Command Structure:    <pre><code>cicada doi list [flags]\n\nFlags:\n  --provider STRING    Provider: datacite, zenodo, all (default: all)\n  --limit INT          Number of results (default: 20)\n  --format STRING      Output format: table, json, yaml\n</code></pre></p> </li> <li> <p>Workflow:</p> </li> <li>Get provider(s) from config</li> <li>Call provider.ListDOIs()</li> <li> <p>Format output</p> </li> <li> <p>Output:    <pre><code>DOI                        Title                Status      Published\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n10.5281/zenodo.123456     Dataset 1            Published   2025-01-15\n10.12345/dataset-001      Dataset 2            Draft       -\n</code></pre></p> </li> </ol> <p>Deliverables: - [ ] <code>cicada doi list</code> command - [ ] Output formatting - [ ] Pagination support</p> <p>Acceptance Criteria: - [ ] Lists DOIs from DataCite - [ ] Lists DOIs from Zenodo - [ ] Can list from all providers</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#phase-4-error-handling-polish-week-4","title":"Phase 4: Error Handling &amp; Polish (Week 4)","text":""},{"location":"ROADMAP_v0.3.0_MILESTONE1/#task-41-comprehensive-error-handling","title":"Task 4.1: Comprehensive Error Handling","text":"<p>Complexity: MEDIUM Estimated Time: 2 days Dependencies: All previous tasks</p> <p>Description: Implement robust error handling for all API operations.</p> <p>Implementation:</p> <ol> <li> <p>Error Types (<code>internal/doi/errors.go</code>):    <pre><code>type APIError struct {\n    StatusCode int\n    Message    string\n    Details    map[string]interface{}\n}\n\ntype AuthenticationError struct{ APIError }\ntype ValidationError struct{ APIError }\ntype RateLimitError struct{ APIError }\ntype NetworkError struct{ error }\n</code></pre></p> </li> <li> <p>Retry Logic:    <pre><code>func WithRetry(fn func() error, maxRetries int) error\n</code></pre></p> </li> <li>Exponential backoff</li> <li>Retry on transient failures (5xx, network)</li> <li> <p>Don't retry on client errors (4xx)</p> </li> <li> <p>User-Friendly Messages:</p> </li> <li>Parse API error responses</li> <li>Provide actionable suggestions</li> <li> <p>Include error codes</p> </li> <li> <p>Error Handling Patterns:</p> </li> <li>Authentication failures: suggest checking credentials</li> <li>Validation errors: show which fields are invalid</li> <li>Rate limiting: suggest wait time</li> <li>Network errors: suggest retry</li> </ol> <p>Deliverables: - [ ] <code>internal/doi/errors.go</code> (error types) - [ ] Retry logic implementation - [ ] Error message improvements - [ ] Error handling tests</p> <p>Acceptance Criteria: - [ ] Transient failures are retried - [ ] Error messages are helpful - [ ] Retry logic uses backoff - [ ] All error types are handled</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#task-42-integration-testing-with-sandbox-apis","title":"Task 4.2: Integration Testing with Sandbox APIs","text":"<p>Complexity: MEDIUM Estimated Time: 3 days Dependencies: All previous tasks</p> <p>Description: Create comprehensive integration tests using sandbox APIs.</p> <p>Implementation:</p> <ol> <li>Test Setup (<code>internal/integration/provider_test.go</code>):</li> <li>Sandbox credentials from environment variables</li> <li>Skip tests if credentials not available</li> <li> <p>Clean up resources after tests</p> </li> <li> <p>DataCite Integration Tests:    <pre><code>func TestDataCite_EndToEnd(t *testing.T)\nfunc TestDataCite_CreateDraftDOI(t *testing.T)\nfunc TestDataCite_PublishDOI(t *testing.T)\nfunc TestDataCite_UpdateDOI(t *testing.T)\nfunc TestDataCite_GetDOI(t *testing.T)\nfunc TestDataCite_AuthenticationFailure(t *testing.T)\nfunc TestDataCite_ValidationError(t *testing.T)\n</code></pre></p> </li> <li> <p>Zenodo Integration Tests:    <pre><code>func TestZenodo_EndToEnd(t *testing.T)\nfunc TestZenodo_CreateDeposition(t *testing.T)\nfunc TestZenodo_UploadFile(t *testing.T)\nfunc TestZenodo_PublishDeposition(t *testing.T)\nfunc TestZenodo_GetDeposition(t *testing.T)\nfunc TestZenodo_AuthenticationFailure(t *testing.T)\n</code></pre></p> </li> <li> <p>CLI Integration Tests:    <pre><code>func TestCLI_DOIPublish_DataCite(t *testing.T)\nfunc TestCLI_DOIPublish_Zenodo(t *testing.T)\nfunc TestCLI_DOIStatus(t *testing.T)\nfunc TestCLI_DOIList(t *testing.T)\n</code></pre></p> </li> </ol> <p>Deliverables: - [ ] <code>internal/integration/provider_datacite_test.go</code> - [ ] <code>internal/integration/provider_zenodo_test.go</code> - [ ] <code>internal/integration/cli_doi_test.go</code> - [ ] Test fixtures and helpers</p> <p>Acceptance Criteria: - [ ] All integration tests pass with sandbox APIs - [ ] Tests clean up resources - [ ] Tests are reproducible - [ ] Tests cover success and error scenarios</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#task-43-documentation","title":"Task 4.3: Documentation","text":"<p>Complexity: LOW Estimated Time: 2 days Dependencies: All previous tasks</p> <p>Description: Create comprehensive documentation for provider integration.</p> <p>Documentation Files:</p> <ol> <li>Provider Setup Guide (update <code>docs/PROVIDERS.md</code>):</li> <li>DataCite sandbox setup (step-by-step)</li> <li>DataCite production setup</li> <li>Zenodo sandbox setup</li> <li>Zenodo production setup</li> <li>Configuration examples</li> <li> <p>Troubleshooting</p> </li> <li> <p>DOI Publishing Guide (new <code>docs/DOI_PUBLISHING.md</code>):</p> </li> <li>End-to-end publishing workflow</li> <li>Provider comparison (when to use each)</li> <li>Command examples</li> <li>Best practices</li> <li> <p>Common issues</p> </li> <li> <p>API Reference (update):</p> </li> <li>Provider interface documentation</li> <li>Client method documentation</li> <li>Error types documentation</li> </ol> <p>Deliverables: - [ ] Updated <code>docs/PROVIDERS.md</code> - [ ] New <code>docs/DOI_PUBLISHING.md</code> - [ ] API reference documentation - [ ] Example workflows</p> <p>Acceptance Criteria: - [ ] User can set up DataCite from documentation - [ ] User can set up Zenodo from documentation - [ ] All commands are documented with examples - [ ] Troubleshooting covers common issues</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#testing-strategy","title":"Testing Strategy","text":""},{"location":"ROADMAP_v0.3.0_MILESTONE1/#unit-tests","title":"Unit Tests","text":"<ul> <li> Provider configuration (Task 1.1)</li> <li> DataCite client (mocked HTTP) (Task 1.2)</li> <li> Zenodo client (mocked HTTP) (Task 1.3)</li> <li> Metadata mappers (Tasks 2.2, 2.3)</li> <li> Error handling (Task 4.1)</li> </ul> <p>Target Coverage: &gt; 80%</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#integration-tests","title":"Integration Tests","text":"<ul> <li> DataCite sandbox API (Task 4.2)</li> <li> Zenodo sandbox API (Task 4.2)</li> <li> CLI commands (Task 4.2)</li> </ul> <p>Environment Variables Required: - <code>DATACITE_SANDBOX_REPOSITORY_ID</code> - <code>DATACITE_SANDBOX_USERNAME</code> - <code>DATACITE_SANDBOX_PASSWORD</code> - <code>ZENODO_SANDBOX_TOKEN</code></p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#manual-tests","title":"Manual Tests","text":"<ul> <li> Test with production DataCite (if available)</li> <li> Test with production Zenodo</li> <li> Test large file upload to Zenodo (&gt; 100 MB)</li> <li> Test error scenarios (bad credentials, network failure)</li> </ul>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#dependencies-prerequisites","title":"Dependencies &amp; Prerequisites","text":""},{"location":"ROADMAP_v0.3.0_MILESTONE1/#required-accounts","title":"Required Accounts","text":"<ol> <li>DataCite Sandbox (free):</li> <li>Sign up at https://support.datacite.org/docs/testing-guide</li> <li> <p>Get repository ID, username, password</p> </li> <li> <p>Zenodo Sandbox (free):</p> </li> <li>Sign up at https://sandbox.zenodo.org</li> <li> <p>Generate API token</p> </li> <li> <p>DataCite Production (institutional):</p> </li> <li>Requires institutional membership</li> <li> <p>Contact library/IT for credentials</p> </li> <li> <p>Zenodo Production (free):</p> </li> <li>Sign up at https://zenodo.org</li> <li>Generate API token</li> </ol>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#external-libraries","title":"External Libraries","text":"<ul> <li><code>net/http</code> - HTTP client (standard library)</li> <li><code>encoding/json</code> - JSON handling (standard library)</li> <li>Consider: <code>hashicorp/go-retryablehttp</code> for retry logic</li> </ul>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#documentation-references","title":"Documentation References","text":"<ul> <li>DataCite REST API: https://support.datacite.org/docs/api</li> <li>DataCite Metadata Schema v4.5: https://schema.datacite.org/meta/kernel-4.5/</li> <li>Zenodo API: https://developers.zenodo.org/</li> <li>Zenodo Depositions: https://developers.zenodo.org/#depositions</li> </ul>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#success-criteria","title":"Success Criteria","text":""},{"location":"ROADMAP_v0.3.0_MILESTONE1/#functional","title":"Functional","text":"<ul> <li> Can create draft DOI in DataCite sandbox</li> <li> Can publish DOI in DataCite sandbox</li> <li> Can create deposition in Zenodo sandbox</li> <li> Can upload files to Zenodo sandbox</li> <li> Can publish DOI in Zenodo sandbox</li> <li> Can retrieve DOI status from both providers</li> <li> Can list DOIs from both providers</li> </ul>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#quality","title":"Quality","text":"<ul> <li> Unit test coverage &gt; 80%</li> <li> All integration tests pass</li> <li> Error messages are helpful</li> <li> Documentation is complete</li> </ul>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#user-experience","title":"User Experience","text":"<ul> <li> Setup takes &lt; 10 minutes with documentation</li> <li> Publishing DOI takes &lt; 30 seconds (excluding upload time)</li> <li> Errors provide clear next steps</li> <li> Progress is visible during long operations</li> </ul>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#risk-mitigation","title":"Risk Mitigation","text":""},{"location":"ROADMAP_v0.3.0_MILESTONE1/#risk-1-api-changes","title":"Risk 1: API Changes","text":"<p>Risk: Provider APIs might change during development Probability: LOW Impact: MEDIUM Mitigation: - Use API versioning in URLs - Monitor provider changelog - Integration tests catch breaking changes</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#risk-2-authentication-issues","title":"Risk 2: Authentication Issues","text":"<p>Risk: Authentication might be more complex than expected Probability: MEDIUM Impact: MEDIUM Mitigation: - Start with sandbox environment - Comprehensive error messages - Fallback to manual token entry</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#risk-3-file-upload-performance","title":"Risk 3: File Upload Performance","text":"<p>Risk: Large file uploads to Zenodo might be slow/unreliable Probability: MEDIUM Impact: LOW Mitigation: - Implement progress tracking - Add resume capability - Document file size limits</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#risk-4-rate-limiting","title":"Risk 4: Rate Limiting","text":"<p>Risk: APIs might have rate limits we hit during testing Probability: LOW Impact: LOW Mitigation: - Implement backoff logic - Cache API responses where possible - Document rate limits</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#timeline-summary","title":"Timeline Summary","text":"Week Phase Key Tasks Deliverables 1 Foundation Config, DataCite client, Zenodo client API clients with unit tests 2 Integration Provider registry, metadata mapping Working provider implementations 3 CLI Publish, status, list commands CLI commands with integration 4 Polish Error handling, integration tests, docs Production-ready milestone <p>Total Duration: 4 weeks Buffer: None (aggressive timeline) Contingency: Week 5 if needed</p>"},{"location":"ROADMAP_v0.3.0_MILESTONE1/#next-steps","title":"Next Steps","text":"<ol> <li>Set up sandbox accounts (Day 1)</li> <li>Implement Task 1.1 (Provider configuration) (Days 1-2)</li> <li>Implement Task 1.2 (DataCite client) (Days 3-5)</li> <li>Implement Task 1.3 (Zenodo client) (Days 6-8)</li> <li>Continue with Phase 2 (Week 2)</li> </ol> <p>Decision Point: End of Week 2 - Evaluate progress and adjust timeline if needed.</p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/","title":"Cicada v0.3.0 Summary","text":"<p>Target Release: Q2 2025 (End of May 2025) Current Status: v0.2.0 Released (January 23, 2025) Planning Complete: \u2705 Ready to start development</p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#quick-overview","title":"Quick Overview","text":"<p>v0.3.0 transforms Cicada from a metadata preparation tool into a complete DOI registration platform with live provider integration and expanded file format support.</p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#core-features","title":"Core Features","text":"Feature Description Priority Live DOI Minting DataCite &amp; Zenodo API integration \u2b50\u2b50\u2b50 CRITICAL Microscopy Support CZI &amp; OME-TIFF metadata extraction \u2b50\u2b50 HIGH Custom Presets User-defined validation rules \u2b50\u2b50 HIGH Interactive Editing TUI for metadata review \u2b50 MEDIUM Bioinformatics BAM/SAM &amp; VCF support \u2b50 MEDIUM"},{"location":"ROADMAP_v0.3.0_SUMMARY/#whats-new-in-v030","title":"What's New in v0.3.0","text":""},{"location":"ROADMAP_v0.3.0_SUMMARY/#1-complete-doi-workflow","title":"1. Complete DOI Workflow","text":"<p>Before (v0.2.0): <pre><code># Prepare metadata\ncicada doi prepare sample.fastq --enrich metadata.yaml --output doi-ready.json\n# \u274c Stop here - no way to actually mint DOI\n</code></pre></p> <p>After (v0.3.0): <pre><code># Prepare and publish in one step\ncicada doi publish sample.fastq --enrich metadata.yaml --provider zenodo\n# \u2705 DOI: 10.5281/zenodo.123456\n# \u2705 URL: https://zenodo.org/record/123456\n</code></pre></p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#2-microscopy-file-support","title":"2. Microscopy File Support","text":"<p>Before (v0.2.0): <pre><code>cicada metadata extract image.czi\n# \u274c Error: Unsupported format\n</code></pre></p> <p>After (v0.3.0): <pre><code>cicada metadata extract image.czi --preset zeiss-lsm-880\n# \u2705 Extracted: 5 channels, 20 Z-slices, 1024x1024 px\n# \u2705 Quality Score: 85/100\n</code></pre></p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#3-custom-presets","title":"3. Custom Presets","text":"<p>Before (v0.2.0): <pre><code># Only 8 built-in presets\ncicada metadata preset list\n# \u274c Can't create lab-specific presets\n</code></pre></p> <p>After (v0.3.0): <pre><code># Create custom preset\ncicada metadata preset create lab-rnaseq \\\n  --template generic-sequencing \\\n  --add-required pipeline_version\n\n# Share with team\ncicada metadata preset export lab-rnaseq &gt; lab-preset.yaml\n</code></pre></p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#4-interactive-metadata-editing","title":"4. Interactive Metadata Editing","text":"<p>Before (v0.2.0): <pre><code># Manual YAML editing\nvim metadata.yaml  # Edit in text editor\ncicada doi prepare sample.fastq --enrich metadata.yaml\n</code></pre></p> <p>After (v0.3.0): <pre><code># Interactive TUI\ncicada metadata edit sample.fastq\n# \u2705 Visual form with validation\n# \u2705 Real-time quality score\n# \u2705 Field help and examples\n</code></pre></p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#timeline","title":"Timeline","text":""},{"location":"ROADMAP_v0.3.0_SUMMARY/#conservative-5-6-months-may-2025","title":"Conservative (5-6 months \u2192 May 2025)","text":"<pre><code>Jan 2025    Feb 2025      Mar 2025       Apr 2025       May 2025\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nv0.2.0      Provider      Microscopy     Custom         Interactive\nReleased    Integration   Formats        Presets        Editing\n            (4 weeks)     (3 weeks)      (2 weeks)      (3 weeks)\n\n                          Bioinformatics  Production\n                          Formats         Readiness\n                          (2 weeks)       (2 weeks)\n\n                                          v0.3.0\n                                          Release\n</code></pre>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#aggressive-3-4-months-april-2025","title":"Aggressive (3-4 months \u2192 April 2025)","text":"<p>Focus on critical features only:</p> <pre><code>Jan 2025    Feb 2025      Mar 2025       Apr 2025\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nv0.2.0      Provider      Microscopy     Production\nReleased    Integration   Formats +      Readiness\n            (3 weeks)     Custom         (1.5 weeks)\n                          Presets\n                          (3.5 weeks)    v0.3.0\n                                         Release\n</code></pre> <p>Defer interactive editing and bioinformatics formats to v0.3.1 or v0.4.0</p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#development-phases","title":"Development Phases","text":""},{"location":"ROADMAP_v0.3.0_SUMMARY/#phase-1-provider-integration-3-4-weeks","title":"Phase 1: Provider Integration (3-4 weeks)","text":"<p>Start: Immediately after v0.2.0 Priority: CRITICAL</p> <ul> <li>DataCite API client (sandbox + production)</li> <li>Zenodo API client (sandbox + production)</li> <li>Configuration management</li> <li>CLI commands: <code>doi publish</code>, <code>doi status</code>, <code>doi list</code></li> <li>Integration tests with sandbox APIs</li> <li>Provider setup documentation</li> </ul> <p>Deliverable: Can mint real DOIs from command line</p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#phase-2-microscopy-support-2-3-weeks","title":"Phase 2: Microscopy Support (2-3 weeks)","text":"<p>Start: After Phase 1 Priority: HIGH</p> <ul> <li>CZI metadata extraction</li> <li>OME-TIFF metadata extraction</li> <li>Integration with Zeiss presets</li> <li>Performance optimization</li> <li>Microscopy documentation</li> </ul> <p>Deliverable: Can extract metadata from microscopy files</p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#phase-3-custom-presets-2-weeks","title":"Phase 3: Custom Presets (2 weeks)","text":"<p>Start: Parallel with Phase 2 Priority: HIGH</p> <ul> <li>Preset creation/editing</li> <li>YAML-based preset format</li> <li>Import/export</li> <li>Preset validation</li> <li>Custom preset documentation</li> </ul> <p>Deliverable: Users can create lab-specific presets</p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#phase-4-production-polish-2-weeks","title":"Phase 4: Production Polish (2 weeks)","text":"<p>Start: After Phases 1-3 Priority: MEDIUM</p> <ul> <li>Metadata caching</li> <li>Error recovery</li> <li>Logging and monitoring</li> <li>Performance optimization</li> <li>Production deployment guide</li> </ul> <p>Deliverable: Production-ready release</p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#immediate-next-steps-week-1","title":"Immediate Next Steps (Week 1)","text":""},{"location":"ROADMAP_v0.3.0_SUMMARY/#day-1-setup","title":"Day 1: Setup","text":"<ul> <li> Create v0.3.0 branch: <code>git checkout -b feature/v0.3.0-provider-integration</code></li> <li> Set up sandbox accounts:</li> <li> DataCite sandbox: https://support.datacite.org/docs/testing-guide</li> <li> Zenodo sandbox: https://sandbox.zenodo.org</li> <li> Store credentials in environment variables</li> </ul>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#days-1-2-provider-configuration","title":"Days 1-2: Provider Configuration","text":"<ul> <li> Implement <code>internal/doi/provider_config.go</code></li> <li> Add config commands for provider credentials</li> <li> Add environment variable support</li> <li> Write unit tests</li> </ul>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#days-3-5-datacite-client","title":"Days 3-5: DataCite Client","text":"<ul> <li> Implement <code>internal/doi/datacite_client.go</code></li> <li> Implement authentication</li> <li> Implement CRUD operations</li> <li> Write unit tests with mocked HTTP</li> </ul>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#days-6-8-zenodo-client","title":"Days 6-8: Zenodo Client","text":"<ul> <li> Implement <code>internal/doi/zenodo_client.go</code></li> <li> Implement authentication</li> <li> Implement deposition + file upload</li> <li> Write unit tests with mocked HTTP</li> </ul> <p>Week 1 Goal: Have working API clients with unit tests</p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#success-criteria","title":"Success Criteria","text":""},{"location":"ROADMAP_v0.3.0_SUMMARY/#v030-launch-criteria","title":"v0.3.0 Launch Criteria","text":"<p>Must have ALL of these:</p> <ul> <li> \u2705 Provider integration complete (DataCite + Zenodo)</li> <li> \u23f3 Can mint DOIs in sandbox</li> <li> \u23f3 Can mint DOIs in production</li> <li> \u23f3 Microscopy formats supported (CZI + OME-TIFF)</li> <li> \u23f3 Custom presets work</li> <li> \u23f3 All integration tests pass</li> <li> \u23f3 Documentation complete</li> <li> \u23f3 80%+ test coverage on new code</li> </ul>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#post-launch-success-3-months","title":"Post-Launch Success (3 months)","text":"<ul> <li> 50+ users adopt v0.3.0</li> <li> 100+ DOIs minted through Cicada</li> <li> 10+ custom presets created by users</li> <li> User satisfaction &gt; \u2158</li> </ul>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#key-decisions","title":"Key Decisions","text":""},{"location":"ROADMAP_v0.3.0_SUMMARY/#decision-1-provider-priority","title":"Decision 1: Provider Priority","text":"<p>Question: DataCite or Zenodo first? Decision: Zenodo Rationale: - Easier authentication (token vs username/password) - Integrated file storage (no separate upload needed) - Free for all users (DataCite requires institutional membership) - Better for target users (small labs)</p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#decision-2-timeline","title":"Decision 2: Timeline","text":"<p>Question: Aggressive (3-4 months) or conservative (5-6 months)? Decision: Start conservative, adjust based on progress Rationale: - First provider integration - unknowns expected - Can accelerate if provider integration goes smoothly - Better to under-promise and over-deliver</p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#decision-3-scope","title":"Decision 3: Scope","text":"<p>Question: Include all features or defer some? Decision: Core features in v0.3.0, others in v0.3.1 Rationale: - Priority 1: Provider integration (must have) - Priority 2: Microscopy + custom presets (should have) - Priority 3: Interactive editing + bioinformatics (nice to have) - Can release v0.3.1 quickly after v0.3.0 with remaining features</p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#decision-4-tui-library","title":"Decision 4: TUI Library","text":"<p>Question: Which library for interactive editing? Decision: bubbletea Rationale: - Modern, actively maintained - Powerful and flexible - Good documentation and examples - Used by successful projects (glow, soft-serve)</p>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#resources-needed","title":"Resources Needed","text":""},{"location":"ROADMAP_v0.3.0_SUMMARY/#human-resources","title":"Human Resources","text":"<ul> <li>1 developer (full-time) for 3-4 months</li> <li>OR 2 developers (part-time) for 2-3 months</li> </ul>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#external-resources","title":"External Resources","text":"<ul> <li>Sandbox accounts (free)</li> <li>Production accounts (Zenodo free, DataCite requires institutional membership)</li> <li>Test data:</li> <li>CZI files from Zeiss microscopes</li> <li>OME-TIFF files</li> <li>BAM/SAM files</li> <li>VCF files</li> </ul>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#infrastructure","title":"Infrastructure","text":"<ul> <li>GitHub repository (existing)</li> <li>CI/CD (GitHub Actions - existing)</li> <li>Test environment for provider integration</li> </ul>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#documentation-plan","title":"Documentation Plan","text":""},{"location":"ROADMAP_v0.3.0_SUMMARY/#new-documentation","title":"New Documentation","text":"<ol> <li><code>docs/DOI_PUBLISHING.md</code> - End-to-end publishing guide</li> <li><code>docs/MICROSCOPY_FORMATS.md</code> - CZI and OME-TIFF guide</li> <li><code>docs/CUSTOM_PRESETS.md</code> - Creating custom presets</li> <li><code>docs/INTERACTIVE_EDITING.md</code> - Using the TUI</li> </ol>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#updated-documentation","title":"Updated Documentation","text":"<ul> <li><code>docs/PROVIDERS.md</code> - Complete provider setup</li> <li><code>docs/USER_SCENARIOS_v0.3.0.md</code> - New scenarios</li> <li><code>README.md</code> - Add v0.3.0 features</li> <li><code>CHANGELOG.md</code> - Track changes</li> </ul>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#risk-assessment","title":"Risk Assessment","text":"Risk Probability Impact Mitigation API changes LOW MEDIUM Version APIs, monitor changelog Auth complexity MEDIUM MEDIUM Start sandbox, clear docs Binary parsing MEDIUM MEDIUM Use libraries, limit scope Timeline slip MEDIUM LOW Conservative estimates, prioritize"},{"location":"ROADMAP_v0.3.0_SUMMARY/#questions-to-resolve","title":"Questions to Resolve","text":"<ol> <li>DataCite Production Access:</li> <li>Do we have institutional membership?</li> <li>Can we get test credentials?</li> <li> <p>Action: Contact library/IT</p> </li> <li> <p>Test Data:</p> </li> <li>Can we get real CZI files?</li> <li>Can we get real BAM/VCF files?</li> <li> <p>Action: Ask lab collaborators</p> </li> <li> <p>Feature Priority:</p> </li> <li>Which features are most important to users?</li> <li> <p>Action: User survey or interviews</p> </li> <li> <p>Timeline Constraints:</p> </li> <li>Any deadlines or dependencies?</li> <li>Action: Confirm with stakeholders</li> </ol>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#communication-plan","title":"Communication Plan","text":""},{"location":"ROADMAP_v0.3.0_SUMMARY/#weekly-updates","title":"Weekly Updates","text":"<ul> <li> Weekly progress report (Friday)</li> <li> Milestone completion announcements</li> <li> Risk/issue escalation as needed</li> </ul>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#milestone-demos","title":"Milestone Demos","text":"<ul> <li> End of Phase 1: Demo DOI minting</li> <li> End of Phase 2: Demo microscopy extraction</li> <li> End of Phase 3: Demo custom presets</li> <li> Pre-release: Full demo and walkthrough</li> </ul>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#getting-started","title":"Getting Started","text":""},{"location":"ROADMAP_v0.3.0_SUMMARY/#for-developers","title":"For Developers","text":"<ol> <li>Read Planning Docs:</li> <li><code>docs/ROADMAP_v0.3.0.md</code> (full roadmap)</li> <li> <p><code>docs/ROADMAP_v0.3.0_MILESTONE1.md</code> (detailed Phase 1 tasks)</p> </li> <li> <p>Set Up Environment:</p> </li> <li>Create sandbox accounts</li> <li>Set environment variables</li> <li> <p>Review DataCite/Zenodo API docs</p> </li> <li> <p>Start Development:</p> </li> <li>Create feature branch</li> <li>Begin Task 1.1 (Provider configuration)</li> </ol>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#for-stakeholders","title":"For Stakeholders","text":"<ol> <li>Review Roadmap: Understand scope and timeline</li> <li>Provide Feedback: Any concerns or suggestions?</li> <li>Commit Resources: Approve time/budget</li> <li>Set Expectations: Conservative timeline, incremental delivery</li> </ol>"},{"location":"ROADMAP_v0.3.0_SUMMARY/#conclusion","title":"Conclusion","text":"<p>v0.3.0 is well-planned and ready to start. The roadmap is:</p> <ul> <li>Realistic: Conservative timeline with buffer</li> <li>Focused: Core features prioritized</li> <li>Achievable: Clear tasks with defined deliverables</li> <li>Valuable: Completes DOI workflow for users</li> </ul> <p>Next Step: Begin Milestone 1 development (Provider Integration)</p> <p>Created: January 23, 2025 Status: Planning Complete \u2705 Next Review: End of Week 2 (evaluate progress)</p>"},{"location":"USER_SCENARIOS/","title":"Cicada User Scenarios","text":"<p>Cicada is a dormant data commons platform for academic research labs, providing federated storage, access control, compute-to-data capabilities, and collaboration primitives. Like a cicada, it lies dormant (consuming minimal resources) until needed, then emerges powerfully for data-intensive work.</p> <p>This document provides persona-based walkthroughs for v0.1.0, which implements the foundational storage and sync layer. These scenarios demonstrate how researchers can leverage Cicada's core capabilities to automate instrument data uploads, backup research data, integrate with analysis pipelines, and ensure data governance compliance.</p>"},{"location":"USER_SCENARIOS/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Lab Researcher: Instrument Data Upload</li> <li>PhD Student: Research Data Backup</li> <li>Bioinformatician: Analysis Pipeline</li> <li>Lab Manager: Data Governance</li> </ol>"},{"location":"USER_SCENARIOS/#scenario-1-lab-researcher-instrument-data-upload","title":"Scenario 1: Lab Researcher - Instrument Data Upload","text":""},{"location":"USER_SCENARIOS/#persona-dr-maria-rodriguez","title":"Persona: Dr. Maria Rodriguez","text":"<p>Background: - Postdoctoral researcher in neuroscience - Uses a Zeiss confocal microscope that writes ~50GB/day of imaging data - Lab policy: All raw data must be in S3 within 24 hours - Technical level: Comfortable with command line basics - Works at the microscope workstation (Ubuntu Linux)</p> <p>Pain Points: - Manually copying files to S3 is tedious and error-prone - Forgot to upload data twice \u2192 got reminders from PI - Large file transfers sometimes fail halfway through - Needs to verify all files uploaded correctly</p> <p>Goals: - Automatic upload of new microscope files - Reliable transfers that handle network interruptions - Confirmation that files are safely in S3</p>"},{"location":"USER_SCENARIOS/#version-info","title":"Version Info","text":"<ul> <li>\u2705 v0.1.0 (Current): All features shown are available today</li> </ul>"},{"location":"USER_SCENARIOS/#day-1-initial-setup-5-minutes","title":"Day 1: Initial Setup (5 minutes)","text":"<p>Step 1: Install Cicada</p> <p>Maria installs Cicada on the microscope workstation:</p> <pre><code># Download from GitHub releases\nwget https://github.com/scttfrdmn/cicada/releases/download/v0.1.0/cicada_0.1.0_Linux_x86_64.tar.gz\n\n# Extract and install\ntar -xzf cicada_0.1.0_Linux_x86_64.tar.gz\nsudo mv cicada /usr/local/bin/\n\n# Verify installation\ncicada version\n</code></pre> <p>Output: <pre><code>cicada version 0.1.0 (commit: f6a1f79, built: 2025-11-23T21:36:10Z, by: goreleaser)\n  Go version: go1.23.4\n  OS/Arch: linux/amd64\n</code></pre></p> <p>What Maria thinks: \"Good, it's installed. Now I need to set up AWS access.\"</p> <p>Step 2: Configure AWS Credentials</p> <p>The lab already has an AWS profile configured on the workstation:</p> <pre><code># Verify AWS credentials work\naws s3 ls s3://rodriguez-lab-data/\n</code></pre> <p>Output: <pre><code>                           PRE microscopy/\n                           PRE sequencing/\n                           PRE analysis/\n</code></pre></p> <p>What Maria thinks: \"Perfect, AWS access is working. Now to configure Cicada.\"</p> <p>Step 3: Initialize Cicada Configuration</p> <pre><code># Create default config\ncicada config init\n</code></pre> <p>Output: <pre><code>\u2713 Created configuration directory: /home/maria/.cicada\n\u2713 Created configuration file: /home/maria/.cicada/config.yaml\n\nDefault configuration created. Customize with:\n  cicada config set &lt;key&gt; &lt;value&gt;\n</code></pre></p> <p>View the created config: <pre><code>cat ~/.cicada/config.yaml\n</code></pre></p> <p>Generated config: <pre><code>version: \"1\"\n\naws:\n  profile: default\n  region: us-west-2\n\nsync:\n  concurrency: 4\n  delete: false\n  exclude:\n    - .git/**\n    - .DS_Store\n    - \"*.tmp\"\n    - \"*.swp\"\n\nwatches: []\n\nsettings:\n  verbose: false\n  check_updates: true\n</code></pre></p> <p>What Maria thinks: \"This looks reasonable. I should update the AWS profile to match our lab's setup.\"</p> <p>Step 4: Configure for Lab Environment</p> <pre><code># Set the lab's AWS profile\ncicada config set aws.profile rodriguez-lab\n\n# Set the correct region\ncicada config set aws.region us-west-2\n\n# Verify configuration\ncicada config list\n</code></pre> <p>Output: <pre><code>version: \"1\"\n\naws:\n  profile: rodriguez-lab\n  region: us-west-2\n\nsync:\n  concurrency: 4\n  delete: false\n  exclude:\n    - .git/**\n    - .DS_Store\n    - \"*.tmp\"\n    - \"*.swp\"\n\nwatches: []\n\nsettings:\n  verbose: false\n  check_updates: true\n</code></pre></p> <p>What Maria thinks: \"Great! Now I'm ready to test uploading some files.\"</p>"},{"location":"USER_SCENARIOS/#day-1-testing-manual-sync-2-minutes","title":"Day 1: Testing Manual Sync (2 minutes)","text":"<p>Step 5: Test Manual Upload</p> <p>Maria has a completed imaging session from yesterday that needs uploading:</p> <pre><code># Check what files are in the directory\nls -lh /mnt/zeiss/2025-11-22_neurons/\n\n# Dry run first to see what would be uploaded\ncicada sync --dry-run \\\n  /mnt/zeiss/2025-11-22_neurons \\\n  s3://rodriguez-lab-data/microscopy/2025-11-22_neurons\n</code></pre> <p>Output: <pre><code>Syncing from /mnt/zeiss/2025-11-22_neurons to s3://rodriguez-lab-data/microscopy/2025-11-22_neurons\n\nScanning source files...\nFound 127 files (48.3 GB)\n\nScanning destination...\nFound 0 files\n\nPlan:\n  Upload: 127 files (48.3 GB)\n  Delete: 0 files\n  Skip: 0 files (already synced)\n\nDRY RUN - no changes made\n</code></pre></p> <p>What Maria thinks: \"Perfect! It found all 127 files. Now let me do the actual upload.\"</p> <p>Step 6: Perform Actual Upload</p> <pre><code># Real upload (remove --dry-run)\ncicada sync \\\n  /mnt/zeiss/2025-11-22_neurons \\\n  s3://rodriguez-lab-data/microscopy/2025-11-22_neurons\n</code></pre> <p>Output (abbreviated): <pre><code>Syncing from /mnt/zeiss/2025-11-22_neurons to s3://rodriguez-lab-data/microscopy/2025-11-22_neurons\n\nScanning source files...\nFound 127 files (48.3 GB)\n\nScanning destination...\nFound 0 files\n\nUploading files... (4 concurrent transfers)\n\n\u2713 image_001.czi (387 MB) - 12s\n\u2713 image_002.czi (391 MB) - 11s\n\u2713 image_003.czi (385 MB) - 12s\n\u2713 image_004.czi (389 MB) - 12s\n...\n\u2713 metadata.xml (42 KB) - 0s\n\nCompleted: 127 files uploaded (48.3 GB) in 18m 32s\n</code></pre></p> <p>What Maria thinks: \"Excellent! 48 GB uploaded in under 20 minutes. Much faster than my manual copies. Now let me set up automatic watching.\"</p>"},{"location":"USER_SCENARIOS/#day-1-setting-up-automatic-watching-3-minutes","title":"Day 1: Setting Up Automatic Watching (3 minutes)","text":"<p>Step 7: Configure Watch for Microscope Output</p> <p>The microscope writes files to <code>/mnt/zeiss/output/</code> as imaging sessions complete.</p> <pre><code># Add a watch with appropriate delays\ncicada watch add \\\n  --debounce 30 \\\n  --min-age 60 \\\n  /mnt/zeiss/output \\\n  s3://rodriguez-lab-data/microscopy/live\n</code></pre> <p>Parameters explained: - <code>--debounce 30</code>: Wait 30 seconds after last file change before syncing (handles burst writes) - <code>--min-age 60</code>: Only sync files older than 60 seconds (prevents syncing incomplete files)</p> <p>Output: <pre><code>\u2713 Watch created: /mnt/zeiss/output-1732397890\n\nConfiguration:\n  Source: /mnt/zeiss/output\n  Destination: s3://rodriguez-lab-data/microscopy/live\n  Debounce: 30s\n  Min-age: 60s\n  Delete source: false\n\nPerforming initial sync...\nFound 3 files (1.2 GB)\n\u2713 Synced 3 files in 2m 15s\n\nWatch is now active. Files will be automatically synced to S3.\nPress Ctrl+C to stop, or let it run in the background.\n</code></pre></p> <p>What Maria thinks: \"Perfect! Now any new files will automatically upload. Let me verify it's working.\"</p> <p>Step 8: Verify Watch is Active</p> <pre><code># List all active watches\ncicada watch list\n</code></pre> <p>Output: <pre><code>Active watches:\n\nID: /mnt/zeiss/output-1732397890\n  Source: /mnt/zeiss/output\n  Destination: s3://rodriguez-lab-data/microscopy/live\n  Status: Running\n  Debounce: 30s\n  Min-age: 60s\n  Last sync: 2 minutes ago (3 files, 1.2 GB)\n  Total syncs: 1\n</code></pre></p> <p>What Maria thinks: \"Great! It's running and already did the initial sync. Now let me test it with a new file.\"</p>"},{"location":"USER_SCENARIOS/#day-1-testing-automatic-sync-5-minutes","title":"Day 1: Testing Automatic Sync (5 minutes)","text":"<p>Step 9: Test with New File</p> <p>Maria copies a test file to simulate the microscope writing data:</p> <pre><code># Copy a test file to the watched directory\ncp /home/maria/test-image.czi /mnt/zeiss/output/\n\n# Wait 90 seconds (min-age 60s + debounce 30s)\n# ... Maria gets a coffee ...\n\n# Check S3 to see if file appeared\naws s3 ls s3://rodriguez-lab-data/microscopy/live/\n</code></pre> <p>Output: <pre><code>2025-11-23 14:23:15  387234561 image_001.czi\n2025-11-23 14:23:18  391847234 image_002.czi\n2025-11-23 14:23:21  385123456 image_003.czi\n2025-11-23 14:28:42  389456789 test-image.czi  \u2190 New file!\n</code></pre></p> <p>What Maria thinks: \"Success! The file was automatically uploaded. This is going to save me so much time.\"</p> <p>Step 10: Check Watch Status Again</p> <pre><code>cicada watch list\n</code></pre> <p>Output: <pre><code>Active watches:\n\nID: /mnt/zeiss/output-1732397890\n  Source: /mnt/zeiss/output\n  Destination: s3://rodriguez-lab-data/microscopy/live\n  Status: Running\n  Debounce: 30s\n  Min-age: 60s\n  Last sync: 1 minute ago (1 file, 389 MB)\n  Total syncs: 2\n</code></pre></p> <p>What Maria thinks: \"Perfect! It tracked the sync. Now I should make sure this starts automatically when the computer reboots.\"</p>"},{"location":"USER_SCENARIOS/#day-1-making-watch-persistent-2-minutes","title":"Day 1: Making Watch Persistent (2 minutes)","text":"<p>Step 11: Verify Configuration Persistence</p> <p>Cicada automatically saves watch configuration:</p> <pre><code># Check the saved configuration\ncat ~/.cicada/config.yaml\n</code></pre> <p>Updated config (watches section): <pre><code>version: \"1\"\n\naws:\n  profile: rodriguez-lab\n  region: us-west-2\n\nsync:\n  concurrency: 4\n  delete: false\n  exclude:\n    - .git/**\n    - .DS_Store\n    - \"*.tmp\"\n    - \"*.swp\"\n\nwatches:\n  - id: /mnt/zeiss/output-1732397890\n    source: /mnt/zeiss/output\n    destination: s3://rodriguez-lab-data/microscopy/live\n    debounce: 30\n    min_age: 60\n    delete_source: false\n    sync_on_start: true\n    enabled: true\n\nsettings:\n  verbose: false\n  check_updates: true\n</code></pre></p> <p>What Maria thinks: \"Good, it's saved in the config. Now I need to set up a systemd service so it starts on boot.\"</p> <p>Step 12: Create Systemd Service</p> <p>Maria creates a systemd service to run Cicada watches automatically:</p> <pre><code># Create service file\nsudo nano /etc/systemd/system/cicada-watch.service\n</code></pre> <p>Service file content: <pre><code>[Unit]\nDescription=Cicada File Watcher\nAfter=network.target\n\n[Service]\nType=simple\nUser=maria\nWorkingDirectory=/home/maria\nExecStart=/usr/local/bin/cicada watch start\nRestart=always\nRestartSec=10\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> <p>Enable and start the service: <pre><code># Reload systemd\nsudo systemctl daemon-reload\n\n# Enable service to start on boot\nsudo systemctl enable cicada-watch\n\n# Start the service now\nsudo systemctl start cicada-watch\n\n# Check status\nsudo systemctl status cicada-watch\n</code></pre></p> <p>Output: <pre><code>\u25cf cicada-watch.service - Cicada File Watcher\n   Loaded: loaded (/etc/systemd/system/cicada-watch.service; enabled)\n   Active: active (running) since Sat 2025-11-23 14:35:12 PST; 5s ago\n Main PID: 12345 (cicada)\n   CGroup: /system.slice/cicada-watch.service\n           \u2514\u250012345 /usr/local/bin/cicada watch start\n\nNov 23 14:35:12 zeiss-workstation systemd[1]: Started Cicada File Watcher.\nNov 23 14:35:13 zeiss-workstation cicada[12345]: \u2713 Watch loaded: /mnt/zeiss/output\nNov 23 14:35:13 zeiss-workstation cicada[12345]: Watching for file changes...\n</code></pre></p> <p>What Maria thinks: \"Perfect! Now it will automatically start watching whenever the computer boots. Setup complete!\"</p>"},{"location":"USER_SCENARIOS/#week-1-daily-usage","title":"Week 1: Daily Usage","text":"<p>Maria's typical workflow now:</p> <ol> <li> <p>Morning: Arrives at lab, checks that Cicada is running    <pre><code>sudo systemctl status cicada-watch\ncicada watch list\n</code></pre></p> </li> <li> <p>During imaging: Works normally at the microscope</p> </li> <li>Files automatically upload as sessions complete</li> <li> <p>No manual intervention needed</p> </li> <li> <p>End of day: Verifies all data is in S3    <pre><code># Check S3 to confirm files are there\naws s3 ls s3://rodriguez-lab-data/microscopy/live/ --recursive\n</code></pre></p> </li> <li> <p>Weekly: Reviews sync statistics    <pre><code>cicada watch list\n</code></pre> Example output:    <pre><code>ID: /mnt/zeiss/output-1732397890\n  Status: Running\n  Last sync: 15 minutes ago (8 files, 3.2 GB)\n  Total syncs: 47\n  Total data: 241 GB\n</code></pre></p> </li> </ol> <p>What Maria experiences: \"I barely think about data uploads anymore. I just check once a day that Cicada is running, and all my microscope data is automatically backed up to S3. This has saved me hours of work every week.\"</p>"},{"location":"USER_SCENARIOS/#key-benefits-for-maria","title":"Key Benefits for Maria","text":"<p>\u2705 Time savings: No manual uploads (saves ~2 hours/week) \u2705 Reliability: Never forgets to upload data \u2705 Peace of mind: Automatic verification via watch status \u2705 Fast transfers: Concurrent uploads maximize bandwidth \u2705 Simple setup: 15-minute one-time configuration</p>"},{"location":"USER_SCENARIOS/#scenario-2-phd-student-research-data-backup","title":"Scenario 2: PhD Student - Research Data Backup","text":""},{"location":"USER_SCENARIOS/#persona-alex-thompson","title":"Persona: Alex Thompson","text":"<p>Background: - 3<sup>rd</sup> year PhD student in bioinformatics - Works on RNA-seq analysis with ~500GB of data - University provides S3 storage for research data - Technical level: Proficient programmer, uses command line daily - Works from laptop (macOS) and university HPC cluster</p> <p>Pain Points: - Lost a week of analysis when laptop hard drive failed - Manually copying files to S3 is tedious - Not sure if backups are complete or up-to-date - Running low on laptop disk space</p> <p>Goals: - Reliable automated backups to S3 - Verify all important data is backed up - Free up local disk space by moving old data to S3 - Easy access to data from both laptop and HPC</p>"},{"location":"USER_SCENARIOS/#day-1-installing-on-macos-2-minutes","title":"Day 1: Installing on macOS (2 minutes)","text":"<p>Step 1: Install via Homebrew</p> <pre><code># Add the Cicada tap\nbrew install scttfrdmn/tap/cicada\n\n# Verify installation\ncicada version\n</code></pre> <p>Output: <pre><code>cicada version 0.1.0 (commit: f6a1f79, built: 2025-11-23T21:36:10Z, by: goreleaser)\n  Go version: go1.23.4\n  OS/Arch: darwin/arm64\n</code></pre></p> <p>What Alex thinks: \"Nice, Homebrew makes this easy. Now to set up AWS.\"</p> <p>Step 2: Configure AWS Credentials</p> <pre><code># Alex already has AWS CLI configured for university account\naws configure list\n\n# Test S3 access\naws s3 ls s3://alex-phd-research/\n</code></pre> <p>Output: <pre><code>                           PRE rnaseq-analysis/\n                           PRE reference-genomes/\n                           PRE publications/\n</code></pre></p> <p>What Alex thinks: \"Good, S3 access works. Let me initialize Cicada.\"</p> <p>Step 3: Initialize Configuration</p> <pre><code># Create config\ncicada config init\n\n# Set university AWS profile\ncicada config set aws.profile university\n\n# Set region\ncicada config set aws.region us-east-1\n\n# Verify\ncicada config get aws.profile\ncicada config get aws.region\n</code></pre> <p>Output: <pre><code>university\nus-east-1\n</code></pre></p> <p>What Alex thinks: \"Configuration looks good. Now let me back up my current analysis.\"</p>"},{"location":"USER_SCENARIOS/#day-1-first-backup-current-analysis-10-minutes","title":"Day 1: First Backup - Current Analysis (10 minutes)","text":"<p>Step 4: Preview Backup</p> <p>Alex wants to back up the current RNA-seq analysis project:</p> <pre><code># See what will be uploaded (dry run)\ncicada sync --dry-run \\\n  ~/research/rnaseq-analysis \\\n  s3://alex-phd-research/rnaseq-analysis\n</code></pre> <p>Output: <pre><code>Syncing from /Users/alex/research/rnaseq-analysis to s3://alex-phd-research/rnaseq-analysis\n\nScanning source files...\nFound 1,847 files (67.3 GB)\n\nScanning destination...\nFound 1,203 files (45.2 GB)\n\nPlan:\n  Upload: 644 new files (22.1 GB)\n  Update: 0 modified files\n  Delete: 0 files\n  Skip: 1,203 files (already synced)\n\nDRY RUN - no changes made\n</code></pre></p> <p>What Alex thinks: \"Good! It recognizes that some files are already there and only needs to upload 644 new ones. Let me do the real sync.\"</p> <p>Step 5: Perform Backup</p> <pre><code># Real backup\ncicada sync \\\n  ~/research/rnaseq-analysis \\\n  s3://alex-phd-research/rnaseq-analysis\n</code></pre> <p>Output: <pre><code>Syncing from /Users/alex/research/rnaseq-analysis to s3://alex-phd-research/rnaseq-analysis\n\nScanning source files...\nFound 1,847 files (67.3 GB)\n\nScanning destination...\nFound 1,203 files (45.2 GB)\n\nUploading files... (4 concurrent transfers)\n\n\u2713 results/DE_analysis.csv (2.3 MB) - 1s\n\u2713 results/counts_matrix.tsv (45 MB) - 3s\n\u2713 plots/volcano_plot.png (387 KB) - 0s\n\u2713 data/sample_A_01.fastq.gz (891 MB) - 24s\n...\n\nCompleted: 644 files uploaded (22.1 GB) in 8m 15s\n  Skipped: 1,203 files (already in sync)\n</code></pre></p> <p>What Alex thinks: \"Excellent! Only 8 minutes to upload 22 GB of new data. And it intelligently skipped the files that were already there.\"</p>"},{"location":"USER_SCENARIOS/#day-1-setting-up-continuous-backup-5-minutes","title":"Day 1: Setting Up Continuous Backup (5 minutes)","text":"<p>Step 6: Configure Watch for Active Project</p> <pre><code># Watch the analysis directory\ncicada watch add \\\n  --debounce 60 \\\n  --min-age 120 \\\n  ~/research/rnaseq-analysis \\\n  s3://alex-phd-research/rnaseq-analysis\n</code></pre> <p>Parameters: - <code>--debounce 60</code>: Wait 1 minute after changes (allows for batch edits) - <code>--min-age 120</code>: Only backup files older than 2 minutes (prevents backing up temp files)</p> <p>Output: <pre><code>\u2713 Watch created: /Users/alex/research/rnaseq-analysis-1732398234\n\nConfiguration:\n  Source: /Users/alex/research/rnaseq-analysis\n  Destination: s3://alex-phd-research/rnaseq-analysis\n  Debounce: 60s\n  Min-age: 120s\n\nPerforming initial sync...\nAll files already synced (1,847 files in sync)\n\nWatch is now active. Changes will be automatically backed up to S3.\n</code></pre></p> <p>What Alex thinks: \"Perfect! Now any changes I make will automatically get backed up. But wait, I need this to run even when I close my terminal...\"</p> <p>Step 7: Run Watch in Background</p> <p>Alex uses macOS LaunchAgent to run Cicada in the background:</p> <pre><code># Create LaunchAgent directory if needed\nmkdir -p ~/Library/LaunchAgents\n\n# Create plist file\ncat &gt; ~/Library/LaunchAgents/com.cicada.watch.plist &lt;&lt;'EOF'\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"&gt;\n&lt;plist version=\"1.0\"&gt;\n&lt;dict&gt;\n    &lt;key&gt;Label&lt;/key&gt;\n    &lt;string&gt;com.cicada.watch&lt;/string&gt;\n    &lt;key&gt;ProgramArguments&lt;/key&gt;\n    &lt;array&gt;\n        &lt;string&gt;/opt/homebrew/bin/cicada&lt;/string&gt;\n        &lt;string&gt;watch&lt;/string&gt;\n        &lt;string&gt;start&lt;/string&gt;\n    &lt;/array&gt;\n    &lt;key&gt;RunAtLoad&lt;/key&gt;\n    &lt;true/&gt;\n    &lt;key&gt;KeepAlive&lt;/key&gt;\n    &lt;true/&gt;\n    &lt;key&gt;StandardOutPath&lt;/key&gt;\n    &lt;string&gt;/Users/alex/Library/Logs/cicada-watch.log&lt;/string&gt;\n    &lt;key&gt;StandardErrorPath&lt;/key&gt;\n    &lt;string&gt;/Users/alex/Library/Logs/cicada-watch-error.log&lt;/string&gt;\n&lt;/dict&gt;\n&lt;/plist&gt;\nEOF\n\n# Load the LaunchAgent\nlaunchctl load ~/Library/LaunchAgents/com.cicada.watch.plist\n\n# Verify it's running\nlaunchctl list | grep cicada\n</code></pre> <p>Output: <pre><code>12346   0   com.cicada.watch\n</code></pre></p> <p>What Alex thinks: \"Great! Now Cicada will run in the background and automatically backup my work.\"</p>"},{"location":"USER_SCENARIOS/#week-1-recovering-from-laptop-issue","title":"Week 1: Recovering from Laptop Issue","text":"<p>Day 5: Laptop Problem</p> <p>Alex's laptop runs out of disk space:</p> <pre><code>df -h /Users/alex\n</code></pre> <p>Output: <pre><code>Filesystem      Size   Used  Avail Capacity\n/dev/disk1s1   500GB  485GB   15GB    97%\n</code></pre></p> <p>What Alex thinks: \"I'm almost out of space! Let me move some old data to S3 and delete the local copies.\"</p> <p>Step 8: Archive Old Data</p> <pre><code># Sync old reference genomes to S3\ncicada sync \\\n  ~/research/reference-genomes \\\n  s3://alex-phd-research/reference-genomes\n\n# Verify all files are in S3\naws s3 ls s3://alex-phd-research/reference-genomes/ --recursive --summarize\n</code></pre> <p>Output: <pre><code>...\n2025-11-28 10:23:15  1234567890 hg38.fa\n2025-11-28 10:23:18  2345678901 hg38.fa.fai\n...\n\nTotal Objects: 47\n   Total Size: 45.8 GB\n</code></pre></p> <p>Verify checksums match: <pre><code># Use sync with --dry-run to verify everything matches\ncicada sync --dry-run \\\n  ~/research/reference-genomes \\\n  s3://alex-phd-research/reference-genomes\n</code></pre></p> <p>Output: <pre><code>Syncing from /Users/alex/research/reference-genomes to s3://alex-phd-research/reference-genomes\n\nScanning source files...\nFound 47 files (45.8 GB)\n\nScanning destination...\nFound 47 files (45.8 GB)\n\nPlan:\n  Upload: 0 files\n  Delete: 0 files\n  Skip: 47 files (already synced) \u2190 All files match!\n\nDRY RUN - no changes made\n</code></pre></p> <p>What Alex thinks: \"Perfect! All 47 files are verified to be in S3 with matching checksums. Safe to delete locally.\"</p> <p>Step 9: Free Up Local Disk Space</p> <pre><code># Delete local copies (keeping only S3)\nrm -rf ~/research/reference-genomes\n\n# Check disk space\ndf -h /Users/alex\n</code></pre> <p>Output: <pre><code>Filesystem      Size   Used  Avail Capacity\n/dev/disk1s1   500GB  439GB   61GB    88%\n</code></pre></p> <p>What Alex thinks: \"Freed up 46 GB! And I can always get the files back from S3 when needed.\"</p>"},{"location":"USER_SCENARIOS/#week-2-working-on-hpc-cluster","title":"Week 2: Working on HPC Cluster","text":"<p>Step 10: Access Data from HPC</p> <p>Alex needs to run analysis on the university HPC cluster:</p> <pre><code># SSH to HPC cluster\nssh alex@hpc.university.edu\n\n# Install Cicada on HPC (from downloaded binary)\nwget https://github.com/scttfrdmn/cicada/releases/download/v0.1.0/cicada_0.1.0_Linux_x86_64.tar.gz\ntar -xzf cicada_0.1.0_Linux_x86_64.tar.gz\nmv cicada ~/bin/\n\n# Initialize config\ncicada config init\ncicada config set aws.profile university\ncicada config set aws.region us-east-1\n\n# Download data from S3\ncicada sync \\\n  s3://alex-phd-research/rnaseq-analysis \\\n  ~/scratch/rnaseq-analysis\n</code></pre> <p>Output: <pre><code>Syncing from s3://alex-phd-research/rnaseq-analysis to /home/alex/scratch/rnaseq-analysis\n\nScanning source...\nFound 1,847 files (67.3 GB)\n\nScanning destination...\nFound 0 files\n\nDownloading files... (4 concurrent transfers)\n\n\u2713 data/sample_A_01.fastq.gz (891 MB) - 18s\n\u2713 data/sample_A_02.fastq.gz (867 MB) - 17s\n...\n\nCompleted: 1,847 files downloaded (67.3 GB) in 12m 42s\n</code></pre></p> <p>What Alex thinks: \"Perfect! All my data is now on the HPC cluster. I can run my analysis and then sync the results back to S3.\"</p> <p>Step 11: Sync Results Back to S3</p> <p>After running analysis on HPC:</p> <pre><code># Upload new results back to S3\ncicada sync \\\n  ~/scratch/rnaseq-analysis/results \\\n  s3://alex-phd-research/rnaseq-analysis/results\n</code></pre> <p>Output: <pre><code>Syncing from /home/alex/scratch/rnaseq-analysis/results to s3://alex-phd-research/rnaseq-analysis/results\n\nScanning source files...\nFound 234 files (8.7 GB)\n\nScanning destination...\nFound 189 files (6.2 GB)\n\nUploading files... (4 concurrent transfers)\n\n\u2713 DE_results_updated.csv (3.4 MB) - 1s\n\u2713 pathway_analysis.txt (892 KB) - 0s\n...\n\nCompleted: 45 new files uploaded (2.5 GB) in 2m 18s\n  Skipped: 189 files (already in sync)\n</code></pre></p> <p>What Alex thinks: \"Excellent! My new results are now in S3 and will sync to my laptop automatically via the watch I set up.\"</p>"},{"location":"USER_SCENARIOS/#month-1-key-benefits-for-alex","title":"Month 1: Key Benefits for Alex","text":"<p>\u2705 Data safety: No more data loss from hardware failures \u2705 Disk space management: Can archive old data to S3, free up local space \u2705 Multi-device workflow: Access data from laptop and HPC seamlessly \u2705 Automatic versioning: S3 versioning enabled, can recover old file versions \u2705 Peace of mind: Continuous automatic backup of active work</p> <p>What Alex experiences: \"Cicada has completely changed how I manage my research data. I never worry about losing work anymore, and I can easily work across my laptop and the HPC cluster. The automatic backup means I focus on research, not file management.\"</p>"},{"location":"USER_SCENARIOS/#scenario-3-bioinformatician-analysis-pipeline","title":"Scenario 3: Bioinformatician - Analysis Pipeline","text":""},{"location":"USER_SCENARIOS/#persona-dr-james-park","title":"Persona: Dr. James Park","text":"<p>Background: - Bioinformatics core facility manager - Runs high-throughput analysis pipelines (WGS, RNA-seq, ChIP-seq) - Processes ~50 TB/month of sequencing data - Technical level: Expert Linux admin and pipeline developer - Infrastructure: 10-node analysis cluster + S3 for long-term storage</p> <p>Pain Points: - Need to move completed analysis results to S3 for archival - Want to automatically clean up local storage after S3 upload - Must verify data integrity before deleting local copies - Current scripts are fragile and require monitoring</p> <p>Goals: - Reliable pipeline integration for S3 archival - Automatic cleanup of local storage after successful upload - Strong verification that uploads completed successfully - Integration with Nextflow/Snakemake pipelines</p>"},{"location":"USER_SCENARIOS/#implementation-pipeline-integration","title":"Implementation: Pipeline Integration","text":"<p>Step 1: Install Cicada on Analysis Nodes</p> <pre><code># Install on all analysis nodes\nfor node in node{01..10}; do\n  ssh $node \"\n    wget -q https://github.com/scttfrdmn/cicada/releases/download/v0.1.0/cicada_0.1.0_Linux_x86_64.tar.gz\n    tar -xzf cicada_0.1.0_Linux_x86_64.tar.gz\n    sudo mv cicada /usr/local/bin/\n    cicada version\n  \"\ndone\n</code></pre> <p>What James thinks: \"Quick installation across all nodes. Now to integrate with our pipeline.\"</p> <p>Step 2: Create Pipeline Archive Script</p> <p>James creates a script for Nextflow integration:</p> <pre><code>#!/bin/bash\n# archive_to_s3.sh - Archive completed analysis to S3\n\nset -euo pipefail\n\nPROJECT_ID=\"$1\"\nLOCAL_DIR=\"/data/analysis/${PROJECT_ID}\"\nS3_PREFIX=\"s3://bioinformatics-archive/projects/${PROJECT_ID}\"\n\necho \"=== Archiving project ${PROJECT_ID} to S3 ===\"\n\n# Step 1: Dry run to validate\necho \"Step 1: Validating files...\"\ncicada sync --dry-run \"${LOCAL_DIR}\" \"${S3_PREFIX}\"\n\n# Step 2: Perform upload\necho \"Step 2: Uploading to S3...\"\ncicada sync --concurrency 16 \"${LOCAL_DIR}\" \"${S3_PREFIX}\"\n\n# Step 3: Verify upload with dry-run (should show everything in sync)\necho \"Step 3: Verifying upload...\"\ncicada sync --dry-run \"${LOCAL_DIR}\" \"${S3_PREFIX}\" | grep \"Skip: .* files (already synced)\"\n\nif [ $? -eq 0 ]; then\n  echo \"\u2713 Upload verified - all files match S3\"\n\n  # Step 4: Clean up local storage (optional)\n  if [ \"${DELETE_LOCAL:-false}\" = \"true\" ]; then\n    echo \"Step 4: Cleaning up local files...\"\n    rm -rf \"${LOCAL_DIR}\"\n    echo \"\u2713 Local files deleted\"\n  fi\nelse\n  echo \"\u2717 Upload verification failed - NOT deleting local files\"\n  exit 1\nfi\n\necho \"=== Archive complete for ${PROJECT_ID} ===\"\n</code></pre> <p>What James thinks: \"This gives me the safety I need - verify before delete, and fail loudly if anything goes wrong.\"</p> <p>Step 3: Integrate with Nextflow Pipeline</p> <pre><code>// nextflow.config\nprocess {\n  withName: archive_results {\n    executor = 'local'\n    memory = '2 GB'\n    cpus = 1\n  }\n}\n</code></pre> <pre><code>// main.nf\nprocess archive_results {\n  publishDir \"${params.outdir}\", mode: 'copy'\n\n  input:\n  path analysis_dir\n\n  output:\n  path \"archive.log\"\n\n  script:\n  \"\"\"\n  /usr/local/bin/archive_to_s3.sh ${params.project_id} &gt; archive.log 2&gt;&amp;1\n  \"\"\"\n}\n\nworkflow {\n  // ... analysis steps ...\n\n  // Archive to S3 after analysis completes\n  archive_results(analysis_output)\n}\n</code></pre> <p>What James thinks: \"Perfect integration with our existing Nextflow pipelines.\"</p> <p>Step 4: Test Pipeline Archival</p> <pre><code># Run test project\nnextflow run analysis_pipeline.nf \\\n  --project_id TEST_001 \\\n  --input samples.csv \\\n  --outdir /data/analysis/TEST_001\n\n# Check archive log\ncat /data/analysis/TEST_001/archive.log\n</code></pre> <p>Output: <pre><code>=== Archiving project TEST_001 to S3 ===\nStep 1: Validating files...\nPlan:\n  Upload: 1,247 files (124 GB)\n  Skip: 0 files\n\nStep 2: Uploading to S3...\nCompleted: 1,247 files uploaded (124 GB) in 15m 32s\n\nStep 3: Verifying upload...\nSkip: 1,247 files (already synced)\n\u2713 Upload verified - all files match S3\n\n=== Archive complete for TEST_001 ===\n</code></pre></p> <p>What James thinks: \"Excellent! Safe, verified archival. Now let me set up automatic cleanup for completed projects.\"</p>"},{"location":"USER_SCENARIOS/#key-benefits-for-james","title":"Key Benefits for James","text":"<p>\u2705 Pipeline integration: Easy to call from Nextflow/Snakemake \u2705 Verification: Dry-run validates uploads before cleanup \u2705 Performance: Configurable concurrency (using 16 for large transfers) \u2705 Reliability: Script fails safely if verification doesn't pass \u2705 Automation: No manual intervention needed</p>"},{"location":"USER_SCENARIOS/#scenario-4-lab-manager-data-governance","title":"Scenario 4: Lab Manager - Data Governance","text":""},{"location":"USER_SCENARIOS/#persona-dr-lisa-zhang","title":"Persona: Dr. Lisa Zhang","text":"<p>Background: - Lab manager for 25-person research lab - Responsible for data compliance and backup policies - Lab generates ~10 TB/month across 5 instruments - Technical level: Comfortable with command line, focuses on policy - Must ensure all data is backed up within 24 hours per NIH guidelines</p> <p>Pain Points: - No centralized visibility into what's backed up - Researchers sometimes forget to upload data - Hard to audit compliance with backup policies - Manual verification is time-consuming</p> <p>Goals: - Automated backup for all lab instruments - Audit trail showing all data is backed up - Compliance with institutional data policies - Easy monitoring and reporting</p>"},{"location":"USER_SCENARIOS/#implementation-lab-wide-deployment","title":"Implementation: Lab-Wide Deployment","text":"<p>Step 1: Instrument-Specific Configurations</p> <p>Lisa creates standard configurations for each instrument type:</p> <pre><code># Microscope workstation config\ncat &gt; /etc/cicada/microscope-watch.yaml &lt;&lt;EOF\n# Cicada watch configuration for Zeiss microscope\n# Auto-generated by lab manager\n\nwatches:\n  - source: /mnt/zeiss/output\n    destination: s3://lab-data-backup/microscopy/zeiss\n    debounce: 30\n    min_age: 60\n    enabled: true\nEOF\n\n# Sequencer workstation config\ncat &gt; /etc/cicada/sequencer-watch.yaml &lt;&lt;EOF\n# Cicada watch configuration for Illumina sequencer\n# Auto-generated by lab manager\n\nwatches:\n  - source: /data/illumina/output\n    destination: s3://lab-data-backup/sequencing/illumina\n    debounce: 60\n    min_age: 300\n    enabled: true\nEOF\n</code></pre> <p>What Lisa thinks: \"Standardized configurations make it easy to deploy across all instruments.\"</p> <p>Step 2: Monitoring Script</p> <p>Lisa creates a monitoring dashboard:</p> <pre><code>#!/bin/bash\n# lab_backup_status.sh - Check backup status across all instruments\n\necho \"=== Lab Backup Status Report ===\"\necho \"Generated: $(date)\"\necho \"\"\n\n# List of instrument workstations\nINSTRUMENTS=(\n  \"zeiss-microscope-1\"\n  \"zeiss-microscope-2\"\n  \"illumina-sequencer\"\n  \"flow-cytometer\"\n  \"mass-spec\"\n)\n\nfor instrument in \"${INSTRUMENTS[@]}\"; do\n  echo \"--- ${instrument} ---\"\n\n  # SSH to instrument and check Cicada status\n  ssh admin@${instrument} \"\n    if systemctl is-active cicada-watch &gt;/dev/null 2&gt;&amp;1; then\n      echo 'Status: \u2713 Running'\n      cicada watch list | tail -n +3\n    else\n      echo 'Status: \u2717 NOT RUNNING'\n    fi\n  \"\n\n  echo \"\"\ndone\n\necho \"=== Report Complete ===\"\n</code></pre> <p>What Lisa thinks: \"This gives me a quick overview of all instrument backups.\"</p> <p>Step 3: Weekly Backup Report</p> <p>Example output from monitoring script:</p> <pre><code>=== Lab Backup Status Report ===\nGenerated: Fri Nov 29 09:00:00 PST 2025\n\n--- zeiss-microscope-1 ---\nStatus: \u2713 Running\nID: /mnt/zeiss/output-1732397890\n  Source: /mnt/zeiss/output\n  Destination: s3://lab-data-backup/microscopy/zeiss\n  Status: Running\n  Last sync: 15 minutes ago (8 files, 3.2 GB)\n  Total syncs: 342\n  Total data: 1.8 TB\n\n--- zeiss-microscope-2 ---\nStatus: \u2713 Running\nID: /mnt/zeiss/output-1732398123\n  Last sync: 2 hours ago (12 files, 5.1 GB)\n  Total syncs: 298\n  Total data: 1.5 TB\n\n--- illumina-sequencer ---\nStatus: \u2713 Running\nID: /data/illumina/output-1732398456\n  Last sync: 30 minutes ago (1 file, 42 GB)\n  Total syncs: 156\n  Total data: 6.7 TB\n\n--- flow-cytometer ---\nStatus: \u2713 Running\nID: /data/cytometer/output-1732398789\n  Last sync: 1 hour ago (45 files, 890 MB)\n  Total syncs: 421\n  Total data: 387 GB\n\n--- mass-spec ---\nStatus: \u2717 NOT RUNNING\n\n=== Report Complete ===\n</code></pre> <p>What Lisa thinks: \"Most instruments are working great. Need to check on the mass-spec workstation.\"</p>"},{"location":"USER_SCENARIOS/#key-benefits-for-lisa","title":"Key Benefits for Lisa","text":"<p>\u2705 Centralized monitoring: Single script monitors all instruments \u2705 Compliance: Automated backups ensure 24-hour policy compliance \u2705 Audit trail: Watch statistics provide backup verification \u2705 Alerting: Can integrate monitoring with Slack/email alerts \u2705 Scalability: Easy to add new instruments with standard configs</p>"},{"location":"USER_SCENARIOS/#summary-common-patterns","title":"Summary: Common Patterns","text":""},{"location":"USER_SCENARIOS/#all-personas-benefit-from","title":"All Personas Benefit From:","text":"<ol> <li>Simple Installation</li> <li>Homebrew (macOS), apt/yum (Linux), or direct binary download</li> <li> <p>Single binary, no dependencies</p> </li> <li> <p>Flexible Configuration</p> </li> <li>YAML config files</li> <li>Environment-specific settings</li> <li> <p>AWS profile integration</p> </li> <li> <p>Reliable Transfers</p> </li> <li>MD5/ETag verification</li> <li>Concurrent transfers</li> <li> <p>Resume capability (via S3 multipart for large files)</p> </li> <li> <p>Automatic Watching</p> </li> <li>File system monitoring</li> <li>Debouncing and min-age filtering</li> <li> <p>Persistent configuration</p> </li> <li> <p>Verification</p> </li> <li>Dry-run mode</li> <li>Built-in integrity checking</li> <li>Clear status reporting</li> </ol>"},{"location":"USER_SCENARIOS/#getting-started","title":"Getting Started","text":"<p>Choose the scenario that best matches your use case and follow the walkthrough. All features shown are available in Cicada v0.1.0.</p> <p>Installation: - macOS: <code>brew install scttfrdmn/tap/cicada</code> - Linux/Windows: Download from releases</p> <p>Documentation: - README.md - Full documentation - VISION.md - Future roadmap - GitHub Issues - Report problems or request features</p>"},{"location":"USER_SCENARIOS_v0.2.0/","title":"Cicada v0.2.0 User Scenarios - Data Commons Management","text":"<p>This document provides persona-based walkthroughs for v0.2.0, which adds comprehensive data management capabilities to Cicada's data commons platform: automated metadata extraction, multi-format support, instrument preset validation, and quality assurance features. These scenarios demonstrate how labs use Cicada for daily data management, organization, and quality control.</p> <p>What's New in v0.2.0: - 14 file format extractors (microscopy, sequencing, mass spec, and more) - 6 instrument-specific metadata types - 8 instrument presets for quality validation - S3 metadata tagging for enhanced data organization - Optional: DOI preparation support for dataset publication</p>"},{"location":"USER_SCENARIOS_v0.2.0/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Postdoc: Managing Sequencing Data</li> <li>Lab Manager: Data Quality Validation</li> <li>Data Curator: Organizing Research Data</li> <li>Advanced: Preparing Data for Publication</li> </ol>"},{"location":"USER_SCENARIOS_v0.2.0/#scenario-1-postdoc-managing-sequencing-data","title":"Scenario 1: Postdoc - Managing Sequencing Data","text":""},{"location":"USER_SCENARIOS_v0.2.0/#persona-dr-sarah-chen","title":"Persona: Dr. Sarah Chen","text":"<p>Background: - Postdoctoral researcher in genomics - Generates ~200 FASTQ files per month from Illumina NovaSeq - Needs to organize, validate, and track sequencing data - Maintains lab data commons with shared S3 storage - Technical level: Comfortable with command line, basic bioinformatics</p> <p>Pain Points: - Manually tracking file metadata is tedious and error-prone - Hard to find specific datasets months later - No systematic quality control process - Uncertainty about which files have been analyzed - Need better data organization for the lab</p> <p>Goals: - Automatically extract and track metadata from sequencing files - Validate data quality against instrument specifications - Organize data in S3 with searchable metadata - Maintain lab data commons for current and future use - Have data well-documented for any future needs (analysis, sharing, publication)</p>"},{"location":"USER_SCENARIOS_v0.2.0/#version-info","title":"Version Info","text":"<ul> <li>\u2705 v0.2.0: Metadata extraction and DOI features</li> </ul>"},{"location":"USER_SCENARIOS_v0.2.0/#day-1-first-metadata-extraction-5-minutes","title":"Day 1: First Metadata Extraction (5 minutes)","text":"<p>Step 1: Install Cicada v0.2.0</p> <p>Sarah already has Cicada v0.1.0 installed, so she updates:</p> <pre><code># Update to v0.2.0\nbrew upgrade cicada\n\n# Verify version\ncicada version\n</code></pre> <p>Output: <pre><code>cicada version 0.2.0\n  Go version: go1.23.4\n  OS/Arch: darwin/arm64\n</code></pre></p> <p>What Sarah thinks: \"Great! Now I have the metadata features. Let me try extracting metadata from one of my FASTQ files.\"</p> <p>Step 2: Extract Metadata from Single FASTQ File</p> <p>Sarah has a completed sequencing run:</p> <pre><code># Check the file\nls -lh /data/sequencing/sample_A_R1.fastq.gz\n\n# Extract metadata\ncicada metadata extract /data/sequencing/sample_A_R1.fastq.gz\n</code></pre> <p>Output: <pre><code>{\n  \"format\": \"FASTQ\",\n  \"compression\": \"gzip\",\n  \"file_name\": \"/data/sequencing/sample_A_R1.fastq.gz\",\n  \"file_size\": 2147483648,\n  \"total_reads\": 45623891,\n  \"total_bases\": 6843583650,\n  \"mean_read_length\": 150,\n  \"min_read_length\": 150,\n  \"max_read_length\": 150,\n  \"gc_content_percent\": 42.3,\n  \"mean_quality_score\": 36.8,\n  \"min_quality_score\": 12,\n  \"max_quality_score\": 41,\n  \"is_paired_end\": true,\n  \"read_pair\": \"R1\",\n  \"instrument_type\": \"sequencing\",\n  \"data_type\": \"nucleotide_sequence\",\n  \"extractor_name\": \"fastq\",\n  \"schema_name\": \"fastq_v1\"\n}\n</code></pre></p> <p>What Sarah thinks: \"Wow! It extracted all the key metrics automatically - 45M reads with mean quality 36.8. This is exactly what I need for my records.\"</p> <p>Step 3: Extract Metadata in Human-Readable Format</p> <pre><code># Get table format for easier reading\ncicada metadata extract /data/sequencing/sample_A_R1.fastq.gz --format table\n</code></pre> <p>Output: <pre><code>Metadata for: sample_A_R1.fastq.gz\n================================\n\nFile Information:\n  format                : FASTQ\n  compression           : gzip\n  file_size             : 2.0 GB\n  extractor_name        : fastq\n  schema_name           : fastq_v1\n\nSequence Statistics:\n  total_reads           : 45,623,891\n  total_bases           : 6,843,583,650\n  mean_read_length      : 150\n  min_read_length       : 150\n  max_read_length       : 150\n  gc_content_percent    : 42.3%\n\nQuality Metrics:\n  mean_quality_score    : 36.8\n  min_quality_score     : 12\n  max_quality_score     : 41\n\nSequencing Info:\n  instrument_type       : sequencing\n  data_type             : nucleotide_sequence\n  is_paired_end         : true\n  read_pair             : R1\n</code></pre></p> <p>What Sarah thinks: \"Perfect! This is much easier to read. Now let me extract metadata from all my files.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#day-1-batch-metadata-extraction-10-minutes","title":"Day 1: Batch Metadata Extraction (10 minutes)","text":"<p>Step 4: Extract Metadata from All FASTQ Files</p> <p>Sarah has 48 FASTQ files (24 samples, paired-end):</p> <pre><code># Create metadata directory\nmkdir -p /data/sequencing/metadata\n\n# Extract metadata for each file and save to JSON\nfor file in /data/sequencing/*.fastq.gz; do\n  basename=$(basename $file .fastq.gz)\n  cicada metadata extract $file --format json --output /data/sequencing/metadata/${basename}.json\ndone\n\n# Count processed files\nls /data/sequencing/metadata/*.json | wc -l\n</code></pre> <p>Output: <pre><code>Metadata extracted to /data/sequencing/metadata/sample_A_R1.json\nMetadata extracted to /data/sequencing/metadata/sample_A_R2.json\nMetadata extracted to /data/sequencing/metadata/sample_B_R1.json\n...\n48\n</code></pre></p> <p>What Sarah thinks: \"Excellent! Now I have metadata for all 48 files. Let me create a summary report.\"</p> <p>Step 5: Create Summary Statistics</p> <p>Sarah creates a simple script to summarize metadata:</p> <pre><code>#!/bin/bash\n# summarize_metadata.sh\n\necho \"Sequencing Run Summary\"\necho \"======================\"\necho \"\"\n\ntotal_reads=0\ntotal_bases=0\ncount=0\n\nfor json in /data/sequencing/metadata/*.json; do\n  reads=$(jq '.total_reads' $json)\n  bases=$(jq '.total_bases' $json)\n\n  total_reads=$((total_reads + reads))\n  total_bases=$((total_bases + bases))\n  count=$((count + 1))\ndone\n\necho \"Total files: $count\"\necho \"Total reads: $(numfmt --grouping $total_reads)\"\necho \"Total bases: $(numfmt --grouping $total_bases)\"\necho \"Average reads per file: $(numfmt --grouping $((total_reads / count)))\"\necho \"Average quality: $(jq -s 'map(.mean_quality_score) | add / length' /data/sequencing/metadata/*.json)\"\n</code></pre> <p>Output: <pre><code>Sequencing Run Summary\n======================\n\nTotal files: 48\nTotal reads: 2,189,946,768\nTotal bases: 328,492,015,200\nAverage reads per file: 45,623,891\nAverage quality: 36.7\n</code></pre></p> <p>What Sarah thinks: \"Perfect! I can include this summary in my lab notebook and grant reports.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#week-1-validating-with-instrument-presets-5-minutes","title":"Week 1: Validating with Instrument Presets (5 minutes)","text":"<p>Step 6: Validate Against Illumina Preset</p> <p>Sarah wants to verify her FASTQ files meet Illumina NovaSeq specifications:</p> <pre><code># Validate one file against Illumina preset\ncicada metadata validate /data/sequencing/sample_A_R1.fastq.gz \\\n  --preset illumina-novaseq\n</code></pre> <p>Output: <pre><code>\u2713 /data/sequencing/sample_A_R1.fastq.gz: valid (FASTQ)\n     Quality Score: 100.0/100\n\nValidation Results:\n  Present Fields (8):\n    \u2713 format\n    \u2713 instrument_type\n    \u2713 data_type\n    \u2713 total_reads\n    \u2713 total_bases\n    \u2713 mean_quality_score\n    \u2713 is_paired_end\n    \u2713 read_pair\n\n  Missing Optional Fields (0):\n    (All optional fields present)\n\n  Errors (0):\n    No errors\n\nSummary: Excellent quality metadata - all fields present\n</code></pre></p> <p>What Sarah thinks: \"Perfect! My data validates successfully against the Illumina NovaSeq preset. This confirms the metadata is complete.\"</p> <p>Step 7: List Available Presets</p> <pre><code># See what other presets are available\ncicada metadata preset list\n</code></pre> <p>Output: <pre><code>Available Instrument Presets:\n\n  Illumina NovaSeq\n    ID: illumina-novaseq\n    Manufacturer: Illumina\n    Type: sequencing\n    Models: NovaSeq 6000, NovaSeq X, NovaSeq X Plus\n    Formats: .fastq, .fq, .fastq.gz, .fq.gz\n\n  Illumina MiSeq\n    ID: illumina-miseq\n    Manufacturer: Illumina\n    Type: sequencing\n    Models: MiSeq\n    Formats: .fastq, .fq, .fastq.gz, .fq.gz\n\n  Illumina NextSeq\n    ID: illumina-nextseq\n    Manufacturer: Illumina\n    Type: sequencing\n    Models: NextSeq 500, NextSeq 550, NextSeq 1000, NextSeq 2000\n    Formats: .fastq, .fq, .fastq.gz, .fq.gz\n\n  Generic Sequencing\n    ID: generic-sequencing\n    Manufacturer: Various\n    Type: sequencing\n    Formats: .fastq, .fq, .fastq.gz, .fq.gz\n\n  Zeiss LSM 880\n    ID: zeiss-lsm-880\n    Manufacturer: Zeiss\n    Type: microscopy\n    Models: LSM 880\n    Formats: .czi\n\n  Zeiss LSM 900\n    ID: zeiss-lsm-900\n    Manufacturer: Zeiss\n    Type: microscopy\n    Models: LSM 900\n    Formats: .czi\n\n  Zeiss LSM 980\n    ID: zeiss-lsm-980\n    Manufacturer: Zeiss\n    Type: microscopy\n    Models: LSM 980\n    Formats: .czi\n\n  Generic Microscopy\n    ID: generic-microscopy\n    Manufacturer: Various\n    Type: microscopy\n    Formats: .tif, .tiff, .czi, .nd2, .lif, .ome.tif, .ome.tiff\n\nTotal: 8 presets\n</code></pre></p> <p>What Sarah thinks: \"Great selection! These cover all the instruments in our lab.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#month-1-key-benefits-for-sarah","title":"Month 1: Key Benefits for Sarah","text":"<p>\u2705 Automated extraction: No more manual metadata tracking \u2705 Quality metrics: Instant access to read counts, GC content, quality scores \u2705 Validation: Verify metadata completeness against instrument specs \u2705 Batch processing: Extract metadata from hundreds of files easily \u2705 Reporting: Generate summaries for grants and publications</p> <p>What Sarah experiences: \"Metadata extraction has saved me hours of manual work. I can now generate comprehensive reports in seconds, and I'm confident my data documentation is complete and accurate.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#scenario-2-graduate-student-doi-preparation-for-publication","title":"Scenario 2: Graduate Student - DOI Preparation for Publication","text":""},{"location":"USER_SCENARIOS_v0.2.0/#persona-marcus-johnson","title":"Persona: Marcus Johnson","text":"<p>Background: - 5<sup>th</sup> year PhD student in molecular biology - About to publish first paper - Journal requires data deposition with DOI - Never registered a DOI before - Technical level: Basic command line, learning as he goes</p> <p>Pain Points: - Don't know how to prepare data for DOI registration - Not sure what metadata is required - Worried about missing required fields - Repository forms are confusing</p> <p>Goals: - Prepare sequencing data for DOI registration - Ensure metadata meets DataCite requirements - Get step-by-step guidance on what's needed - Submit to repository with confidence</p>"},{"location":"USER_SCENARIOS_v0.2.0/#week-1-understanding-doi-requirements-10-minutes","title":"Week 1: Understanding DOI Requirements (10 minutes)","text":"<p>Step 1: Initial DOI Validation</p> <p>Marcus has raw sequencing data from his experiments:</p> <pre><code># Check what's needed for DOI\ncicada doi validate /data/experiment/sample_001.fastq.gz\n</code></pre> <p>Output: <pre><code>DOI Validation Results\n======================\n\nFile: sample_001.fastq.gz\n\n\u2717 NOT READY for DOI minting\n\nQuality Score: 47.0/100 (Moderate)\n\nPresent Fields (8):\n  \u2713 identifier (auto-generated)\n  \u2713 title\n  \u2713 publisher\n  \u2713 publication_year\n  \u2713 resource_type\n  \u2713 description\n  \u2713 license\n  \u2713 keywords\n\nMissing Fields (9):\n  \u2717 real creator names\n  \u2717 url\n  \u2717 version\n  \u2717 language\n  \u2717 dates\n  \u2717 related_identifiers\n  \u2717 contributors\n  \u2717 funding_references\n  \u2717 geo_locations\n\nErrors (1):\n  \u2022 creator must be specified (currently set to 'Unknown Creator')\n\nWarnings (5):\n  \u2022 publisher should be specified (currently set to 'Unknown Publisher')\n  \u2022 landing page URL is recommended\n  \u2022 author ORCIDs are recommended for attribution\n  \u2022 author affiliations are recommended\n  \u2022 Quality score 47.0 is below minimum threshold 60.0\n\nRecommendations:\n  Fix all errors before minting DOI:\n    - creator must be specified (currently set to 'Unknown Creator')\n  Metadata quality is moderate. To improve:\n    - Add missing recommended fields\n    - Enhance description with methods and context\n    - Include author ORCIDs for proper attribution\n</code></pre></p> <p>What Marcus thinks: \"Okay, I need to add author information and improve the description. Let me create an enrichment file.\"</p> <p>Step 2: Create Metadata Enrichment File</p> <p>Marcus creates a file with additional metadata:</p> <pre><code># Create enrichment file\ncat &gt; /data/experiment/enrichment.yaml &lt;&lt;EOF\ntitle: \"RNA-seq data for CRISPR knockout of gene X in human cell lines\"\n\nauthors:\n  - name: Marcus Johnson\n    orcid: 0000-0002-1234-5678\n    affiliation: Department of Molecular Biology, State University\n  - name: Dr. Jennifer Lee\n    orcid: 0000-0003-9876-5432\n    affiliation: Department of Molecular Biology, State University\n    role: supervisor\n\ndescription: |\n  This dataset contains RNA-seq data from CRISPR knockout experiments targeting\n  gene X in HEK293 cells. Three biological replicates were sequenced for both\n  control and knockout conditions using Illumina NovaSeq 6000. Reads were\n  quality-filtered and aligned to the human genome (GRCh38).\n\n  Experimental conditions:\n  - Cell line: HEK293\n  - Treatment: CRISPR knockout of gene X\n  - Sequencing: Illumina NovaSeq 6000, 2x150bp paired-end\n  - Biological replicates: 3 per condition\n\nkeywords:\n  - RNA-seq\n  - CRISPR\n  - gene knockout\n  - HEK293\n  - transcriptomics\n\npublisher: State University Genomics Core\nlicense: CC-BY-4.0\n\nfunding_references:\n  - funder_name: National Science Foundation\n    award_number: NSF-1234567\n\nrelated_identifiers:\n  - identifier: \"10.1234/journal.2025.123\"\n    relation: IsSupplementTo\n    type: DOI\nEOF\n</code></pre> <p>What Marcus thinks: \"I've added all my information. Let me see how this improves the DOI readiness.\"</p> <p>Step 3: Prepare DOI with Enrichment</p> <pre><code># Prepare DOI with enrichment\ncicada doi prepare /data/experiment/sample_001.fastq.gz \\\n  --enrich /data/experiment/enrichment.yaml \\\n  --publisher \"State University Genomics Core\"\n</code></pre> <p>Output: <pre><code>DOI Preparation Results\n=======================\n\nFile: sample_001.fastq.gz\n\nDataset Information:\n  Title: RNA-seq data for CRISPR knockout of gene X in human cell lines\n  Authors: 2\n    1. Marcus Johnson (ORCID: 0000-0002-1234-5678)\n       Department of Molecular Biology, State University\n    2. Dr. Jennifer Lee (ORCID: 0000-0003-9876-5432)\n       Department of Molecular Biology, State University\n       Role: supervisor\n  Publisher: State University Genomics Core\n  Resource Type: Dataset\n  Keywords: 5\n  Related Publications: 1\n\nValidation:\n  \u2713 Ready for DOI minting\n  Quality Score: 85.0/100 (Excellent)\n  Errors: 0\n  Warnings: 1\n\nWarnings:\n  \u2022 landing page URL is recommended (will be added by repository)\n\nRecommendations:\n  Metadata quality is excellent. Ready for DOI registration.\n  Optional improvements:\n    - Add temporal coverage if applicable\n    - Include spatial/geographical context if relevant\n</code></pre></p> <p>What Marcus thinks: \"Excellent! Quality score jumped from 47 to 85. I'm ready to submit this to the repository.\"</p> <p>Step 4: Save DOI-Ready Metadata</p> <pre><code># Save prepared metadata to file for repository submission\ncicada doi prepare /data/experiment/sample_001.fastq.gz \\\n  --enrich /data/experiment/enrichment.yaml \\\n  --publisher \"State University Genomics Core\" \\\n  --format json \\\n  --output /data/experiment/datacite_metadata.json\n</code></pre> <p>Output: <pre><code>DOI preparation complete\nMetadata saved to /data/experiment/datacite_metadata.json\n\nThis file contains DataCite-compliant metadata ready for repository submission.\n</code></pre></p> <p>View the generated metadata: <pre><code>cat /data/experiment/datacite_metadata.json\n</code></pre></p> <p>Output (abbreviated): <pre><code>{\n  \"identifier\": {\n    \"identifier\": \"dataset-2025-001\",\n    \"identifierType\": \"local\"\n  },\n  \"creators\": [\n    {\n      \"name\": \"Johnson, Marcus\",\n      \"nameType\": \"Personal\",\n      \"givenName\": \"Marcus\",\n      \"familyName\": \"Johnson\",\n      \"nameIdentifiers\": [\n        {\n          \"nameIdentifier\": \"https://orcid.org/0000-0002-1234-5678\",\n          \"nameIdentifierScheme\": \"ORCID\"\n        }\n      ],\n      \"affiliation\": [\"Department of Molecular Biology, State University\"]\n    },\n    {\n      \"name\": \"Lee, Jennifer\",\n      \"nameType\": \"Personal\",\n      \"givenName\": \"Jennifer\",\n      \"familyName\": \"Lee\",\n      \"nameIdentifiers\": [\n        {\n          \"nameIdentifier\": \"https://orcid.org/0000-0003-9876-5432\",\n          \"nameIdentifierScheme\": \"ORCID\"\n        }\n      ],\n      \"affiliation\": [\"Department of Molecular Biology, State University\"]\n    }\n  ],\n  \"titles\": [\n    {\n      \"title\": \"RNA-seq data for CRISPR knockout of gene X in human cell lines\"\n    }\n  ],\n  \"publisher\": \"State University Genomics Core\",\n  \"publicationYear\": 2025,\n  \"resourceType\": {\n    \"resourceType\": \"Dataset\",\n    \"resourceTypeGeneral\": \"Dataset\"\n  },\n  \"subjects\": [\n    {\"subject\": \"RNA-seq\"},\n    {\"subject\": \"CRISPR\"},\n    {\"subject\": \"gene knockout\"},\n    {\"subject\": \"HEK293\"},\n    {\"subject\": \"transcriptomics\"}\n  ],\n  \"fundingReferences\": [\n    {\n      \"funderName\": \"National Science Foundation\",\n      \"awardNumber\": \"NSF-1234567\"\n    }\n  ],\n  \"relatedIdentifiers\": [\n    {\n      \"relatedIdentifier\": \"10.1234/journal.2025.123\",\n      \"relatedIdentifierType\": \"DOI\",\n      \"relationType\": \"IsSupplementTo\"\n    }\n  ],\n  \"descriptions\": [\n    {\n      \"description\": \"This dataset contains RNA-seq data from CRISPR knockout experiments...\",\n      \"descriptionType\": \"Abstract\"\n    }\n  ],\n  \"rightsList\": [\n    {\n      \"rights\": \"Creative Commons Attribution 4.0 International\",\n      \"rightsURI\": \"https://creativecommons.org/licenses/by/4.0/\",\n      \"rightsIdentifier\": \"CC-BY-4.0\"\n    }\n  ]\n}\n</code></pre></p> <p>What Marcus thinks: \"Perfect! This metadata file is exactly what I need for repository submission. Now I can upload to Zenodo or Dryad with confidence.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#week-2-repository-submission","title":"Week 2: Repository Submission","text":"<p>Step 5: Upload to Repository</p> <p>Marcus uploads his data to Zenodo using the prepared metadata:</p> <ol> <li>Upload files to Zenodo via web interface</li> <li>Import metadata from <code>datacite_metadata.json</code></li> <li>Add landing page URL (provided by Zenodo)</li> <li>Publish to get DOI</li> </ol> <p>Result: Dataset published with DOI <code>10.5281/zenodo.1234567</code></p>"},{"location":"USER_SCENARIOS_v0.2.0/#month-1-key-benefits-for-marcus","title":"Month 1: Key Benefits for Marcus","text":"<p>\u2705 Validation: Know exactly what metadata is required \u2705 Guidance: Clear recommendations for improvement \u2705 Quality scoring: Track metadata completeness (47 \u2192 85%) \u2705 DataCite compliance: Automated mapping to repository schema \u2705 Confidence: Submit knowing metadata is complete and correct</p> <p>What Marcus experiences: \"Cicada made DOI preparation so much easier. Instead of guessing what metadata I needed, I got clear guidance every step of the way. My data is now properly documented and citable.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#scenario-3-lab-manager-preset-validation","title":"Scenario 3: Lab Manager - Preset Validation","text":""},{"location":"USER_SCENARIOS_v0.2.0/#persona-dr-emily-rodriguez","title":"Persona: Dr. Emily Rodriguez","text":"<p>Background: - Lab manager for multi-instrument core facility - Manages Zeiss microscopes, Illumina sequencers, flow cytometer - 50+ users generating diverse data types - Needs to ensure consistent metadata quality - Technical level: System administrator, scripting experience</p> <p>Pain Points: - Users don't document metadata consistently - Hard to verify instrument-specific metadata is complete - Need to enforce metadata standards across instruments - Manual validation is time-consuming</p> <p>Goals: - Automated validation of instrument metadata - Enforce metadata standards for each instrument - Generate compliance reports for users - Integrate validation into data upload workflow</p>"},{"location":"USER_SCENARIOS_v0.2.0/#week-1-setting-up-preset-validation-15-minutes","title":"Week 1: Setting Up Preset Validation (15 minutes)","text":"<p>Step 1: Create Validation Script</p> <p>Emily creates a script to validate all data before S3 upload:</p> <pre><code>#!/bin/bash\n# validate_before_upload.sh\n# Validates metadata before uploading to S3\n\nset -euo pipefail\n\nFILE=\"$1\"\nINSTRUMENT_TYPE=\"$2\"\n\necho \"=== Validating ${FILE} ===\"\n\n# Determine preset based on instrument\ncase \"${INSTRUMENT_TYPE}\" in\n  \"zeiss-880\")\n    PRESET=\"zeiss-lsm-880\"\n    ;;\n  \"zeiss-900\")\n    PRESET=\"zeiss-lsm-900\"\n    ;;\n  \"illumina-novaseq\")\n    PRESET=\"illumina-novaseq\"\n    ;;\n  \"illumina-miseq\")\n    PRESET=\"illumina-miseq\"\n    ;;\n  *)\n    echo \"Unknown instrument type: ${INSTRUMENT_TYPE}\"\n    exit 1\n    ;;\nesac\n\necho \"Using preset: ${PRESET}\"\n\n# Validate metadata\ncicada metadata validate \"${FILE}\" --preset \"${PRESET}\"\n\nif [ $? -eq 0 ]; then\n  echo \"\u2713 Validation passed - ready for upload\"\n  exit 0\nelse\n  echo \"\u2717 Validation failed - fix metadata before upload\"\n  exit 1\nfi\n</code></pre> <p>What Emily thinks: \"This ensures all data is validated before upload. Users will get immediate feedback if metadata is incomplete.\"</p> <p>Step 2: Test Validation with Good Data</p> <pre><code># Test with valid Illumina FASTQ\n./validate_before_upload.sh \\\n  /data/illumina/sample_A_R1.fastq.gz \\\n  illumina-novaseq\n</code></pre> <p>Output: <pre><code>=== Validating /data/illumina/sample_A_R1.fastq.gz ===\nUsing preset: illumina-novaseq\n\n\u2713 /data/illumina/sample_A_R1.fastq.gz: valid (FASTQ)\n     Quality Score: 100.0/100\n\nValidation Results:\n  Present Fields (8): All required fields present\n  Missing Optional Fields (0): All optional fields present\n  Errors (0): No errors\n\n\u2713 Validation passed - ready for upload\n</code></pre></p> <p>What Emily thinks: \"Perfect! Validation passed. Now let me test with incomplete data.\"</p> <p>Step 3: Test Validation with Incomplete Data</p> <pre><code># Test with file missing metadata\n./validate_before_upload.sh \\\n  /data/illumina/incomplete.fastq \\\n  illumina-novaseq\n</code></pre> <p>Output: <pre><code>=== Validating /data/illumina/incomplete.fastq ===\nUsing preset: illumina-novaseq\n\n\u2717 /data/illumina/incomplete.fastq: invalid (FASTQ)\n     Quality Score: 62.5/100\n\nValidation Results:\n  Present Fields (5):\n    \u2713 format\n    \u2713 instrument_type\n    \u2713 data_type\n    \u2713 total_reads\n    \u2713 total_bases\n\n  Missing Required Fields (3):\n    \u2717 mean_quality_score\n    \u2717 is_paired_end\n    \u2717 read_pair\n\n  Errors (3):\n    \u2022 missing required field: mean_quality_score\n    \u2022 missing required field: is_paired_end\n    \u2022 missing required field: read_pair\n\n\u2717 Validation failed - fix metadata before upload\n</code></pre></p> <p>What Emily thinks: \"Good! The validation caught the missing fields. Users will know exactly what needs to be fixed.\"</p> <p>Step 4: Integrate with Upload Workflow</p> <p>Emily updates the Cicada watch configuration to include validation:</p> <pre><code># /etc/cicada/illumina-watch-with-validation.yaml\n\nwatches:\n  - source: /data/illumina/output\n    destination: s3://facility-data/illumina\n    debounce: 60\n    min_age: 120\n    enabled: true\n    hooks:\n      pre_sync: |\n        # Validate before upload\n        for file in ${CHANGED_FILES}; do\n          if [[ $file == *.fastq* ]]; then\n            cicada metadata validate \"$file\" --preset illumina-novaseq || exit 1\n          fi\n        done\n</code></pre> <p>What Emily thinks: \"Now validation happens automatically before any upload. Users will get immediate feedback if their data has metadata issues.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#week-2-generating-compliance-reports-10-minutes","title":"Week 2: Generating Compliance Reports (10 minutes)","text":"<p>Step 5: Create Validation Report Script</p> <pre><code>#!/bin/bash\n# generate_validation_report.sh\n# Generate weekly metadata compliance report\n\nOUTPUT_DIR=\"/data/reports\"\nmkdir -p \"${OUTPUT_DIR}\"\n\nREPORT_FILE=\"${OUTPUT_DIR}/metadata_compliance_$(date +%Y-%m-%d).md\"\n\ncat &gt; \"${REPORT_FILE}\" &lt;&lt;EOF\n# Metadata Compliance Report\nGenerated: $(date)\n\n## Illumina NovaSeq Files\nEOF\n\n# Validate all Illumina files\nfor file in /data/illumina/archive/*.fastq.gz; do\n  echo \"Validating: $(basename $file)\"\n\n  result=$(cicada metadata validate \"$file\" --preset illumina-novaseq 2&gt;&amp;1)\n\n  if echo \"$result\" | grep -q \"\u2713.*valid\"; then\n    echo \"- \u2713 $(basename $file): PASS\" &gt;&gt; \"${REPORT_FILE}\"\n  else\n    echo \"- \u2717 $(basename $file): FAIL\" &gt;&gt; \"${REPORT_FILE}\"\n    echo \"  - Issues: $(echo \"$result\" | grep \"Error\\|Warning\" | head -3)\" &gt;&gt; \"${REPORT_FILE}\"\n  fi\ndone\n\ncat &gt;&gt; \"${REPORT_FILE}\" &lt;&lt;EOF\n\n## Zeiss LSM 880 Files\nEOF\n\n# Validate Zeiss files\nfor file in /data/zeiss/archive/*.czi; do\n  echo \"Validating: $(basename $file)\"\n\n  result=$(cicada metadata validate \"$file\" --preset zeiss-lsm-880 2&gt;&amp;1)\n\n  if echo \"$result\" | grep -q \"\u2713.*valid\"; then\n    echo \"- \u2713 $(basename $file): PASS\" &gt;&gt; \"${REPORT_FILE}\"\n  else\n    echo \"- \u2717 $(basename $file): FAIL\" &gt;&gt; \"${REPORT_FILE}\"\n    echo \"  - Issues: $(echo \"$result\" | grep \"Error\\|Warning\" | head -3)\" &gt;&gt; \"${REPORT_FILE}\"\n  fi\ndone\n\necho \"Report generated: ${REPORT_FILE}\"\n</code></pre> <p>Example Report Output: <pre><code># Metadata Compliance Report\nGenerated: Fri Nov 29 2025 10:00:00\n\n## Illumina NovaSeq Files\n- \u2713 sample_001_R1.fastq.gz: PASS\n- \u2713 sample_001_R2.fastq.gz: PASS\n- \u2713 sample_002_R1.fastq.gz: PASS\n- \u2717 sample_003_R1.fastq.gz: FAIL\n  - Issues: missing required field: mean_quality_score\n- \u2713 sample_004_R1.fastq.gz: PASS\n\n## Zeiss LSM 880 Files\n- \u2713 experiment_001.czi: PASS\n- \u2713 experiment_002.czi: PASS\n- \u2717 experiment_003.czi: FAIL\n  - Issues: missing required field: image_width\n</code></pre></p> <p>What Emily thinks: \"Perfect! Now I can send weekly compliance reports to users showing which files need metadata fixes.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#month-1-key-benefits-for-emily","title":"Month 1: Key Benefits for Emily","text":"<p>\u2705 Automated validation: No more manual metadata checking \u2705 Instrument-specific: Different presets for different instruments \u2705 Pre-upload validation: Catch issues before data is uploaded \u2705 Compliance reporting: Track metadata quality over time \u2705 User feedback: Clear error messages guide users to fix issues</p> <p>What Emily experiences: \"Preset validation has transformed our metadata quality. Users get immediate feedback when metadata is incomplete, and I can generate compliance reports effortlessly. Our data is now consistently well-documented.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#scenario-4-data-curator-metadata-enrichment","title":"Scenario 4: Data Curator - Metadata Enrichment","text":""},{"location":"USER_SCENARIOS_v0.2.0/#persona-dr-thomas-liu","title":"Persona: Dr. Thomas Liu","text":"<p>Background: - Data curator for university research data repository - Receives datasets from 100+ research groups - Responsible for DOI minting and long-term preservation - Must ensure metadata meets repository standards - Technical level: Expert in metadata standards, METS/MODS, Dublin Core, DataCite</p> <p>Pain Points: - Researchers submit datasets with minimal metadata - Manual enrichment is time-consuming (15-30 min per dataset) - Need to track metadata quality improvements - Repository standards require comprehensive metadata</p> <p>Goals: - Streamline metadata enrichment workflow - Ensure all datasets meet repository standards - Track metadata quality scores - Reduce time spent on metadata cleanup</p>"},{"location":"USER_SCENARIOS_v0.2.0/#week-1-metadata-quality-assessment-15-minutes","title":"Week 1: Metadata Quality Assessment (15 minutes)","text":"<p>Step 1: Initial Quality Check</p> <p>Thomas receives a new dataset submission:</p> <pre><code># Assess initial metadata quality\ncicada doi validate /repository/submissions/chen-2025-001/sample.fastq.gz\n</code></pre> <p>Output: <pre><code>DOI Validation Results\n======================\n\nFile: sample.fastq.gz\n\n\u2717 NOT READY for DOI minting\n\nQuality Score: 52.0/100 (Moderate)\n\nPresent Fields (10):\n  \u2713 identifier\n  \u2713 title\n  \u2713 publisher\n  \u2713 publication_year\n  \u2713 resource_type\n  \u2713 description (basic)\n  \u2713 license\n  \u2713 keywords (limited)\n  \u2713 format\n  \u2713 file_size\n\nMissing Recommended Fields (7):\n  \u2717 author ORCIDs\n  \u2717 author affiliations\n  \u2717 funding information\n  \u2717 related publications\n  \u2717 temporal coverage\n  \u2717 methods description\n  \u2717 version information\n\nWarnings (4):\n  \u2022 description is minimal, consider adding methodology\n  \u2022 only 2 keywords provided, recommend 5-10\n  \u2022 author ORCIDs improve attribution and discovery\n  \u2022 funding information recommended for compliance\n</code></pre></p> <p>What Thomas thinks: \"Score of 52/100. I need to enrich this with ORCIDs, funding info, and better description. Let me contact the researcher.\"</p> <p>Step 2: Create Enrichment Template</p> <p>Thomas generates a template for the researcher to fill out:</p> <pre><code># Generate enrichment template from existing metadata\ncicada doi prepare /repository/submissions/chen-2025-001/sample.fastq.gz \\\n  --format yaml \\\n  --output /repository/submissions/chen-2025-001/enrichment_template.yaml\n</code></pre> <p>Generated template (abbreviated): <pre><code>title: \"Existing title from file metadata\"\n\nauthors:\n  - name: \"Unknown Creator\"  # \u2190 NEEDS UPDATE\n    # orcid: \"0000-0000-0000-0000\"  # \u2190 ADD ORCID\n    # affiliation: \"Institution Name\"  # \u2190 ADD AFFILIATION\n\ndescription: |\n  Basic description from file.\n\n  # \u2190 ADD: Detailed methodology\n  # \u2190 ADD: Experimental conditions\n  # \u2190 ADD: Data processing steps\n\nkeywords:\n  - keyword1\n  - keyword2\n  # \u2190 ADD: More keywords (5-10 total recommended)\n\n# \u2190 ADD: Funding information\n# funding_references:\n#   - funder_name: \"Funding Agency\"\n#     award_number: \"Grant-12345\"\n\n# \u2190 ADD: Related publications\n# related_identifiers:\n#   - identifier: \"10.1234/journal.2025.123\"\n#     relation: \"IsSupplementTo\"\n#     type: \"DOI\"\n</code></pre></p> <p>What Thomas thinks: \"I'll send this template to Dr. Chen with clear instructions on what to add.\"</p> <p>Step 3: Researcher Provides Enrichment</p> <p>Dr. Chen fills out the enrichment file:</p> <pre><code>title: \"Whole genome sequencing of antibiotic-resistant E. coli strains\"\n\nauthors:\n  - name: Dr. Sarah Chen\n    orcid: 0000-0002-1234-5678\n    affiliation: Department of Microbiology, State University\n  - name: Dr. James Park\n    orcid: 0000-0003-9876-5432\n    affiliation: Department of Bioinformatics, State University\n\ndescription: |\n  This dataset contains whole genome sequencing data from 24 clinical isolates\n  of antibiotic-resistant E. coli collected from hospital patients between\n  2023-2024. Samples were sequenced using Illumina NovaSeq 6000 platform with\n  2x150bp paired-end reads.\n\n  Methodology:\n  - DNA extraction: Qiagen DNeasy Blood &amp; Tissue Kit\n  - Library prep: Illumina DNA Prep\n  - Sequencing: NovaSeq 6000, S4 flow cell\n  - Coverage: Average 100x\n  - Quality filtering: fastp v0.23.0 (Q30)\n  - Assembly: SPAdes v3.15.5\n\n  Data includes:\n  - Raw FASTQ files (R1 and R2 for each sample)\n  - Quality control reports\n  - Assembly statistics\n\nkeywords:\n  - whole genome sequencing\n  - Escherichia coli\n  - antibiotic resistance\n  - antimicrobial resistance genes\n  - genomic epidemiology\n  - bacterial genomics\n  - clinical isolates\n\nfunding_references:\n  - funder_name: National Institutes of Health\n    award_number: R01AI123456\n  - funder_name: State University Research Foundation\n    award_number: SURF-2024-789\n\nrelated_identifiers:\n  - identifier: \"10.1234/journal.microbiology.2025.456\"\n    relation: IsSupplementTo\n    type: DOI\n\ntemporal_coverage:\n  start: \"2023-01-01\"\n  end: \"2024-12-31\"\n\nlicense: CC-BY-4.0\n</code></pre> <p>What Thomas thinks: \"Excellent enrichment! Now let me validate the improved metadata.\"</p> <p>Step 4: Validate Enriched Metadata</p> <pre><code># Prepare DOI with enrichment\ncicada doi prepare /repository/submissions/chen-2025-001/sample.fastq.gz \\\n  --enrich /repository/submissions/chen-2025-001/enrichment.yaml \\\n  --publisher \"State University Research Data Repository\"\n</code></pre> <p>Output: <pre><code>DOI Preparation Results\n=======================\n\nFile: sample.fastq.gz\n\nDataset Information:\n  Title: Whole genome sequencing of antibiotic-resistant E. coli strains\n  Authors: 2 (both with ORCIDs)\n    1. Dr. Sarah Chen (ORCID: 0000-0002-1234-5678)\n       Department of Microbiology, State University\n    2. Dr. James Park (ORCID: 0000-0003-9876-5432)\n       Department of Bioinformatics, State University\n  Publisher: State University Research Data Repository\n  Resource Type: Dataset\n  Keywords: 7\n  Funding: 2 grants\n  Related Publications: 1\n  Temporal Coverage: 2023-01-01 to 2024-12-31\n\nValidation:\n  \u2713 Ready for DOI minting\n  Quality Score: 94.0/100 (Excellent)\n  Errors: 0\n  Warnings: 0\n\nSummary:\n  Excellent metadata quality. All required and recommended fields present.\n  Ready for DOI registration and publication.\n\nQuality Improvement: +42 points (52 \u2192 94)\n</code></pre></p> <p>What Thomas thinks: \"Perfect! Quality improved from 52 to 94. This is now ready for DOI minting and repository publication.\"</p> <p>Step 5: Export DataCite Metadata</p> <pre><code># Export final DataCite-compliant metadata\ncicada doi prepare /repository/submissions/chen-2025-001/sample.fastq.gz \\\n  --enrich /repository/submissions/chen-2025-001/enrichment.yaml \\\n  --publisher \"State University Research Data Repository\" \\\n  --format json \\\n  --output /repository/submissions/chen-2025-001/datacite_final.json\n</code></pre> <p>What Thomas thinks: \"Now I can upload this to DataCite and mint the DOI. The whole enrichment process took only 10 minutes instead of 30.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#month-1-workflow-automation-20-minutes","title":"Month 1: Workflow Automation (20 minutes)","text":"<p>Step 6: Create Automated Enrichment Workflow</p> <p>Thomas creates a script to streamline the process:</p> <pre><code>#!/bin/bash\n# enrich_submission.sh\n# Automated metadata enrichment workflow\n\nSUBMISSION_ID=\"$1\"\nSUBMISSION_DIR=\"/repository/submissions/${SUBMISSION_ID}\"\nDATA_FILE=\"${SUBMISSION_DIR}/$(ls ${SUBMISSION_DIR}/*.{fastq,fastq.gz,czi,tif} 2&gt;/dev/null | head -1)\"\n\necho \"=== Processing Submission: ${SUBMISSION_ID} ===\"\n\n# Step 1: Initial quality check\necho \"Step 1: Assessing initial metadata quality...\"\ncicada doi validate \"${DATA_FILE}\" &gt; \"${SUBMISSION_DIR}/initial_quality.txt\"\n\ninitial_score=$(grep \"Quality Score:\" \"${SUBMISSION_DIR}/initial_quality.txt\" | awk '{print $3}')\necho \"Initial quality score: ${initial_score}\"\n\n# Step 2: Generate enrichment template\necho \"Step 2: Generating enrichment template...\"\ncicada doi prepare \"${DATA_FILE}\" \\\n  --format yaml \\\n  --output \"${SUBMISSION_DIR}/enrichment_template.yaml\"\n\necho \"Template saved to: ${SUBMISSION_DIR}/enrichment_template.yaml\"\necho \"\"\necho \"\u2192 Please have researcher fill out enrichment_template.yaml\"\necho \"\u2192 When complete, run: ./finalize_submission.sh ${SUBMISSION_ID}\"\n</code></pre> <p>Finalization script: <pre><code>#!/bin/bash\n# finalize_submission.sh\n# Finalize metadata and mint DOI\n\nSUBMISSION_ID=\"$1\"\nSUBMISSION_DIR=\"/repository/submissions/${SUBMISSION_ID}\"\nDATA_FILE=\"${SUBMISSION_DIR}/$(ls ${SUBMISSION_DIR}/*.{fastq,fastq.gz,czi,tif} 2&gt;/dev/null | head -1)\"\nENRICHMENT=\"${SUBMISSION_DIR}/enrichment.yaml\"\n\necho \"=== Finalizing Submission: ${SUBMISSION_ID} ===\"\n\n# Validate enriched metadata\necho \"Validating enriched metadata...\"\ncicada doi prepare \"${DATA_FILE}\" \\\n  --enrich \"${ENRICHMENT}\" \\\n  --publisher \"State University Research Data Repository\"\n\nif [ $? -ne 0 ]; then\n  echo \"\u2717 Validation failed - check enrichment file\"\n  exit 1\nfi\n\n# Export DataCite metadata\necho \"Exporting DataCite metadata...\"\ncicada doi prepare \"${DATA_FILE}\" \\\n  --enrich \"${ENRICHMENT}\" \\\n  --publisher \"State University Research Data Repository\" \\\n  --format json \\\n  --output \"${SUBMISSION_DIR}/datacite.json\"\n\necho \"\u2713 Ready for DOI minting\"\necho \"DataCite metadata: ${SUBMISSION_DIR}/datacite.json\"\n</code></pre></p> <p>What Thomas thinks: \"This workflow automates the repetitive parts while guiding researchers through enrichment. Time savings: 20 minutes per dataset.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#month-1-key-benefits-for-thomas","title":"Month 1: Key Benefits for Thomas","text":"<p>\u2705 Quality scoring: Track metadata improvements (52 \u2192 94) \u2705 Template generation: Auto-generate enrichment templates \u2705 Validation workflow: Ensure completeness before DOI minting \u2705 Time savings: Reduce enrichment time from 30 to 10 minutes \u2705 Compliance: All datasets meet DataCite and repository standards</p> <p>What Thomas experiences: \"Cicada's metadata enrichment workflow has doubled my throughput. I can now process 20-30 datasets per week instead of 10-15, and the quality is consistently higher. Researchers appreciate the clear guidance on what metadata to provide.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#summary-v020-common-patterns","title":"Summary: v0.2.0 Common Patterns","text":""},{"location":"USER_SCENARIOS_v0.2.0/#all-personas-benefit-from","title":"All Personas Benefit From:","text":"<ol> <li>Automated Metadata Extraction</li> <li>Extract comprehensive metadata from scientific files</li> <li>Support for FASTQ, with more formats coming</li> <li> <p>Rich metadata: quality metrics, read counts, compression, pairing</p> </li> <li> <p>Instrument Preset Validation</p> </li> <li>Validate against instrument-specific requirements</li> <li>8 default presets (Illumina, Zeiss, generic)</li> <li> <p>Clear error messages and recommendations</p> </li> <li> <p>DOI Preparation</p> </li> <li>Assess DOI readiness with quality scoring</li> <li>DataCite Schema v4.5 compliance</li> <li> <p>Metadata enrichment via YAML/JSON</p> </li> <li> <p>Quality Scoring</p> </li> <li>0-100 scale with clear thresholds</li> <li>Track improvements over time</li> <li> <p>Identify missing fields</p> </li> <li> <p>Flexible Output</p> </li> <li>JSON, YAML, or human-readable table</li> <li>Save to files or stdout</li> <li>Integration-ready formats</li> </ol>"},{"location":"USER_SCENARIOS_v0.2.0/#getting-started-with-v020","title":"Getting Started with v0.2.0","text":"<p>Upgrade from v0.1.0: <pre><code># macOS\nbrew upgrade cicada\n\n# Linux\nwget https://github.com/scttfrdmn/cicada/releases/download/v0.2.0/cicada_0.2.0_Linux_x86_64.tar.gz\ntar -xzf cicada_0.2.0_Linux_x86_64.tar.gz\nsudo mv cicada /usr/local/bin/\n</code></pre></p> <p>Quick Start: <pre><code># Extract metadata\ncicada metadata extract your-file.fastq.gz\n\n# Validate against preset\ncicada metadata validate your-file.fastq.gz --preset illumina-novaseq\n\n# Prepare for DOI\ncicada doi prepare your-file.fastq.gz --enrich metadata.yaml\n</code></pre></p> <p>Documentation: - Metadata Extraction Guide - DOI Workflow Guide - Instrument Preset Guide - Provider Setup - Integration Testing</p>"},{"location":"USER_SCENARIOS_v0.2.0/#roadmap-whats-next","title":"Roadmap: What's Next","text":""},{"location":"USER_SCENARIOS_v0.2.0/#v030-planned","title":"v0.3.0 (Planned)","text":"<ul> <li>Additional format extractors (CZI, OME-TIFF, TIFF)</li> <li>More instrument presets</li> <li>Custom preset creation</li> <li>Metadata versioning</li> </ul>"},{"location":"USER_SCENARIOS_v0.2.0/#v040-planned","title":"v0.4.0 (Planned)","text":"<ul> <li>Direct DataCite/Zenodo API integration</li> <li>Automated DOI minting</li> <li>Metadata history tracking</li> <li>Repository integration</li> </ul>"},{"location":"USER_SCENARIOS_v0.2.0/#see-roadmapmd-for-full-roadmap","title":"See ROADMAP.md for full roadmap.","text":""},{"location":"USER_SCENARIOS_v0.2.0/#scenario-5-small-lab-complete-adoption-journey","title":"Scenario 5: Small Lab - Complete Adoption Journey","text":""},{"location":"USER_SCENARIOS_v0.2.0/#lab-profile-thompson-kumar-lab","title":"Lab Profile: Thompson &amp; Kumar Lab","text":"<p>Lab Composition: - 2 PIs: Dr. Rachel Thompson (Cell Biology), Dr. Arun Kumar (Bioinformatics) - Research Staff: 1 lab manager (Lisa), 1 research technician (David) - Postdoc: Dr. Maria Santos (molecular imaging) - Graduate Students: 3 PhD students (Alex, Jordan, Sam) - Undergraduate Students: 2 research assistants (Taylor, Morgan)</p> <p>Infrastructure: - Instruments:   - Zeiss LSM 880 confocal microscope (~3 TB/month)   - Illumina MiSeq sequencer (~2 TB/month) - Current Data: ~12 TB accumulated over 2 years - Current Workflow:   - Local storage on instrument workstations (rapidly filling up)   - Manual copying to external drives for backup   - Analysis on Terra/Dnanexus (PaaS with 100GB free tier)   - Data regularly deleted to free space</p> <p>Pain Points: - \ud83d\udcbe Storage Crisis: Workstations at 95% capacity, deleting old data - \ud83d\udd04 Manual Workflows: Copying files manually is time-consuming - \ud83d\udcb0 PaaS Costs: Paying $500/month for Terra storage, but still limited - \ud83d\udcca Poor Organization: Data scattered across drives, hard to find files - \ud83d\udd0d No Metadata: Can't search for experiments, relying on folder names - \ud83d\udc65 Collaboration Issues: Hard to share data between lab members - \ud83d\udcc4 Publication Delays: Scrambling to find/organize data for papers</p> <p>Budget: $2,000/year for data management</p> <p>Goals: - Centralize all data in S3 (cheaper than PaaS storage) - Automate instrument data upload - Extract and track metadata - Prepare datasets for DOI/publication - Enable lab-wide data access - Reduce manual data management time</p>"},{"location":"USER_SCENARIOS_v0.2.0/#month-1-assessment-and-planning","title":"Month 1: Assessment and Planning","text":"<p>Week 1: Lab Meeting - Understanding the Problem</p> <p>The lab meets to discuss their data crisis:</p> <p>Dr. Thompson: \"We're at 95% capacity on both workstations. We've been deleting old data, but what if we need it later?\"</p> <p>Lisa (Lab Manager): \"I spend 3-4 hours per week manually copying files to external drives. We have 8 different drives now and I'm not even sure what's on half of them.\"</p> <p>Alex (PhD Student): \"I can't find my imaging data from last year. I think it's on one of the external drives, but which one?\"</p> <p>Dr. Kumar: \"We're paying $500/month for Terra storage but it's still not enough. AWS S3 would be much cheaper - about $30/month for 12 TB.\"</p> <p>Decision: Adopt Cicada for centralized S3 storage and automation.</p> <p>Week 2: Calculating Costs and Benefits</p> <p>Lisa creates a cost comparison:</p> <p>Current Costs (Annual): - Terra storage: \\(500/month \u00d7 12 = **\\)6,000/year** - External drives: \\(200 \u00d7 4/year = **\\)800/year** - Lisa's manual labor: 4 hours/week \u00d7 50 weeks \u00d7 \\(30/hour = **\\)6,000/year** - Total: $12,800/year</p> <p>Projected Costs with Cicada (Annual): - AWS S3 storage (15 TB): ~\\(350/year - AWS data transfer: ~\\)200/year - Cicada: Free (open source) - Lisa's time reduced to 1 hour/week: $1,500/year - Total: $2,050/year</p> <p>Savings: $10,750/year (84% reduction)</p> <p>Dr. Kumar: \"This pays for itself in the first month. Let's start implementation.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#month-2-initial-implementation-v010-storage-sync","title":"Month 2: Initial Implementation (v0.1.0 - Storage &amp; Sync)","text":"<p>Week 1: AWS Setup</p> <p>Lisa sets up AWS infrastructure:</p> <pre><code># Create S3 bucket\naws s3 mb s3://thompson-kumar-lab\n\n# Enable versioning (data recovery)\naws s3api put-bucket-versioning \\\n  --bucket thompson-kumar-lab \\\n  --versioning-configuration Status=Enabled\n\n# Set up lifecycle policy (transition to Glacier after 1 year)\ncat &gt; lifecycle.json &lt;&lt;'LIFECYCLE'\n{\n  \"Rules\": [{\n    \"Id\": \"Archive old data\",\n    \"Status\": \"Enabled\",\n    \"Transitions\": [{\n      \"Days\": 365,\n      \"StorageClass\": \"GLACIER\"\n    }]\n  }]\n}\nLIFECYCLE\n\naws s3api put-bucket-lifecycle-configuration \\\n  --bucket thompson-kumar-lab \\\n  --lifecycle-configuration file://lifecycle.json\n</code></pre> <p>Lisa's Note: \"Versioning protects against accidental deletions. Glacier archival will save us money on old data.\"</p> <p>Week 2: Install Cicada on Instruments</p> <p>Microscope Workstation (Ubuntu Linux): <pre><code># Install Cicada\nwget https://github.com/scttfrdmn/cicada/releases/download/v0.1.0/cicada_0.1.0_Linux_x86_64.tar.gz\ntar -xzf cicada_0.1.0_Linux_x86_64.tar.gz\nsudo mv cicada /usr/local/bin/\n\n# Initialize config\ncicada config init\ncicada config set aws.profile lab\ncicada config set aws.region us-west-2\n\n# Test sync\ncicada sync --dry-run \\\n  /data/zeiss/2025-01/ \\\n  s3://thompson-kumar-lab/microscopy/2025-01/\n</code></pre></p> <p>Sequencer Workstation (Windows with WSL): <pre><code># Same installation process\n# Point to different S3 prefix\ncicada sync --dry-run \\\n  /mnt/d/illumina/runs/ \\\n  s3://thompson-kumar-lab/sequencing/runs/\n</code></pre></p> <p>Week 3: Migrate Historical Data</p> <p>Lisa runs a migration to upload all existing data:</p> <pre><code>#!/bin/bash\n# migrate_historical_data.sh\n\necho \"=== Historical Data Migration ===\"\n\n# Microscopy data (6 TB)\necho \"Migrating microscopy data...\"\ncicada sync \\\n  /data/zeiss/archive/ \\\n  s3://thompson-kumar-lab/microscopy/archive/\n\n# Sequencing data (6 TB)\necho \"Migrating sequencing data...\"\ncicada sync \\\n  /mnt/d/illumina/archive/ \\\n  s3://thompson-kumar-lab/sequencing/archive/\n\n# Check total uploaded\naws s3 ls s3://thompson-kumar-lab/ --recursive --summarize\n</code></pre> <p>Result: 12 TB migrated over 3 days (4 TB/day)</p> <p>Cost: $0 for upload (AWS doesn't charge for data ingress)</p> <p>Week 4: Set Up Automatic Watching</p> <p>Microscope - Auto-upload new imaging data: <pre><code># Watch microscope output directory\ncicada watch add \\\n  --debounce 30 \\\n  --min-age 60 \\\n  /data/zeiss/output \\\n  s3://thompson-kumar-lab/microscopy/live\n\n# Create systemd service for persistence\nsudo systemctl enable cicada-watch\nsudo systemctl start cicada-watch\n</code></pre></p> <p>Sequencer - Auto-upload new runs: <pre><code># Watch sequencer output\ncicada watch add \\\n  --debounce 60 \\\n  --min-age 300 \\\n  /mnt/d/illumina/output \\\n  s3://thompson-kumar-lab/sequencing/live\n\n# Windows Task Scheduler for persistence\n# (Run cicada watch start on login)\n</code></pre></p> <p>Dr. Santos (Postdoc): \"I just finished an imaging session and the files are already in S3. I didn't have to do anything!\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#month-3-first-results-and-cleanup","title":"Month 3: First Results and Cleanup","text":"<p>Successes: \u2705 All 12 TB of historical data safely in S3 \u2705 Automatic upload working for both instruments \u2705 No manual file copying needed \u2705 Can delete local copies to free space (verified in S3 first)</p> <p>Storage Freed: - Microscope workstation: 5 TB freed (from 95% to 35% full) - Sequencer workstation: 4 TB freed (from 90% to 40% full) - External drives: 8 drives now in storage (no longer needed)</p> <p>Time Savings: - Lisa: 3 hours/week \u2192 0.5 hours/week (monitoring only) - Saved: 2.5 hours/week = 125 hours/year = $3,750/year</p> <p>Cost Reality Check: - Month 1 AWS bill: \\(35 (12 TB storage + retrieval testing) - **Projected annual: ~\\)420 (vs $6,800 for Terra)**</p> <p>Lab Reaction:</p> <p>Dr. Thompson: \"This is transformative. We're not deleting data anymore, everyone can access files, and it's costing us less than one month of Terra storage.\"</p> <p>Alex (PhD Student): \"I can finally find my old data! It's all organized by date and instrument in S3.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#month-4-adding-metadata-v020","title":"Month 4: Adding Metadata (v0.2.0)","text":"<p>Week 1: Upgrade to v0.2.0</p> <p>Lisa upgrades both workstations:</p> <pre><code># Microscope workstation\nwget https://github.com/scttfrdmn/cicada/releases/download/v0.2.0/cicada_0.2.0_Linux_x86_64.tar.gz\ntar -xzf cicada_0.2.0_Linux_x86_64.tar.gz\nsudo mv cicada /usr/local/bin/\ncicada version  # Verify v0.2.0\n</code></pre> <p>What's New: Metadata extraction, preset validation, DOI preparation</p> <p>Week 2: Extract Metadata from Sequencing Data</p> <p>Jordan (PhD student) needs to document their RNA-seq data:</p> <pre><code># Extract metadata from all FASTQ files\nfor file in /mnt/d/illumina/project-A/*.fastq.gz; do\n  basename=$(basename $file .fastq.gz)\n  cicada metadata extract $file \\\n    --format json \\\n    --output /mnt/d/illumina/project-A/metadata/${basename}.json\ndone\n\n# Create summary report\ncat &gt; summary.sh &lt;&lt;'SCRIPT'\n#!/bin/bash\ntotal_reads=0\nfor json in metadata/*.json; do\n  reads=$(jq '.total_reads' $json)\n  total_reads=$((total_reads + reads))\ndone\necho \"Total reads: $(numfmt --grouping $total_reads)\"\necho \"Total files: $(ls metadata/*.json | wc -l)\"\necho \"Mean quality: $(jq -s 'map(.mean_quality_score) | add / length' metadata/*.json)\"\nSCRIPT\n\nchmod +x summary.sh\n./summary.sh\n</code></pre> <p>Output: <pre><code>Total reads: 324,567,891\nTotal files: 24\nMean quality: 37.2\n</code></pre></p> <p>Jordan: \"Perfect! Now I have all the statistics for my methods section.\"</p> <p>Week 3: Validate Microscopy Data</p> <p>Maria (postdoc) validates imaging data against Zeiss preset:</p> <pre><code># Validate confocal imaging files\ncicada metadata validate \\\n  /data/zeiss/experiment-001.czi \\\n  --preset zeiss-lsm-880\n</code></pre> <p>Output: <pre><code>\u2713 experiment-001.czi: valid (CZI)\n     Quality Score: 85.0/100\n\nValidation Results:\n  Present Fields (12): All required fields present\n  Missing Optional Fields (3):\n    \u2022 objective_na (recommended)\n    \u2022 pixel_size_z (recommended for 3D imaging)\n    \u2022 acquisition_date (helpful for tracking)\n\nRecommendations:\n  Good quality metadata. Consider adding:\n    - Objective numerical aperture\n    - Z-step size for 3D reconstructions\n    - Acquisition timestamp\n</code></pre></p> <p>Maria: \"The metadata looks good, but I should add the objective NA and z-step size for better documentation.\"</p> <p>Week 4: Create Lab-Wide Metadata Script</p> <p>Lisa creates a standard script for all lab members:</p> <pre><code>#!/bin/bash\n# extract_and_upload_metadata.sh\n# Extract metadata and upload alongside data\n\nFILE=\"$1\"\n\necho \"Processing: $(basename $FILE)\"\n\n# Extract metadata\nMETADATA_FILE=\"${FILE}.metadata.json\"\ncicada metadata extract \"$FILE\" --format json --output \"$METADATA_FILE\"\n\n# Upload both file and metadata\ncicada sync \"$FILE\" s3://thompson-kumar-lab/data/\ncicada sync \"$METADATA_FILE\" s3://thompson-kumar-lab/metadata/\n\necho \"\u2713 Data and metadata uploaded\"\n</code></pre> <p>Usage: <pre><code># Any lab member can use this\n./extract_and_upload_metadata.sh experiment_001.fastq.gz\n</code></pre></p> <p>Lab Adoption: All 9 lab members now use this script for their experiments</p>"},{"location":"USER_SCENARIOS_v0.2.0/#month-6-publication-preparation","title":"Month 6: Publication Preparation","text":"<p>Scenario: Sam (PhD student) is preparing first paper</p> <p>Week 1: Dataset Preparation</p> <p>Sam needs to prepare sequencing data for repository submission. Creates enrichment metadata file with: - Title and detailed description - Authors with ORCIDs and affiliations - Keywords - Funding information - Related publication DOI (bioRxiv preprint)</p> <p>Week 2: DOI Preparation</p> <p>Sam prepares dataset for DOI registration:</p> <pre><code># Check DOI readiness\ncicada doi prepare /data/rnaseq/sample_001.fastq.gz \\\n  --enrich paper_dataset_enrichment.yaml \\\n  --publisher \"State University Research Data Repository\"\n</code></pre> <p>Output: <pre><code>DOI Preparation Results\n=======================\n\nFile: sample_001.fastq.gz\n\nDataset Information:\n  Title: RNA-seq analysis of stress response in yeast under oxidative conditions\n  Authors: 2 (both with ORCIDs)\n  Quality Score: 91.0/100 (Excellent)\n\nSummary: Ready for repository submission\n</code></pre></p> <p>Sam: \"Score of 91! I'm ready to submit to the repository.\"</p> <p>Week 3: Export Metadata for Repository</p> <p>Sam exports DataCite metadata and uploads to Dryad, gets DOI, and cites in paper.</p> <p>Reviewers' Response: \"Excellent data availability and documentation. Data is well-organized with comprehensive metadata.\"</p>"},{"location":"USER_SCENARIOS_v0.2.0/#month-9-lab-wide-benefits-assessment","title":"Month 9: Lab-Wide Benefits Assessment","text":"<p>Lab Meeting - 9-Month Review</p> <p>Lisa presents results:</p> <p>Data Management: - \u2705 15 TB now in S3 (12 TB historical + 3 TB new data) - \u2705 Zero data loss (vs. multiple losses before) - \u2705 100% of new data automatically backed up within 1 hour - \u2705 3 datasets published with DOIs</p> <p>Cost Savings: | Item | Before | After | Savings | |------|--------|-------|---------| | Storage (Terra) | $6,000/yr | $0 | $6,000 | | S3 Storage | $0 | \\(450/yr | -\\)450 | | External drives | $800/yr | $0 | $800 | | Labor (Lisa) | \\(6,000/yr | (1,500/yr | \\(4,500 | | **Total** | **\\)12,800/yr** | **\\)1,950/yr** | **\\)10,850/yr** |</p> <p>ROI: Saving $10,850/year (85% reduction)</p>"},{"location":"USER_SCENARIOS_v0.2.0/#year-2-lab-transformation","title":"Year 2: Lab Transformation","text":"<p>New Lab Culture: - Every experiment automatically documented with metadata - All data backed up within minutes of generation - Easy data discovery - Confident publication submissions with DOI-ready datasets - New students onboard in 15 minutes</p> <p>Unexpected Benefits: 1. Grant Writing: Can easily quantify data generation 2. Compliance: NIH data sharing plan compliance is trivial 3. Collaboration: Easy to share data with external collaborators 4. Student Training: Students learn best practices 5. Lab Reputation: Known for excellent data documentation</p>"},{"location":"USER_SCENARIOS_v0.2.0/#key-takeaways-small-lab-adoption","title":"Key Takeaways: Small Lab Adoption","text":""},{"location":"USER_SCENARIOS_v0.2.0/#success-factors","title":"Success Factors","text":"<ol> <li>Start Small: Begin with v0.1.0, add features gradually</li> <li>Clear ROI: Calculate cost savings to justify adoption</li> <li>Lab Buy-in: Get PIs and all members on board</li> <li>Standardize: Create lab-wide scripts for common tasks</li> <li>Document: Maintain simple guides for lab members</li> </ol>"},{"location":"USER_SCENARIOS_v0.2.0/#timeline","title":"Timeline","text":"<ul> <li>Month 1: Planning and setup</li> <li>Month 2: v0.1.0 implementation (storage/sync)</li> <li>Month 3: Verify success, clean up local storage</li> <li>Month 4: Add v0.2.0 features (metadata/DOI)</li> <li>Month 6: First publication with DOI</li> <li>Month 12: Full lab transformation</li> </ul>"},{"location":"USER_SCENARIOS_v0.2.0/#costs-annual","title":"Costs (Annual)","text":"Phase Cost Benefit Before Cicada $12,800 Constant data crisis After Cicada $1,950 15TB organized, automated, DOI-ready Savings $10,850 85% reduction + peace of mind"},{"location":"USER_SCENARIOS_v0.2.0/#realistic-expectations","title":"Realistic Expectations","text":"<p>What Cicada Does Today (v0.2.0): - \u2705 Automated S3 backup - \u2705 FASTQ metadata extraction - \u2705 Preset validation (Illumina, Zeiss) - \u2705 DOI preparation - \u2705 Multi-format output (JSON, YAML, table)</p> <p>What Requires Workarounds: - \u26a0\ufe0f CZI/microscopy metadata (use manual documentation until v0.3.0) - \u26a0\ufe0f Direct DOI minting (use Dryad/Zenodo web interface with prepared metadata) - \u26a0\ufe0f Real-time collaboration (use presigned URLs until access control in v0.5.0)</p> <p>Key Message: Even without every feature, Cicada provides immediate value. As new versions release, labs can adopt additional capabilities incrementally.</p>"},{"location":"USER_SCENARIOS_v0.2.0/#small-lab-adoption-checklist","title":"Small Lab Adoption Checklist","text":""},{"location":"USER_SCENARIOS_v0.2.0/#prerequisites","title":"Prerequisites","text":"<ul> <li> AWS account with S3 access</li> <li> 2-4 hours for initial setup</li> <li> Lab-wide agreement to adopt</li> <li> Basic command line comfort (at least one person)</li> </ul>"},{"location":"USER_SCENARIOS_v0.2.0/#month-1-assessment","title":"Month 1: Assessment","text":"<ul> <li> Document current data volumes</li> <li> Calculate current costs (storage, labor)</li> <li> Identify pain points</li> <li> Present Cicada proposal to PIs</li> </ul>"},{"location":"USER_SCENARIOS_v0.2.0/#month-2-implementation-v010","title":"Month 2: Implementation (v0.1.0)","text":"<ul> <li> Set up AWS S3 bucket with versioning</li> <li> Install Cicada on instrument workstations</li> <li> Test sync with small dataset</li> <li> Migrate historical data</li> <li> Set up automatic watches</li> <li> Create systemd/launchd services</li> </ul>"},{"location":"USER_SCENARIOS_v0.2.0/#month-3-validation","title":"Month 3: Validation","text":"<ul> <li> Verify all data in S3</li> <li> Test data retrieval</li> <li> Delete local copies (keep local cache)</li> <li> Document lab procedures</li> <li> Train all lab members</li> </ul>"},{"location":"USER_SCENARIOS_v0.2.0/#month-4-metadata-v020","title":"Month 4: Metadata (v0.2.0)","text":"<ul> <li> Upgrade to v0.2.0</li> <li> Extract metadata from key datasets</li> <li> Create metadata extraction scripts</li> <li> Validate against presets</li> <li> Document metadata workflows</li> </ul>"},{"location":"USER_SCENARIOS_v0.2.0/#month-6-publication","title":"Month 6: Publication","text":"<ul> <li> Prepare dataset for DOI</li> <li> Create enrichment metadata</li> <li> Validate DOI readiness</li> <li> Submit to repository</li> <li> Update paper materials section</li> </ul>"},{"location":"USER_SCENARIOS_v0.2.0/#month-12-review","title":"Month 12: Review","text":"<ul> <li> Calculate actual costs and savings</li> <li> Assess time savings</li> <li> Survey lab member satisfaction</li> <li> Plan for future Cicada versions</li> <li> Share success with department/university</li> </ul>"},{"location":"cli-reference/","title":"Cicada CLI Reference","text":"<p>Complete command-line interface reference with examples for all major workflows.</p>"},{"location":"cli-reference/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation &amp; Setup</li> <li>Basic Commands</li> <li>Data Sync</li> <li>Metadata Management</li> <li>Workflows</li> <li>Workstations</li> <li>DOI Management</li> <li>User &amp; Project Management</li> <li>Cost Management</li> <li>Compliance</li> </ol>"},{"location":"cli-reference/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"cli-reference/#install-cicada","title":"Install Cicada","text":"<pre><code># macOS / Linux (via Homebrew)\nbrew install cicada\n\n# Or download binary directly\ncurl -sSL https://cicada.sh/install | sh\n\n# Or build from source\ngit clone https://github.com/your-org/cicada.git\ncd cicada\nmake install\n\n# Verify installation\ncicada version\n</code></pre>"},{"location":"cli-reference/#initial-setup","title":"Initial Setup","text":"<pre><code># Interactive setup wizard\ncicada init\n\n# Example output:\n# Welcome to Cicada! Let's set up your lab's data management.\n# \n# Step 1: AWS Configuration\n#   Do you have AWS credentials? [Y/n] y\n#   AWS Access Key ID: ****************\n#   AWS Secret Access Key: ****************\n#   Default region [us-east-1]: us-west-2\n#   \u2713 Credentials validated\n# \n# Step 2: Storage Configuration\n#   Lab name (for bucket naming): smith-lab\n#   Create S3 bucket? [Y/n] y\n#   Bucket name [smith-lab-data-20241122]: \n#   Enable versioning (recommended)? [Y/n] y\n#   Enable intelligent tiering? [Y/n] y\n#   \u2713 Bucket created: s3://smith-lab-data-20241122\n# \n# Step 3: Cost Controls\n#   Monthly budget alert threshold: $100\n#   Email for alerts: pi@university.edu\n#   \u2713 CloudWatch budget alert configured\n# \n# Setup complete!\n\n# Or non-interactive\ncicada init \\\n  --lab-name smith-lab \\\n  --region us-west-2 \\\n  --bucket smith-lab-data \\\n  --budget 100 \\\n  --email pi@university.edu\n</code></pre>"},{"location":"cli-reference/#configuration","title":"Configuration","text":"<pre><code># View current configuration\ncicada config show\n\n# Edit configuration\ncicada config edit\n\n# Set specific values\ncicada config set sync.concurrency 20\ncicada config set cost.budget_limit 150\ncicada config set auth.provider globus\n</code></pre>"},{"location":"cli-reference/#basic-commands","title":"Basic Commands","text":""},{"location":"cli-reference/#status-information","title":"Status &amp; Information","text":"<pre><code># Show overall status\ncicada status\n\n# Example output:\n# Cicada Status\n# Lab: smith-lab\n# Bucket: smith-lab-data-20241122\n# Region: us-west-2\n# \n# Storage:\n#   Total: 12.3 TB\n#   This month: $78.40\n# \n# Active Resources:\n#   \u2022 Daemon: running (PID 12345)\n#   \u2022 Watches: 2 locations\n#   \u2022 Workflows: 1 running\n#   \u2022 Workstations: 0 active\n# \n# Recent Activity:\n#   \u2022 2m ago: 45 files synced (8.2 GB)\n#   \u2022 1h ago: Workflow completed\n#   \u2022 3h ago: User added to project\n\n# Check daemon status\ncicada daemon status\n\n# View logs\ncicada logs --tail 50 --follow\n</code></pre>"},{"location":"cli-reference/#data-sync","title":"Data Sync","text":""},{"location":"cli-reference/#manual-sync","title":"Manual Sync","text":"<pre><code># Basic sync (upload local to S3)\ncicada sync /local/data s3://lab-bucket/data/\n\n# Sync with options\ncicada sync /local/data s3://lab-bucket/data/ \\\n  --dry-run \\\n  --checksum \\\n  --delete \\\n  --exclude \"*.tmp\" \\\n  --exclude \".DS_Store\" \\\n  --concurrency 10 \\\n  --bandwidth-limit 50MB\n\n# Bidirectional sync\ncicada sync /local/data s3://lab-bucket/data/ --bidirectional\n\n# Download from S3\ncicada sync s3://lab-bucket/data/ /local/data/\n\n# Show sync statistics\ncicada sync --stats s3://lab-bucket/data/\n\n# Example output:\n# Sync Statistics: s3://lab-bucket/data/\n# Files: 1,247\n# Total size: 15.3 GB\n# Last modified: 2 hours ago\n# Storage class:\n#   - Standard: 500 GB\n#   - Intelligent-Tiering: 14.8 GB\n</code></pre>"},{"location":"cli-reference/#automated-watching","title":"Automated Watching","text":"<pre><code># Add a watch location\ncicada watch add microscope-1 \\\n  --path /Volumes/ZeissMicroscope/Export \\\n  --destination s3://lab-bucket/raw/microscopy/ \\\n  --sync-on-new \\\n  --min-age 5m \\\n  --delete-source\n\n# With schedule\ncicada watch add sequencer \\\n  --path /data/sequencer/output \\\n  --destination s3://lab-bucket/raw/sequencing/ \\\n  --sync-schedule \"0 2 * * *\"  # 2 AM daily\n\n# List watches\ncicada watch list\n\n# Example output:\n# Active Watches (2):\n# \n#   microscope-1\n#     Path: /Volumes/ZeissMicroscope/Export\n#     Destination: s3://lab-bucket/raw/microscopy/\n#     Trigger: on new files (min age: 5m)\n#     Delete source: yes\n#     Status: watching (last sync: 3m ago)\n# \n#   sequencer\n#     Path: /data/sequencer/output\n#     Destination: s3://lab-bucket/raw/sequencing/\n#     Schedule: daily at 2:00 AM\n#     Status: idle (next sync: today 2:00 AM)\n\n# Show watch details\ncicada watch status microscope-1\n\n# Pause/resume watch\ncicada watch pause microscope-1\ncicada watch resume microscope-1\n\n# Remove watch\ncicada watch remove microscope-1\n</code></pre>"},{"location":"cli-reference/#daemon-management","title":"Daemon Management","text":"<pre><code># Start daemon (required for watches)\ncicada daemon start\n\n# Start with web UI\ncicada daemon start --web --port 7878\n\n# Stop daemon\ncicada daemon stop\n\n# Restart daemon\ncicada daemon restart\n\n# View daemon logs\ncicada daemon logs --follow\n</code></pre>"},{"location":"cli-reference/#metadata-management","title":"Metadata Management","text":""},{"location":"cli-reference/#schema-management","title":"Schema Management","text":"<pre><code># List available schemas\ncicada metadata schema list\n\n# Search for schemas\ncicada metadata schema search \"microscopy\"\n\n# Example output:\n# Community Schemas (3 results):\n#   1. fluorescence-microscopy (smith-lab)\n#      \u2605\u2605\u2605\u2605\u2606 23 labs using\n#      \"Comprehensive metadata for fluorescence microscopy\"\n# \n#   2. super-resolution-microscopy (chen-lab)\n#      \u2605\u2605\u2605\u2606\u2606 8 labs using\n#      \"Extended fields for PALM/STORM imaging\"\n# \n#   3. live-cell-imaging (williams-lab)\n#      \u2605\u2605\u2605\u2605\u2605 45 labs using\n#      \"Time-lapse microscopy metadata\"\n\n# View schema details\ncicada metadata schema show fluorescence-microscopy\n\n# Install community schema\ncicada metadata schema install live-cell-imaging --from williams-lab\n\n# Create custom schema\ncicada metadata schema create my-experiment \\\n  --template microscopy-basic \\\n  --edit\n\n# Apply schema to folder\ncicada metadata schema apply fluorescence-microscopy \\\n  --path s3://lab-bucket/raw/microscopy/\n\n# Validate schema\ncicada metadata schema validate my-experiment.yaml\n</code></pre>"},{"location":"cli-reference/#working-with-metadata","title":"Working with Metadata","text":"<pre><code># Upload with automatic metadata extraction\ncicada upload experiment_001.czi s3://lab-bucket/raw/microscopy/\n\n# Example interaction:\n# Uploading: experiment_001.czi (2.3 GB)\n# \n# Auto-extracted metadata:\n#   \u2713 Instrument: Zeiss LSM 980\n#   \u2713 Magnification: 63x\n#   \u2713 Dimensions: 2048x2048x45x100\n#   \u2713 Channels: 4 (DAPI, GFP, RFP, Cy5)\n# \n# Please provide additional metadata:\n# Sample ID: [WT-001]\n# Strain: [BY4741]\n# Treatment: [1) control  2) heat-shock  3) drug-A]\n# Choose: [2]\n# Duration (min): [30]\n# \n# \u2713 Metadata saved\n\n# Upload with metadata file\ncicada upload experiment_002.czi \\\n  --metadata metadata.json \\\n  s3://lab-bucket/raw/microscopy/\n\n# Upload with template\ncicada upload experiment_003.czi \\\n  --template heat-shock-standard \\\n  --override sample_id=WT-003 \\\n  s3://lab-bucket/raw/microscopy/\n\n# Batch upload with CSV\ncicada upload --batch samples.csv \\\n  --files-dir /path/to/experiments/ \\\n  s3://lab-bucket/raw/microscopy/\n\n# View metadata\ncicada metadata show s3://lab-bucket/raw/microscopy/experiment_001.czi\n\n# Edit metadata\ncicada metadata edit s3://lab-bucket/raw/microscopy/experiment_001.czi\n\n# Export metadata\ncicada metadata export s3://lab-bucket/project/final_data/ \\\n  --format datacite \\\n  --output metadata/\n\n# Other formats: isa-tab, dats-json, frictionless, ro-crate\n\n# Search by metadata\ncicada search \\\n  --schema fluorescence-microscopy \\\n  --where \"treatment.condition=heat-shock\" \\\n  --where \"magnification=63\" \\\n  --where \"date&gt;=2024-11-01\"\n\n# Quality check\ncicada metadata quality-check s3://lab-bucket/project/\n\n# Example output:\n# Metadata Quality Report\n# Overall Score: 87/100 (Good)\n# \n# Completeness: 92/100\n#   \u2713 1,234 files have complete metadata\n#   \u26a0 45 files missing \"replication\" fields\n# \n# Recommendations:\n#   1. Add protocol IDs \u2192 [Fix]\n#   2. Standardize strain names \u2192 [Fix automatically]\n</code></pre>"},{"location":"cli-reference/#metadata-templates","title":"Metadata Templates","text":"<pre><code># Save current metadata as template\ncicada metadata template save \\\n  --from experiment_001.czi \\\n  --name heat-shock-standard \\\n  --fields strain,treatment,protocol_id\n\n# List templates\ncicada metadata template list\n\n# Use template\ncicada upload experiment_new.czi \\\n  --template heat-shock-standard \\\n  s3://lab-bucket/raw/\n</code></pre>"},{"location":"cli-reference/#workflows","title":"Workflows","text":""},{"location":"cli-reference/#running-workflows","title":"Running Workflows","text":"<pre><code># Run Snakemake workflow\ncicada workflow run snakemake \\\n  --snakefile Snakefile \\\n  --config input=s3://lab-bucket/raw/experiment_123/ \\\n  --cores 32 \\\n  --memory 64GB \\\n  --spot\n\n# Run Nextflow workflow\ncicada workflow run nextflow \\\n  --workflow pipeline.nf \\\n  --input \"s3://lab-bucket/raw/**.fastq.gz\" \\\n  --outdir s3://lab-bucket/results/run_456/\n\n# Run with config file\ncicada workflow run --config cicada-workflow.yaml\n\n# Example cicada-workflow.yaml:\n# name: image-processing-pipeline\n# engine: snakemake\n# workflow: Snakefile\n# compute:\n#   type: batch\n#   instance_types: [c5.4xlarge, c5.9xlarge]\n#   spot: true\n#   max_vcpus: 256\n# storage:\n#   input: s3://lab-bucket/raw/\n#   output: s3://lab-bucket/processed/\n# cost_limit: 50\n\n# Test workflow locally first\ncicada workflow run snakemake \\\n  --snakefile Snakefile \\\n  --local \\\n  --cores 4\n\n# Monitor running workflows\ncicada workflow list\n\n# Example output:\n# Active Workflows (2):\n# \n#   cell-segmentation (run_456)\n#     Started: 15 minutes ago\n#     Progress: 12/45 steps (26%)\n#     Cost so far: $2.15\n#     ETA: 30 minutes\n#     [View logs] [Stop]\n# \n#   rnaseq-pipeline (run_457)\n#     Started: 2 hours ago\n#     Status: Queued (waiting for resources)\n\n# View workflow details\ncicada workflow status run_456\n\n# View logs\ncicada workflow logs run_456 --follow\n\n# Stop workflow\ncicada workflow stop run_456\n\n# Enable auto-workflow on new data\ncicada workflow enable auto-pipeline\n\n# Example auto-pipeline.yaml:\n# name: auto-fastq-processing\n# trigger:\n#   watch: /data/sequencer/output/\n#   pattern: \"*.fastq.gz\"\n#   min_files: 2  # Wait for R1 and R2\n# workflow:\n#   engine: nextflow\n#   pipeline: nf-core/rnaseq\n</code></pre>"},{"location":"cli-reference/#workstations","title":"Workstations","text":""},{"location":"cli-reference/#launching-remote-workstations","title":"Launching Remote Workstations","text":"<pre><code># Launch basic workstation\ncicada workstation launch viz-session\n\n# Launch with specific configuration\ncicada workstation launch viz-session \\\n  --instance g4dn.xlarge \\\n  --image napari-workstation \\\n  --storage s3://lab-bucket/data/experiment_123/ \\\n  --spot\n\n# Example output:\n# Launching workstation...\n# Instance starting: i-0abc123def456\n# Waiting for connection... ready!\n# \n# Connect via:\n#   Web:  https://viz-session-abc123.cicada.cloud\n#   VNC:  vnc://54.123.45.67:5901\n#   SSH:  ssh cicada@54.123.45.67\n# \n# Data mounted at: /data\n# Auto-shutdown: 2 hours of inactivity\n# Cost: ~$0.50/hour (spot)\n\n# List available images\ncicada workstation images\n\n# Example output:\n# Available Workstation Images:\n#   - basic-linux      Ubuntu 22.04, basic tools\n#   - imagej           ImageJ, FIJI, common plugins\n#   - matlab           MATLAB R2024a (requires license)\n#   - napari           Napari, Python scientific stack\n#   - paraview         ParaView, VTK visualization\n#   - rstudio          RStudio Server, tidyverse\n#   - jupyter          JupyterLab, scipy stack\n\n# Build custom image\ncicada workstation build \\\n  --from napari \\\n  --add-package cellpose \\\n  --add-package stardist \\\n  --name my-workstation\n\n# List active sessions\ncicada workstation list\n\n# Example output:\n# Active Sessions (1):\n# \n#   viz-session\n#     Instance: g4dn.xlarge (GPU)\n#     Running: 1h 23m\n#     Cost: $0.68\n#     Auto-shutdown: in 37 minutes\n#     URL: https://viz-session-abc123.cicada.cloud\n\n# Reconnect to session\ncicada workstation connect viz-session\n\n# Extend auto-shutdown\ncicada workstation extend viz-session --hours 4\n\n# Stop workstation\ncicada workstation stop viz-session\n\n# Resume stopped workstation\ncicada workstation start viz-session\n\n# Snapshot workstation\ncicada workstation snapshot viz-session \\\n  --name \"before-analysis\"\n\n# Launch from snapshot\ncicada workstation launch \\\n  --from-snapshot \"before-analysis\"\n</code></pre>"},{"location":"cli-reference/#doi-management","title":"DOI Management","text":""},{"location":"cli-reference/#minting-dois","title":"Minting DOIs","text":"<pre><code># Prepare dataset for publication\ncicada dataset prepare s3://lab-bucket/project/final_data/ \\\n  --title \"Protein localization under heat shock\" \\\n  --authors \"Smith J, Garcia A, Chen L\" \\\n  --description description.md \\\n  --keywords \"protein localization, heat shock, yeast\" \\\n  --license CC-BY-4.0\n\n# Validate dataset readiness\ncicada dataset validate s3://lab-bucket/project/final_data/\n\n# Example output:\n# Dataset Validation\n# \u2713 All files have metadata\n# \u2713 README present\n# \u2713 Protocols documented\n# \u2713 Code available (GitHub linked)\n# \u2713 FAIR principles satisfied\n# \n# Ready to publish!\n\n# Mint DOI and publish\ncicada dataset publish s3://lab-bucket/project/final_data/ \\\n  --mint-doi \\\n  --notify-coauthors\n\n# Example output:\n# Minting DOI...\n# \u2713 DOI registered: 10.12345/cicada.smith-lab.2024.001\n# \n# Landing page: https://smithlab.cicada.sh/doi/10.12345/...\n# Citation: Smith J, Garcia A, Chen L (2024). Protein\n#   localization under heat shock. Smith Lab Data Repository.\n#   https://doi.org/10.12345/cicada.smith-lab.2024.001\n# \n# Data availability statement (copy for manuscript):\n# \"Data are available at https://doi.org/10.12345/...\n#  under a CC-BY-4.0 license.\"\n# \n# Cost: $1.00 (charged to lab account)\n\n# Publish with embargo\ncicada dataset publish s3://lab-bucket/project/final_data/ \\\n  --mint-doi \\\n  --embargo-until 2025-06-01 \\\n  --embargo-type metadata-only\n\n# List lab DOIs\ncicada doi list\n\n# Example output:\n# Lab DOIs (3):\n#   10.12345/cicada.smith-lab.2024.001\n#     Title: Protein localization under heat shock\n#     Status: Public\n#     Downloads: 47\n#     Citations: 3\n# \n#   10.12345/cicada.smith-lab.2024.002\n#     Title: Cell division time-lapse\n#     Status: Embargo (until 2025-06-01)\n\n# View DOI details\ncicada doi show 10.12345/cicada.smith-lab.2024.001\n\n# Update DOI metadata\ncicada doi update 10.12345/cicada.smith-lab.2024.001 \\\n  --add-author \"Williams R\" \\\n  --version 1.1\n\n# Configure DOI provider\ncicada lab configure-doi --provider datacite\n# or --provider zenodo\n# or --provider institution\n</code></pre>"},{"location":"cli-reference/#user-project-management","title":"User &amp; Project Management","text":""},{"location":"cli-reference/#user-management","title":"User Management","text":"<pre><code># Add user to lab\ncicada user add jsmith@university.edu \\\n  --role postdoc \\\n  --groups protein-structure,methods-dev\n\n# List users\ncicada user list\n\n# Example output:\n# Lab Members (5):\n#   \ud83d\udc64 pi@university.edu (PI, admin)\n#   \ud83d\udc64 jsmith@university.edu (Postdoc)\n#      Groups: protein-structure, methods-dev\n#   \ud83d\udc64 agarcia@university.edu (Grad Student)\n#      Groups: metabolism\n\n# Remove user\ncicada user remove jsmith@university.edu\n\n# Modify user permissions\ncicada user update jsmith@university.edu \\\n  --add-group new-project \\\n  --role senior-researcher\n</code></pre>"},{"location":"cli-reference/#project-management","title":"Project Management","text":"<pre><code># Create project\ncicada project create NIH-R01-2024 \\\n  --description \"Protein structure determination\" \\\n  --members pi@university.edu,jsmith@university.edu \\\n  --budget 200\n\n# List projects\ncicada project list\n\n# Show project details\ncicada project info NIH-R01-2024\n\n# Example output:\n# Project: NIH-R01-2024\n# Description: Protein structure determination\n# \n# Storage:\n#   Total: 2.3 TB\n#   Cost: $45/month\n#   Growth: +50 GB/week\n# \n# Members (3):\n#   \ud83d\udc64 pi@university.edu (admin)\n#   \ud83d\udc64 jsmith@university.edu (read-write)\n#   \ud83d\udc64 external@otheruniv.edu (read-only)\n# \n# Recent Activity:\n#   \u2022 2h ago: jsmith uploaded 45 files\n#   \u2022 1d ago: workflow completed\n#   \u2022 3d ago: external accessed dataset_v3\n\n# Add member to project\ncicada project add-member NIH-R01-2024 external@otheruniv.edu \\\n  --permission read-only\n\n# Remove member\ncicada project remove-member NIH-R01-2024 external@otheruniv.edu\n</code></pre>"},{"location":"cli-reference/#cost-management","title":"Cost Management","text":"<pre><code># View cost report\ncicada cost report\n\n# Example output:\n# Cost Report: November 2024\n# \n# Storage:\n#   S3 Standard:              $12.30  (500 GB)\n#   S3 Intelligent-Tiering:   $45.80  (6.2 TB)\n#   S3 Glacier:               $8.20   (2.1 TB)\n# \n# Compute:\n#   Batch (spot):             $3.45   (28 vCPU-hours)\n#   Workstations:             $12.60  (24 instance-hours)\n# \n# Transfer:\n#   Data egress:              $2.15   (24 GB)\n# \n# Total: $84.50 / $100.00 budget\n# \n# Recommendations:\n#   \u26a1 Archive data &gt;2 years \u2192 save $15/month\n#   \ud83d\udca1 Use smaller instances \u2192 save $5/month\n\n# Detailed cost breakdown\ncicada cost breakdown --month 2024-11\n\n# Cost prediction\ncicada cost predict --action \"archive data older than 2 years\"\n\n# Example output:\n# Estimated savings: $15.20/month\n# One-time cost: $2.50\n# Payback: 1 month\n\n# Set budget alerts\ncicada cost set-budget 100 --alert-at 80\n\n# View by project\ncicada cost by-project\n</code></pre>"},{"location":"cli-reference/#compliance","title":"Compliance","text":"<pre><code># Enable compliance mode\ncicada lab configure --compliance nist-800-171\n\n# Generate compliance report\ncicada compliance report --standard nist-800-171\n\n# View audit log\ncicada audit log --user jsmith --last 30d\n\n# Export audit logs\ncicada audit export --format csv --year 2024\n\n# Scan for sensitive data\ncicada scan s3://lab-bucket/data/ --detect-sensitive\n\n# Example output:\n# \u26a0\ufe0f  Found sensitive data:\n#   - 15 files contain SSN patterns\n#   - 8 files contain credit card numbers\n#   - 142 files contain dates of birth\n# \n# Recommendations:\n#   1. Apply stricter access controls\n#   2. Enable field-level encryption\n\n# Verify file integrity\ncicada verify s3://lab-bucket/data/experiment.tif\n</code></pre>"},{"location":"cli-reference/#advanced-examples","title":"Advanced Examples","text":""},{"location":"cli-reference/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code># 1. Initial setup\ncicada init --lab-name smith-lab\n\n# 2. Configure automatic syncing\ncicada watch add microscope \\\n  --path /Volumes/Microscope/Export \\\n  --destination s3://smith-lab-data/raw/microscopy/ \\\n  --sync-on-new \\\n  --delete-source\n\n# 3. Upload existing data with metadata\ncicada upload /archive/old_data/ \\\n  s3://smith-lab-data/archive/ \\\n  --batch metadata.csv\n\n# 4. Run analysis workflow\ncicada workflow run snakemake \\\n  --snakefile analysis.smk \\\n  --config experiment=exp_123 \\\n  --spot\n\n# 5. Visualize results\ncicada workstation launch \\\n  --image napari-workstation \\\n  --data s3://smith-lab-data/processed/exp_123/\n\n# 6. Prepare for publication\ncicada dataset prepare s3://smith-lab-data/final/ \\\n  --title \"My Research Dataset\" \\\n  --authors \"Smith J, et al\"\n\n# 7. Mint DOI and publish\ncicada dataset publish s3://smith-lab-data/final/ \\\n  --mint-doi \\\n  --license CC-BY-4.0\n\n# 8. Monitor costs\ncicada cost report\n</code></pre>"},{"location":"cli-reference/#getting-help","title":"Getting Help","text":"<pre><code># General help\ncicada --help\n\n# Command-specific help\ncicada sync --help\ncicada workflow --help\n\n# Version info\ncicada version\n\n# Check for updates\ncicada update --check\n</code></pre>"},{"location":"domain-schemas/","title":"Domain-Specific Metadata Schemas","text":"<p>This document provides detailed metadata schemas for various research domains, demonstrating Cicada's flexibility.</p>"},{"location":"domain-schemas/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Microscopy &amp; Imaging</li> <li>Genomics &amp; Sequencing</li> <li>Proteomics &amp; Mass Spectrometry</li> <li>Flow Cytometry</li> <li>Chromatography (HPLC/GC)</li> <li>Spectroscopy (NMR, IR, UV-Vis)</li> <li>X-Ray Crystallography</li> <li>Electron Microscopy</li> <li>Behavioral Studies</li> <li>Clinical Trials</li> <li>Environmental Sampling</li> <li>Materials Science</li> </ol>"},{"location":"domain-schemas/#1-microscopy-imaging","title":"1. Microscopy &amp; Imaging","text":""},{"location":"domain-schemas/#fluorescence-microscopy-schema","title":"Fluorescence Microscopy Schema","text":"<pre><code># schemas/microscopy/fluorescence.yaml\nschema_version: \"1.0\"\nname: fluorescence-microscopy\ndescription: Comprehensive metadata for fluorescence microscopy experiments\ndomain: imaging\n\n# See full schema in previous artifact\n</code></pre>"},{"location":"domain-schemas/#2-genomics-sequencing","title":"2. Genomics &amp; Sequencing","text":""},{"location":"domain-schemas/#rna-seq-schema","title":"RNA-seq Schema","text":"<pre><code># schemas/sequencing/rnaseq.yaml\n# See full schema in previous artifact\n</code></pre>"},{"location":"domain-schemas/#3-proteomics-mass-spectrometry","title":"3. Proteomics &amp; Mass Spectrometry","text":"<pre><code># schemas/proteomics/mass-spec.yaml\n# See full schema in previous artifact\n</code></pre>"},{"location":"domain-schemas/#4-flow-cytometry","title":"4. Flow Cytometry","text":"<pre><code># schemas/cytometry/flow-cytometry.yaml\n# See full schema in previous artifact\n</code></pre>"},{"location":"domain-schemas/#5-chromatography-hplcgc","title":"5. Chromatography (HPLC/GC)","text":"<pre><code># schemas/chemistry/chromatography.yaml\n# See full schema in previous artifact\n</code></pre>"},{"location":"domain-schemas/#6-spectroscopy-nmr","title":"6. Spectroscopy (NMR)","text":"<pre><code># schemas/spectroscopy/nmr.yaml\n# See full schema in previous artifact\n</code></pre> <p>For complete schemas, see individual YAML files in the repository.</p>"},{"location":"v0.2.0-progress/","title":"Cicada v0.2.0 Development Progress","text":"<p>Branch: <code>feature/v0.2.0-metadata-foundation</code> Started: 2025-11-23 Status: 87.5% Complete (7 of 8 issues closed)</p>"},{"location":"v0.2.0-progress/#milestone-summary","title":"Milestone Summary","text":"<p>Phase 2: Metadata &amp; Data Management</p> Issue Status Description #17 \u2705 CLOSED Define Metadata Core Types #18 \u2705 CLOSED Implement Extractor Interface #19 \u2705 CLOSED S3 Object Tagging Integration #20 \u2705 CLOSED Implement Zeiss CZI Extractor #21 \u2705 CLOSED CLI Integration for Metadata Extraction #22 \u2705 CLOSED Instrument Preset System #23 \u2705 CLOSED Pluggable DOI Provider System #25 \ud83d\udd04 OPEN Documentation &amp; Testing <p>Progress: \u215e issues complete (87.5%)</p>"},{"location":"v0.2.0-progress/#issue-17-define-metadata-core-types-closed","title":"Issue #17: Define Metadata Core Types \u2705 CLOSED","text":"<p>Status: Complete Closed: 2025-11-24</p>"},{"location":"v0.2.0-progress/#implementation","title":"Implementation","text":"<p>Created Files: - <code>internal/metadata/types.go</code> (600+ lines)   - 6 instrument-specific metadata structures   - MicroscopyMetadata (30+ fields)   - SequencingMetadata (30+ fields)   - MassSpecMetadata, FlowCytometryMetadata, CryoEMMetadata, XRayMetadata   - Full JSON/YAML serialization support</p> <p>Test Coverage: - <code>internal/metadata/types_test.go</code> (600+ lines)   - 14 comprehensive test functions   - JSON and YAML serialization tests   - All tests passing</p> <p>Key Features: - Type-safe instrument-specific structures - Extensive field coverage for each instrument type - JSON/YAML marshaling with proper tags - <code>omitempty</code> for optional fields</p>"},{"location":"v0.2.0-progress/#issue-18-implement-extractor-interface-closed","title":"Issue #18: Implement Extractor Interface \u2705 CLOSED","text":"<p>Status: Complete Closed: 2025-11-24</p>"},{"location":"v0.2.0-progress/#implementation_1","title":"Implementation","text":"<p>The <code>Extractor</code> interface is fully implemented with registry system:</p> <pre><code>type Extractor interface {\n    CanHandle(filename string) bool\n    Extract(filepath string) (map[string]interface{}, error)\n    ExtractFromReader(r io.Reader, filename string) (map[string]interface{}, error)\n    Name() string\n    SupportedFormats() []string\n}\n</code></pre> <p>Extractors Implemented (14 total): - TIFF, OME-TIFF - Zeiss CZI, Nikon ND2, Leica LIF - FASTQ, BAM - mzML, MGF - HDF5, Zarr - DICOM, FCS - Generic (fallback)</p> <p>Test Coverage: - <code>internal/metadata/extractor_test.go</code> (324 lines)   - 19 test functions   - Registry operations tested   - GenericExtractor tests   - All tests passing</p>"},{"location":"v0.2.0-progress/#issue-19-s3-object-tagging-integration-closed","title":"Issue #19: S3 Object Tagging Integration \u2705 CLOSED","text":"<p>Status: Complete Closed: 2025-11-24</p>"},{"location":"v0.2.0-progress/#implementation_2","title":"Implementation","text":"<p>Files Created/Updated: - <code>internal/metadata/s3tags.go</code> - S3 tag conversion - <code>internal/sync/s3.go</code> - S3 backend with tagging - <code>internal/metadata/s3tags_test.go</code> - Unit tests - <code>internal/integration/s3_test.go</code> - Integration tests - <code>internal/integration/README.md</code> - IAM documentation</p> <p>Key Features: - <code>WriteWithMetadata()</code> - Upload with metadata tags - <code>PutObjectTagging()</code> - Tag existing objects - <code>GetObjectTagging()</code> - Retrieve metadata from tags - Priority-based tag selection (10-tag S3 limit) - Automatic tag sanitization</p> <p>Test Coverage: - Unit tests: tagsToString, tag conversion - Integration tests: 4 comprehensive subtests - IAM permissions documented</p>"},{"location":"v0.2.0-progress/#issue-20-implement-zeiss-czi-extractor-closed","title":"Issue #20: Implement Zeiss CZI Extractor \u2705 CLOSED","text":"<p>Status: Complete Closed: 2025-11-24</p>"},{"location":"v0.2.0-progress/#implementation_3","title":"Implementation","text":"<p>File: <code>internal/metadata/zeiss_czi.go</code> (557 lines)</p> <p>Features: - Binary CZI format parsing - XML metadata segment extraction - 20+ metadata fields extracted:   - Instrument info (manufacturer, model, type)   - Objective details (magnification, NA, immersion)   - Image dimensions (X, Y, Z, C, T)   - Pixel scaling (micrometers)   - Channel information (wavelengths, dyes)   - Acquisition date, operator, software version</p> <p>Documentation: - 150+ lines of inline documentation - CZI format overview - References to official ZEISS libraries - Bio-Formats integration notes</p> <p>Test Coverage: - <code>internal/metadata/zeiss_czi_test.go</code> (435 lines)   - 11 comprehensive tests   - Coverage: Extract 88.9%, parseXMLMetadata 95.5%   - Mock CZI file generation   - All tests passing</p> <p>Compatible With: LSM 880, 900, 980, Axio Scan.Z1, ELYRA systems</p>"},{"location":"v0.2.0-progress/#issue-21-cli-integration-for-metadata-extraction-closed","title":"Issue #21: CLI Integration for Metadata Extraction \u2705 CLOSED","text":"<p>Status: Complete Closed: 2025-11-24</p>"},{"location":"v0.2.0-progress/#implementation_4","title":"Implementation","text":"<p>File: <code>internal/cli/metadata.go</code> (639 lines)</p> <p>Commands Implemented:</p> <ol> <li><code>cicada metadata extract &lt;path&gt;</code></li> <li>Extract metadata from files</li> <li><code>--format</code> flag (json, yaml, table)</li> <li><code>--output</code> flag to save to file</li> <li> <p><code>--extractor</code> flag to force specific extractor</p> </li> <li> <p><code>cicada metadata show &lt;path&gt;</code></p> </li> <li>Human-readable metadata display</li> <li> <p>Formatted tables with colors</p> </li> <li> <p><code>cicada metadata validate &lt;path&gt;</code></p> </li> <li>Validate file metadata</li> <li><code>--preset</code> flag for instrument validation</li> <li>Colored output (\u2713, \u274c, \u26a0\ufe0f)</li> <li> <p>Quality scoring (0-100)</p> </li> <li> <p><code>cicada metadata list</code></p> </li> <li> <p>List all available extractors</p> </li> <li> <p><code>cicada metadata preset</code> subcommands</p> </li> <li><code>preset list</code> - List available presets</li> <li><code>preset show &lt;id&gt;</code> - Show preset details</li> </ol> <p>Test Coverage: - <code>internal/cli/metadata_test.go</code> (234 lines)   - 14 test cases   - All commands tested   - All tests passing</p> <p>Documentation: - <code>docs/METADATA_EXTRACTION.md</code> (731 lines) - <code>docs/PRESETS.md</code> (1086 lines) - Examples in main README</p>"},{"location":"v0.2.0-progress/#issue-22-instrument-preset-system-closed","title":"Issue #22: Instrument Preset System \u2705 CLOSED","text":"<p>Status: Complete Closed: 2025-11-24</p>"},{"location":"v0.2.0-progress/#implementation_5","title":"Implementation","text":"<p>File: <code>internal/metadata/preset.go</code> (500+ lines)</p> <p>Features: - <code>PresetRegistry</code> for managing presets - <code>InstrumentPreset</code> structure - Field validation (type, enum, range) - Quality scoring (0-100) - Detailed validation results</p> <p>Default Presets (8 total): 1. Generic Microscopy 2. Generic Sequencing 3. Zeiss LSM 880 4. Illumina NovaSeq 5. Illumina NextSeq 6. Illumina MiSeq 7. Thermo Fisher Orbitrap 8. Beckman Coulter CytoFLEX</p> <p>Test Coverage: - <code>internal/metadata/preset_test.go</code>   - 6 test functions   - Registry operations tested   - Quality scoring tests   - All tests passing</p> <p>CLI Integration: - <code>cicada metadata preset list</code> - <code>cicada metadata preset show &lt;id&gt;</code> - <code>cicada metadata validate --preset &lt;id&gt;</code></p>"},{"location":"v0.2.0-progress/#issue-23-pluggable-doi-provider-system-closed","title":"Issue #23: Pluggable DOI Provider System \u2705 CLOSED","text":"<p>Status: Complete Closed: 2025-11-24 (prior session)</p>"},{"location":"v0.2.0-progress/#implementation_6","title":"Implementation","text":"<p>Provider Support: - DataCite integration - Zenodo integration - Pluggable provider interface</p> <p>Documentation: - <code>docs/DOI_WORKFLOW.md</code> - <code>docs/PROVIDERS.md</code> - Integration tests</p>"},{"location":"v0.2.0-progress/#issue-25-documentation-testing-in-progress","title":"Issue #25: Documentation &amp; Testing \ud83d\udd04 IN PROGRESS","text":"<p>Status: In Progress</p>"},{"location":"v0.2.0-progress/#completed","title":"Completed","text":"<ul> <li>\u2705 Integration test suite (28 tests, all passing)</li> <li>\u2705 USER_SCENARIOS_v0.2.0.md exists</li> <li>\u2705 Core documentation (METADATA_EXTRACTION.md, PRESETS.md)</li> <li>\u2705 v0.2.0 progress tracking</li> </ul>"},{"location":"v0.2.0-progress/#remaining-tasks","title":"Remaining Tasks","text":"<ul> <li> Create MIGRATION_v0.2.0.md guide</li> <li> Create MULTI_USER_SETUP.md guide (if needed)</li> <li> Run performance &amp; load testing</li> <li> Final documentation review</li> <li> Prepare release notes</li> </ul>"},{"location":"v0.2.0-progress/#test-summary","title":"Test Summary","text":""},{"location":"v0.2.0-progress/#unit-tests","title":"Unit Tests","text":"<pre><code>$ go test ./internal/metadata/...\nPASS\nok      github.com/scttfrdmn/cicada/internal/metadata\n\n$ go test ./internal/cli/...\nPASS\nok      github.com/scttfrdmn/cicada/internal/cli\n</code></pre>"},{"location":"v0.2.0-progress/#integration-tests","title":"Integration Tests","text":"<pre><code>$ go test ./internal/integration/... -short\nPASS\nok      github.com/scttfrdmn/cicada/internal/integration\n</code></pre> <p>Total Test Count: 100+ tests across all packages</p>"},{"location":"v0.2.0-progress/#linter-status","title":"Linter Status","text":"<pre><code>$ golangci-lint run ./internal/...\n0 issues\n</code></pre>"},{"location":"v0.2.0-progress/#documentation-status","title":"Documentation Status","text":""},{"location":"v0.2.0-progress/#comprehensive-guides","title":"Comprehensive Guides","text":"<ul> <li>\u2705 <code>docs/METADATA_EXTRACTION.md</code> (731 lines)</li> <li>\u2705 <code>docs/PRESETS.md</code> (1086 lines)</li> <li>\u2705 <code>docs/DOI_WORKFLOW.md</code></li> <li>\u2705 <code>docs/PROVIDERS.md</code></li> <li>\u2705 <code>docs/USER_SCENARIOS_v0.2.0.md</code></li> <li>\u2705 <code>internal/integration/README.md</code> (S3 &amp; DOI integration)</li> <li>\u2705 <code>presets/README.md</code></li> </ul>"},{"location":"v0.2.0-progress/#api-documentation","title":"API Documentation","text":"<ul> <li>\u2705 Inline code documentation (1000+ lines)</li> <li>\u2705 Package-level documentation</li> <li>\u2705 Example usage in README</li> </ul>"},{"location":"v0.2.0-progress/#key-achievements","title":"Key Achievements","text":"<ol> <li>Comprehensive Metadata System</li> <li>6 instrument-specific metadata types</li> <li>14 file format extractors</li> <li> <p>Full CZI extractor with 20+ fields</p> </li> <li> <p>S3 Integration</p> </li> <li>Metadata stored as S3 object tags</li> <li>Priority-based tag selection</li> <li> <p>Integration tests with real AWS</p> </li> <li> <p>CLI Tools</p> </li> <li>5 major commands</li> <li>Multiple output formats</li> <li> <p>Interactive validation</p> </li> <li> <p>Preset System</p> </li> <li>8 instrument presets</li> <li>Quality scoring</li> <li> <p>Validation framework</p> </li> <li> <p>Testing</p> </li> <li>100+ tests</li> <li>Unit, integration, CLI tests</li> <li> <p>All passing, 0 linter issues</p> </li> <li> <p>Documentation</p> </li> <li>2600+ lines of guides</li> <li>Comprehensive API docs</li> <li>Integration examples</li> </ol>"},{"location":"v0.2.0-progress/#next-steps-for-issue-25","title":"Next Steps for Issue #25","text":"<ol> <li>Create MIGRATION_v0.2.0.md guide</li> <li>Performance testing</li> <li>Final documentation review</li> <li>Release preparation</li> </ol>"},{"location":"v0.2.0-progress/#release-readiness","title":"Release Readiness","text":"<p>Code Quality: \u2705 Excellent - All tests passing - 0 linter issues - Comprehensive test coverage</p> <p>Documentation: \u2705 Excellent - 2600+ lines of user guides - API documentation complete - Integration examples provided</p> <p>Feature Completeness: \u2705 87.5% - Core functionality complete - CLI tools working - Integration tested</p> <p>Ready for Beta Release: Yes, pending Issue #25 completion</p>"},{"location":"about/changelog/","title":"Changelog","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"about/citation/","title":"Citation","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"about/license/","title":"License","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"advanced/doi-workflow/","title":"Doi Workflow","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"advanced/integrations/","title":"Integrations","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"advanced/providers/","title":"Providers","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"development/building/","title":"Building","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"development/contributing/","title":"Contributing","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"development/testing/","title":"Testing","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Documentation in Progress</p> <p>This page is being updated as part of v0.3.0.</p>"},{"location":"getting-started/configuration/#configuration-file","title":"Configuration File","text":"<p>Cicada uses <code>~/.cicada/config.yaml</code> for configuration.</p> <pre><code>version: \"1\"\n\naws:\n  profile: default\n  region: us-west-2\n\nsync:\n  concurrency: 4\n</code></pre> <p>For complete current documentation, see the README.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Documentation in Progress</p> <p>This page is being updated as part of v0.3.0. Complete installation guide coming soon.</p>"},{"location":"getting-started/installation/#quick-install","title":"Quick Install","text":""},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/scttfrdmn/cicada.git\ncd cicada\nmake install\ncicada version\n</code></pre>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Go 1.23+ (for building from source)</li> <li>AWS credentials configured (for S3 functionality)</li> </ul>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide</li> <li>Configuration</li> </ul> <p>For the current complete documentation, see the README.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Documentation in Progress</p> <p>This page is being updated as part of v0.3.0. Complete quick start guide coming soon.</p>"},{"location":"getting-started/quick-start/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quick-start/#sync-to-s3","title":"Sync to S3","text":"<pre><code>cicada sync /local/data s3://my-bucket/data\n</code></pre>"},{"location":"getting-started/quick-start/#extract-metadata","title":"Extract Metadata","text":"<pre><code>cicada metadata extract sample.fastq.gz\n</code></pre>"},{"location":"getting-started/quick-start/#watch-for-changes","title":"Watch for Changes","text":"<pre><code>cicada watch add /data/microscope s3://lab-data/microscopy\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>User Guide</li> <li>CLI Reference</li> </ul> <p>For complete current documentation, see the README.</p>"},{"location":"reference/architecture/","title":"Architecture","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"reference/cli/","title":"Cli","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"reference/configuration/","title":"Configuration","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"reference/formats/","title":"Formats","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"user-guide/metadata/","title":"Metadata","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"user-guide/overview/","title":"Overview","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"user-guide/presets/","title":"Presets","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"user-guide/storage-sync/","title":"Storage Sync","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"},{"location":"user-guide/watch/","title":"Watch","text":"<p>Documentation in Progress</p> <p>This page is being created as part of v0.3.0 documentation improvements.</p> <p>Complete documentation coming soon! In the meantime, see:</p> <ul> <li>GitHub Repository</li> <li>README</li> <li>Existing Documentation</li> </ul>"}]}